{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "09-NLP Applications - Text Classification - Machine Learning and Basic DNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PLOatR1LRaE_"
      },
      "source": [
        "# Sentiment Analysis - Machine Learning and Basic Deep Neural Network Models\n",
        "\n",
        "We have already discussed that sentiment analysis, also popularly known as opinion analysis or opinion mining is one of the most important applications of NLP. The key idea is to predict the potential sentiment of a body of text based on the textual content. In this sub-unit, we will be exploring supervised learning models. \n",
        "\n",
        "![](https://github.com/dipanjanS/nlp_workshop_dhs18/blob/master/Unit%2012%20-%20Project%209%20-%20Sentiment%20Analysis%20-%20Supervised%20Learning/sentiment_cover.png?raw=1)\n",
        "\n",
        "Another way to build a model to understand the text content and predict the sentiment of the text based reviews is to use supervised machine learning. To be more specific, we will be using classification models for solving this problem. We will be building an automated sentiment text classification system in subsequent sections. The major steps to achieve this are mentioned as follows.\n",
        "\n",
        "1.\tPrepare train and test datasets (optionally a validation dataset)\n",
        "2.\tPre-process and normalize text documents\n",
        "3.\tFeature Engineering \n",
        "4.\tModel training\n",
        "5.\tModel prediction and evaluation\n",
        "\n",
        "These are the major steps for building our system. Optionally the last step would be to deploy the model in your server or on the cloud. The following figure shows a detailed workflow for building a standard text classification system with supervised learning (classification) models.\n",
        "\n",
        "![](https://github.com/dipanjanS/nlp_workshop_dhs18/blob/master/Unit%2012%20-%20Project%209%20-%20Sentiment%20Analysis%20-%20Supervised%20Learning/sentiment_classifier_workflow.png?raw=1)\n",
        "\n",
        "\n",
        "In our scenario, documents indicate the movie reviews and classes indicate the review sentiments which can either be positive or negative making it a binary classification problem. We will build models using both traditional machine learning methods and newer deep learning in the subsequent sections. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "610bNp_4SQma",
        "outputId": "57acfd55-1514-438c-fe1a-d04546dfac01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/85/41/c3dfd5feb91a8d587ed1a59f553f07c05f95ad4e5d00ab78702fbf8fe48a/contractions-0.0.24-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 30.3MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 50.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81708 sha256=5ae73bbabfd97f3a7bafa02a5358b67cbb53f7704be64be23f6b4387f9315b1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.24 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch) (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UMFvLtEHRaFM"
      },
      "source": [
        "# Load and View Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qgphrYufRaFR",
        "outputId": "6c120a92-4e54-4f44-d293-c48ed9070409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv(r'https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2011%20-%20Sentiment%20Analysis%20-%20Unsupervised%20Learning/movie_reviews.csv.bz2')\n",
        "dataset.info()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5nkEEGExRaFc",
        "outputId": "86e71c1a-74d0-4ff1-a393-fedff997ea6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-V1NWGhcRaFi"
      },
      "source": [
        "# Build Train and Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7JP10IEYRaFj",
        "colab": {}
      },
      "source": [
        "# build train and test datasets\n",
        "reviews = dataset['review'].values\n",
        "sentiments = dataset['sentiment'].values\n",
        "\n",
        "train_reviews = reviews[:35000]\n",
        "train_sentiments = sentiments[:35000]\n",
        "\n",
        "test_reviews = reviews[35000:]\n",
        "test_sentiments = sentiments[35000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i45AFxXNRaFn"
      },
      "source": [
        "# Text Wrangling & Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oHZ0lEGNRaFo",
        "colab": {}
      },
      "source": [
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "import tqdm\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "  return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "def pre_process_corpus(docs):\n",
        "  norm_docs = []\n",
        "  for doc in tqdm.tqdm(docs):\n",
        "    doc = strip_html_tags(doc)\n",
        "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    doc = doc.lower()\n",
        "    doc = remove_accented_chars(doc)\n",
        "    doc = contractions.fix(doc)\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "    doc = doc.strip()  \n",
        "    norm_docs.append(doc)\n",
        "  \n",
        "  return norm_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CO3ESug2RaFr",
        "outputId": "36eb914f-67c3-46ae-86e1-45743e8466a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "norm_train_reviews = pre_process_corpus(train_reviews)\n",
        "norm_test_reviews = pre_process_corpus(test_reviews)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35000/35000 [00:16<00:00, 2185.58it/s]\n",
            "100%|██████████| 15000/15000 [00:06<00:00, 2205.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 22.7 s, sys: 125 ms, total: 22.8 s\n",
            "Wall time: 22.8 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ucDv8n50RaFu"
      },
      "source": [
        "# Traditional Supervised Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AOZ7Rn0jRaFv"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JD8q5QoERaFw",
        "outputId": "36152867-326d-462b-8f7e-a3ebfb8857cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# build BOW features on train reviews\n",
        "cv = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2))\n",
        "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
        "\n",
        "\n",
        "# build TFIDF features on train reviews\n",
        "tv = TfidfVectorizer(use_idf=True, min_df=5, max_df=1.0, ngram_range=(1,2),\n",
        "                     sublinear_tf=True)\n",
        "tv_train_features = tv.fit_transform(norm_train_reviews)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 43.3 s, sys: 822 ms, total: 44.2 s\n",
            "Wall time: 44.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sSvqpHRYRaFz",
        "outputId": "5aaf6e6d-c79b-4d8c-b9a7-899fd4cb1d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# transform test reviews into features\n",
        "cv_test_features = cv.transform(norm_test_reviews)\n",
        "tv_test_features = tv.transform(norm_test_reviews)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9.97 s, sys: 11.9 ms, total: 9.98 s\n",
            "Wall time: 9.99 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mfQPYw8PRaF2",
        "outputId": "2a886db4-6482-43ae-e93a-72fdd130d4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
        "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW model:> Train features shape: (35000, 194922)  Test features shape: (15000, 194922)\n",
            "TFIDF model:> Train features shape: (35000, 194922)  Test features shape: (15000, 194922)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3aUUsxMrRaF7"
      },
      "source": [
        "## Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nPA5UtYFRaF8"
      },
      "source": [
        "### Try out Logistic Regression\n",
        "\n",
        "The logistic regression model is actually a statistical model developed by statistician\n",
        "David Cox in 1958. It is also known as the logit or logistic model since it uses the\n",
        "logistic (popularly also known as sigmoid) mathematical function to estimate the\n",
        "parameter values. These are the coefficients of all our features such that the overall loss\n",
        "is minimized when predicting the outcome—"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JGHQErnMRaF9",
        "outputId": "4a4ac29f-8be4-401a-f43f-ab838a82721a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Logistic Regression model on BOW features\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# instantiate model\n",
        "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, solver='lbfgs', random_state=42)\n",
        "\n",
        "# train model\n",
        "lr.fit(cv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "lr_bow_predictions = lr.predict(cv_test_features)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 13s, sys: 47.5 s, total: 2min\n",
            "Wall time: 1min 1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wqeRyayrRaGA",
        "outputId": "9dfe533a-5fe1-4650-929f-975eb99f5ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, lr_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.90      0.90      7490\n",
            "    positive       0.90      0.91      0.90      7510\n",
            "\n",
            "    accuracy                           0.90     15000\n",
            "   macro avg       0.90      0.90      0.90     15000\n",
            "weighted avg       0.90      0.90      0.90     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6754</td>\n",
              "      <td>736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>709</td>\n",
              "      <td>6801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6754       736\n",
              "positive       709      6801"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uQEAz6O6RaGC",
        "outputId": "662d4c6c-0580-463b-9e88-68cbf9691d98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Logistic Regression model on TF-IDF features\n",
        "\n",
        "# train model\n",
        "lr.fit(tv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "lr_tfidf_predictions = lr.predict(tv_test_features)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.33 s, sys: 2.06 s, total: 5.39 s\n",
            "Wall time: 2.85 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HNCfZnUORaGE",
        "outputId": "a46057c8-9053-4ad6-f20b-c000bdeef5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, lr_tfidf_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_tfidf_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.91      0.89      0.90      7490\n",
            "    positive       0.90      0.91      0.90      7510\n",
            "\n",
            "    accuracy                           0.90     15000\n",
            "   macro avg       0.90      0.90      0.90     15000\n",
            "weighted avg       0.90      0.90      0.90     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6694</td>\n",
              "      <td>796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>665</td>\n",
              "      <td>6845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6694       796\n",
              "positive       665      6845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FoOoEAiXRaGH"
      },
      "source": [
        "### Try out Random Forest\n",
        "\n",
        "Decision trees are a family of supervised machine learning algorithms that can represent\n",
        "and interpret sets of rules automatically from the underlying data. They use metrics like\n",
        "information gain and gini-index to build the tree. However, a major drawback of decision\n",
        "trees is that since they are non-parametric, the more data there is, greater the depth of\n",
        "the tree. We can end up with really huge and deep trees that are prone to overfitting. The\n",
        "model might work really well on training data, but instead of learning, it just memorizes\n",
        "all the training samples and builds very specific rules to them. Hence, it performs really\n",
        "poorly on the test data. Random forests try to tackle this problem.\n",
        "\n",
        "A random forest is a meta-estimator or an ensemble model that fits a number of\n",
        "decision tree classifiers on various sub-samples of the dataset and uses averaging to\n",
        "improve the predictive accuracy and control over-fitting. The sub-sample size is always\n",
        "the same as the original input sample size, but the samples are drawn with replacement\n",
        "(bootstrap samples). In random forests, all the trees are trained in parallel (bagging\n",
        "model/bootstrap aggregation). Besides this, each tree in the ensemble is built from a\n",
        "sample drawn with replacement (i.e., a bootstrap sample) from the training set. Also,\n",
        "when splitting a node during the construction of the tree, the split that is chosen is no\n",
        "longer the best split among all features. Instead, the split that is picked is the best split\n",
        "among a random subset of the features. T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YzaSSOpYRaGH",
        "outputId": "84fc2f8d-981a-4613-958d-20f9629aef7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Random Forest model on BOW features\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# instantiate model\n",
        "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "\n",
        "# train model\n",
        "rf.fit(cv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "rf_bow_predictions = rf.predict(cv_test_features)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 45s, sys: 108 ms, total: 3min 45s\n",
            "Wall time: 1min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "617Kuv7_RaGJ",
        "outputId": "8dff5dfb-0bba-41ef-af30-6b7e19d6e29b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, rf_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, rf_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.86      0.86      7490\n",
            "    positive       0.86      0.86      0.86      7510\n",
            "\n",
            "    accuracy                           0.86     15000\n",
            "   macro avg       0.86      0.86      0.86     15000\n",
            "weighted avg       0.86      0.86      0.86     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6409</td>\n",
              "      <td>1081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1083</td>\n",
              "      <td>6427</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6409      1081\n",
              "positive      1083      6427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WdvmBOrPRaGM",
        "outputId": "73aae5db-351a-4818-98a6-2fdc3556f3f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "# Random Forest model on TF-IDF features\n",
        "\n",
        "# train model\n",
        "rf.fit(tv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "rf_tfidf_predictions = rf.predict(tv_test_features)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 7s, sys: 150 ms, total: 3min 7s\n",
            "Wall time: 1min 35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8hBOMh6uRaGP",
        "outputId": "8bb5c320-53d1-4c80-a48d-46b5ad0c9420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, rf_tfidf_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, rf_tfidf_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.85      0.86      0.85      7490\n",
            "    positive       0.86      0.84      0.85      7510\n",
            "\n",
            "    accuracy                           0.85     15000\n",
            "   macro avg       0.85      0.85      0.85     15000\n",
            "weighted avg       0.85      0.85      0.85     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6447</td>\n",
              "      <td>1043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1174</td>\n",
              "      <td>6336</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6447      1043\n",
              "positive      1174      6336"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QoqZhMQFRaGS"
      },
      "source": [
        "# Newer Supervised Deep Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QZw6LYNHRaGT",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GiZbcv_gRaGZ"
      },
      "source": [
        "## Prediction class label encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JnhC4rWaRaGb",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder()\n",
        "# tokenize train reviews & encode train labels\n",
        "tokenized_train = [nltk.word_tokenize(text)\n",
        "                       for text in norm_train_reviews]\n",
        "y_train = le.fit_transform(train_sentiments)\n",
        "# tokenize test reviews & encode test labels\n",
        "tokenized_test = [nltk.word_tokenize(text)\n",
        "                       for text in norm_test_reviews]\n",
        "y_test = le.fit_transform(test_sentiments)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4ogDRDh4RaGg",
        "outputId": "6cd4a809-e9bb-4b09-ecb9-ba911c704bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# print class label encoding map and encoded labels\n",
        "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "print('Sample test label transformation:\\n'+'-'*35,\n",
        "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_test[:3])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
            "Sample test label transformation:\n",
            "----------------------------------- \n",
            "Actual Labels: ['negative' 'positive' 'negative'] \n",
            "Encoded Labels: [0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hdexbrYXRaGk"
      },
      "source": [
        "## Feature Engineering with word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G5S0u0BbiN2a",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T9kfCw6LRaGl",
        "outputId": "872ace06-b861-48bf-d6a2-18d548ebe5d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "# build word2vec model\n",
        "w2v_num_features = 300\n",
        "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
        "                                   min_count=10, workers=4, iter=5)    "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-27 20:10:59,537 : INFO : collecting all words and their counts\n",
            "2020-05-27 20:10:59,538 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-05-27 20:10:59,940 : INFO : PROGRESS: at sentence #10000, processed 2294933 words, keeping 82417 word types\n",
            "2020-05-27 20:11:00,349 : INFO : PROGRESS: at sentence #20000, processed 4591079 words, keeping 124832 word types\n",
            "2020-05-27 20:11:00,750 : INFO : PROGRESS: at sentence #30000, processed 6884452 words, keeping 159825 word types\n",
            "2020-05-27 20:11:00,965 : INFO : collected 176258 word types from a corpus of 8035381 raw words and 35000 sentences\n",
            "2020-05-27 20:11:00,966 : INFO : Loading a fresh vocabulary\n",
            "2020-05-27 20:11:01,656 : INFO : effective_min_count=10 retains 24661 unique words (13% of original 176258, drops 151597)\n",
            "2020-05-27 20:11:01,656 : INFO : effective_min_count=10 leaves 7763119 word corpus (96% of original 8035381, drops 272262)\n",
            "2020-05-27 20:11:01,728 : INFO : deleting the raw counts dictionary of 176258 items\n",
            "2020-05-27 20:11:01,734 : INFO : sample=0.001 downsamples 49 most-common words\n",
            "2020-05-27 20:11:01,734 : INFO : downsampling leaves estimated 5721143 word corpus (73.7% of prior 7763119)\n",
            "2020-05-27 20:11:01,794 : INFO : estimated required memory for 24661 words and 300 dimensions: 71516900 bytes\n",
            "2020-05-27 20:11:01,795 : INFO : resetting layer weights\n",
            "2020-05-27 20:11:06,272 : INFO : training model with 4 workers on 24661 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=150\n",
            "2020-05-27 20:11:07,448 : INFO : EPOCH 1 - PROGRESS: at 1.55% examples, 78186 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:08,472 : INFO : EPOCH 1 - PROGRESS: at 3.49% examples, 89876 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:09,584 : INFO : EPOCH 1 - PROGRESS: at 5.12% examples, 86994 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:10,668 : INFO : EPOCH 1 - PROGRESS: at 6.97% examples, 89432 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:11,749 : INFO : EPOCH 1 - PROGRESS: at 8.48% examples, 88308 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:12,820 : INFO : EPOCH 1 - PROGRESS: at 10.18% examples, 88859 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:13,822 : INFO : EPOCH 1 - PROGRESS: at 11.95% examples, 90035 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:14,897 : INFO : EPOCH 1 - PROGRESS: at 13.46% examples, 89285 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:16,161 : INFO : EPOCH 1 - PROGRESS: at 15.35% examples, 89189 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:17,414 : INFO : EPOCH 1 - PROGRESS: at 17.11% examples, 89168 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:18,611 : INFO : EPOCH 1 - PROGRESS: at 19.18% examples, 89579 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-27 20:11:19,877 : INFO : EPOCH 1 - PROGRESS: at 21.31% examples, 90000 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:20,909 : INFO : EPOCH 1 - PROGRESS: at 23.17% examples, 90828 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:21,923 : INFO : EPOCH 1 - PROGRESS: at 24.85% examples, 91187 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:22,946 : INFO : EPOCH 1 - PROGRESS: at 26.38% examples, 90979 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:23,978 : INFO : EPOCH 1 - PROGRESS: at 28.21% examples, 91240 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:25,031 : INFO : EPOCH 1 - PROGRESS: at 29.85% examples, 91282 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:26,171 : INFO : EPOCH 1 - PROGRESS: at 31.48% examples, 90937 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:11:27,187 : INFO : EPOCH 1 - PROGRESS: at 33.29% examples, 91508 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:28,249 : INFO : EPOCH 1 - PROGRESS: at 34.92% examples, 91228 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:29,326 : INFO : EPOCH 1 - PROGRESS: at 36.65% examples, 91204 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:30,381 : INFO : EPOCH 1 - PROGRESS: at 38.35% examples, 91287 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:31,389 : INFO : EPOCH 1 - PROGRESS: at 40.03% examples, 91529 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:32,518 : INFO : EPOCH 1 - PROGRESS: at 41.81% examples, 91317 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:33,542 : INFO : EPOCH 1 - PROGRESS: at 43.40% examples, 91220 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-27 20:11:34,659 : INFO : EPOCH 1 - PROGRESS: at 45.27% examples, 91314 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:35,709 : INFO : EPOCH 1 - PROGRESS: at 46.97% examples, 91416 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:36,838 : INFO : EPOCH 1 - PROGRESS: at 48.68% examples, 91251 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:37,949 : INFO : EPOCH 1 - PROGRESS: at 50.42% examples, 91180 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-27 20:11:38,980 : INFO : EPOCH 1 - PROGRESS: at 52.32% examples, 91508 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:40,068 : INFO : EPOCH 1 - PROGRESS: at 53.90% examples, 91255 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:11:41,075 : INFO : EPOCH 1 - PROGRESS: at 55.82% examples, 91625 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:42,159 : INFO : EPOCH 1 - PROGRESS: at 57.41% examples, 91587 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-27 20:11:43,266 : INFO : EPOCH 1 - PROGRESS: at 58.94% examples, 91300 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:44,316 : INFO : EPOCH 1 - PROGRESS: at 60.79% examples, 91547 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:45,351 : INFO : EPOCH 1 - PROGRESS: at 62.40% examples, 91442 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:46,392 : INFO : EPOCH 1 - PROGRESS: at 64.21% examples, 91692 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:47,432 : INFO : EPOCH 1 - PROGRESS: at 65.75% examples, 91578 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:48,445 : INFO : EPOCH 1 - PROGRESS: at 67.37% examples, 91550 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:49,497 : INFO : EPOCH 1 - PROGRESS: at 69.25% examples, 91755 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:50,588 : INFO : EPOCH 1 - PROGRESS: at 70.83% examples, 91546 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:51,812 : INFO : EPOCH 1 - PROGRESS: at 72.79% examples, 91558 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:52,855 : INFO : EPOCH 1 - PROGRESS: at 74.56% examples, 91753 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:53,862 : INFO : EPOCH 1 - PROGRESS: at 76.24% examples, 91705 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:54,865 : INFO : EPOCH 1 - PROGRESS: at 77.90% examples, 91689 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:55,893 : INFO : EPOCH 1 - PROGRESS: at 79.55% examples, 91773 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:56,926 : INFO : EPOCH 1 - PROGRESS: at 81.19% examples, 91692 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:58,025 : INFO : EPOCH 1 - PROGRESS: at 82.93% examples, 91636 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:11:59,027 : INFO : EPOCH 1 - PROGRESS: at 84.80% examples, 91896 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:00,033 : INFO : EPOCH 1 - PROGRESS: at 86.11% examples, 91621 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:01,034 : INFO : EPOCH 1 - PROGRESS: at 87.75% examples, 91741 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-27 20:12:02,202 : INFO : EPOCH 1 - PROGRESS: at 89.50% examples, 91580 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:12:03,390 : INFO : EPOCH 1 - PROGRESS: at 91.39% examples, 91636 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:04,516 : INFO : EPOCH 1 - PROGRESS: at 93.31% examples, 91769 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:05,542 : INFO : EPOCH 1 - PROGRESS: at 95.24% examples, 91964 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:06,578 : INFO : EPOCH 1 - PROGRESS: at 96.91% examples, 91894 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:07,637 : INFO : EPOCH 1 - PROGRESS: at 98.57% examples, 91897 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:08,256 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-27 20:12:08,370 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-27 20:12:08,383 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-27 20:12:08,385 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-27 20:12:08,386 : INFO : EPOCH - 1 : training on 8035381 raw words (5720885 effective words) took 62.1s, 92114 effective words/s\n",
            "2020-05-27 20:12:09,462 : INFO : EPOCH 2 - PROGRESS: at 1.43% examples, 78859 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:10,471 : INFO : EPOCH 2 - PROGRESS: at 3.21% examples, 87906 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:11,528 : INFO : EPOCH 2 - PROGRESS: at 4.99% examples, 89427 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:12,531 : INFO : EPOCH 2 - PROGRESS: at 6.72% examples, 91443 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:13,647 : INFO : EPOCH 2 - PROGRESS: at 8.36% examples, 90770 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:14,698 : INFO : EPOCH 2 - PROGRESS: at 10.18% examples, 92159 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:15,728 : INFO : EPOCH 2 - PROGRESS: at 11.95% examples, 92548 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:16,736 : INFO : EPOCH 2 - PROGRESS: at 13.60% examples, 93019 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:17,741 : INFO : EPOCH 2 - PROGRESS: at 15.13% examples, 92802 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:18,831 : INFO : EPOCH 2 - PROGRESS: at 16.71% examples, 92437 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:12:19,863 : INFO : EPOCH 2 - PROGRESS: at 18.57% examples, 93264 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:20,947 : INFO : EPOCH 2 - PROGRESS: at 20.17% examples, 92464 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-27 20:12:22,103 : INFO : EPOCH 2 - PROGRESS: at 22.18% examples, 92888 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:23,294 : INFO : EPOCH 2 - PROGRESS: at 24.15% examples, 92954 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:24,484 : INFO : EPOCH 2 - PROGRESS: at 26.01% examples, 92943 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:25,694 : INFO : EPOCH 2 - PROGRESS: at 28.07% examples, 92943 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:26,835 : INFO : EPOCH 2 - PROGRESS: at 29.95% examples, 93216 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:27,884 : INFO : EPOCH 2 - PROGRESS: at 31.63% examples, 93161 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:28,938 : INFO : EPOCH 2 - PROGRESS: at 33.30% examples, 93146 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:29,966 : INFO : EPOCH 2 - PROGRESS: at 35.06% examples, 93234 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:30,972 : INFO : EPOCH 2 - PROGRESS: at 36.65% examples, 93105 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:32,013 : INFO : EPOCH 2 - PROGRESS: at 38.35% examples, 93159 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:33,099 : INFO : EPOCH 2 - PROGRESS: at 40.03% examples, 93043 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:34,184 : INFO : EPOCH 2 - PROGRESS: at 41.94% examples, 93180 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:35,210 : INFO : EPOCH 2 - PROGRESS: at 43.79% examples, 93551 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:36,383 : INFO : EPOCH 2 - PROGRESS: at 45.50% examples, 93106 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:37,398 : INFO : EPOCH 2 - PROGRESS: at 47.23% examples, 93266 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:38,398 : INFO : EPOCH 2 - PROGRESS: at 48.93% examples, 93427 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:39,470 : INFO : EPOCH 2 - PROGRESS: at 50.71% examples, 93366 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-27 20:12:40,496 : INFO : EPOCH 2 - PROGRESS: at 52.32% examples, 93246 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:41,526 : INFO : EPOCH 2 - PROGRESS: at 54.16% examples, 93511 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:42,607 : INFO : EPOCH 2 - PROGRESS: at 55.82% examples, 93220 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:43,794 : INFO : EPOCH 2 - PROGRESS: at 57.67% examples, 93249 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:44,962 : INFO : EPOCH 2 - PROGRESS: at 59.54% examples, 93326 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:46,115 : INFO : EPOCH 2 - PROGRESS: at 61.55% examples, 93442 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:47,268 : INFO : EPOCH 2 - PROGRESS: at 63.51% examples, 93550 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:48,270 : INFO : EPOCH 2 - PROGRESS: at 65.16% examples, 93644 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:49,342 : INFO : EPOCH 2 - PROGRESS: at 66.85% examples, 93596 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:50,527 : INFO : EPOCH 2 - PROGRESS: at 68.90% examples, 93637 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:51,527 : INFO : EPOCH 2 - PROGRESS: at 70.58% examples, 93732 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:52,605 : INFO : EPOCH 2 - PROGRESS: at 72.32% examples, 93667 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:53,626 : INFO : EPOCH 2 - PROGRESS: at 73.94% examples, 93716 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:54,688 : INFO : EPOCH 2 - PROGRESS: at 75.75% examples, 93657 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:55,690 : INFO : EPOCH 2 - PROGRESS: at 77.64% examples, 93905 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:12:56,772 : INFO : EPOCH 2 - PROGRESS: at 79.20% examples, 93700 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-27 20:12:57,877 : INFO : EPOCH 2 - PROGRESS: at 81.02% examples, 93718 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:12:58,968 : INFO : EPOCH 2 - PROGRESS: at 83.07% examples, 93901 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:00,085 : INFO : EPOCH 2 - PROGRESS: at 84.92% examples, 93915 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:01,096 : INFO : EPOCH 2 - PROGRESS: at 86.59% examples, 93984 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:02,149 : INFO : EPOCH 2 - PROGRESS: at 88.26% examples, 93971 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:03,183 : INFO : EPOCH 2 - PROGRESS: at 89.86% examples, 93867 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:04,390 : INFO : EPOCH 2 - PROGRESS: at 91.71% examples, 93832 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:05,494 : INFO : EPOCH 2 - PROGRESS: at 93.69% examples, 93975 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:06,680 : INFO : EPOCH 2 - PROGRESS: at 95.75% examples, 93994 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-27 20:13:07,824 : INFO : EPOCH 2 - PROGRESS: at 97.77% examples, 94058 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:08,872 : INFO : EPOCH 2 - PROGRESS: at 99.51% examples, 94161 words/s, in_qsize 4, out_qsize 0\n",
            "2020-05-27 20:13:08,988 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-27 20:13:09,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-27 20:13:09,046 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-27 20:13:09,061 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-27 20:13:09,062 : INFO : EPOCH - 2 : training on 8035381 raw words (5721462 effective words) took 60.7s, 94304 effective words/s\n",
            "2020-05-27 20:13:10,125 : INFO : EPOCH 3 - PROGRESS: at 1.55% examples, 86573 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:11,159 : INFO : EPOCH 3 - PROGRESS: at 3.21% examples, 87498 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:12,184 : INFO : EPOCH 3 - PROGRESS: at 5.12% examples, 92230 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:13,234 : INFO : EPOCH 3 - PROGRESS: at 6.72% examples, 90803 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:14,274 : INFO : EPOCH 3 - PROGRESS: at 8.46% examples, 92686 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:15,376 : INFO : EPOCH 3 - PROGRESS: at 10.18% examples, 92057 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:16,423 : INFO : EPOCH 3 - PROGRESS: at 12.08% examples, 93216 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:17,453 : INFO : EPOCH 3 - PROGRESS: at 13.69% examples, 93391 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:18,507 : INFO : EPOCH 3 - PROGRESS: at 15.34% examples, 93347 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:19,628 : INFO : EPOCH 3 - PROGRESS: at 17.00% examples, 93359 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:20,707 : INFO : EPOCH 3 - PROGRESS: at 18.94% examples, 93715 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:21,752 : INFO : EPOCH 3 - PROGRESS: at 20.80% examples, 94284 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:22,790 : INFO : EPOCH 3 - PROGRESS: at 22.53% examples, 94302 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:23,922 : INFO : EPOCH 3 - PROGRESS: at 24.37% examples, 94212 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:24,962 : INFO : EPOCH 3 - PROGRESS: at 26.13% examples, 94557 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:25,998 : INFO : EPOCH 3 - PROGRESS: at 27.95% examples, 94599 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:27,025 : INFO : EPOCH 3 - PROGRESS: at 29.60% examples, 94615 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:28,027 : INFO : EPOCH 3 - PROGRESS: at 31.25% examples, 94719 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:29,028 : INFO : EPOCH 3 - PROGRESS: at 32.79% examples, 94505 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:30,039 : INFO : EPOCH 3 - PROGRESS: at 34.68% examples, 94947 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:31,093 : INFO : EPOCH 3 - PROGRESS: at 36.41% examples, 94832 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-27 20:13:32,094 : INFO : EPOCH 3 - PROGRESS: at 38.09% examples, 94986 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:33,102 : INFO : EPOCH 3 - PROGRESS: at 39.68% examples, 94791 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:34,116 : INFO : EPOCH 3 - PROGRESS: at 41.40% examples, 94836 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:35,124 : INFO : EPOCH 3 - PROGRESS: at 43.15% examples, 94941 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:36,194 : INFO : EPOCH 3 - PROGRESS: at 44.88% examples, 94804 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-27 20:13:37,224 : INFO : EPOCH 3 - PROGRESS: at 46.69% examples, 95079 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:38,254 : INFO : EPOCH 3 - PROGRESS: at 48.32% examples, 94849 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:39,282 : INFO : EPOCH 3 - PROGRESS: at 50.20% examples, 95101 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:40,350 : INFO : EPOCH 3 - PROGRESS: at 51.82% examples, 94781 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:13:41,515 : INFO : EPOCH 3 - PROGRESS: at 53.77% examples, 94853 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:42,568 : INFO : EPOCH 3 - PROGRESS: at 55.79% examples, 95212 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:43,708 : INFO : EPOCH 3 - PROGRESS: at 57.41% examples, 94901 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:44,885 : INFO : EPOCH 3 - PROGRESS: at 59.30% examples, 94908 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:46,046 : INFO : EPOCH 3 - PROGRESS: at 61.28% examples, 94956 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:47,063 : INFO : EPOCH 3 - PROGRESS: at 63.03% examples, 94994 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:48,142 : INFO : EPOCH 3 - PROGRESS: at 64.70% examples, 94877 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:49,202 : INFO : EPOCH 3 - PROGRESS: at 66.49% examples, 94983 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:50,276 : INFO : EPOCH 3 - PROGRESS: at 68.25% examples, 94901 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:51,290 : INFO : EPOCH 3 - PROGRESS: at 69.98% examples, 94937 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:52,297 : INFO : EPOCH 3 - PROGRESS: at 71.58% examples, 94840 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:53,373 : INFO : EPOCH 3 - PROGRESS: at 73.25% examples, 94756 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:54,369 : INFO : EPOCH 3 - PROGRESS: at 74.99% examples, 94821 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:55,436 : INFO : EPOCH 3 - PROGRESS: at 76.75% examples, 94738 words/s, in_qsize 8, out_qsize 1\n",
            "2020-05-27 20:13:56,613 : INFO : EPOCH 3 - PROGRESS: at 78.73% examples, 94751 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:57,615 : INFO : EPOCH 3 - PROGRESS: at 80.51% examples, 94968 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:58,645 : INFO : EPOCH 3 - PROGRESS: at 82.13% examples, 94822 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:13:59,727 : INFO : EPOCH 3 - PROGRESS: at 84.06% examples, 94871 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:00,732 : INFO : EPOCH 3 - PROGRESS: at 85.75% examples, 94939 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:01,805 : INFO : EPOCH 3 - PROGRESS: at 87.41% examples, 94871 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:02,887 : INFO : EPOCH 3 - PROGRESS: at 89.27% examples, 94923 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-27 20:14:03,931 : INFO : EPOCH 3 - PROGRESS: at 90.76% examples, 94780 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-27 20:14:04,970 : INFO : EPOCH 3 - PROGRESS: at 92.58% examples, 94885 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:06,001 : INFO : EPOCH 3 - PROGRESS: at 94.23% examples, 94764 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:07,166 : INFO : EPOCH 3 - PROGRESS: at 96.27% examples, 94813 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:08,361 : INFO : EPOCH 3 - PROGRESS: at 98.21% examples, 94770 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:09,214 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-27 20:14:09,215 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-27 20:14:09,229 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-27 20:14:09,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-27 20:14:09,237 : INFO : EPOCH - 3 : training on 8035381 raw words (5722674 effective words) took 60.2s, 95109 effective words/s\n",
            "2020-05-27 20:14:10,379 : INFO : EPOCH 4 - PROGRESS: at 1.55% examples, 80520 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:11,519 : INFO : EPOCH 4 - PROGRESS: at 3.63% examples, 89646 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:12,559 : INFO : EPOCH 4 - PROGRESS: at 5.49% examples, 93024 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:13,604 : INFO : EPOCH 4 - PROGRESS: at 7.09% examples, 91557 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:14,667 : INFO : EPOCH 4 - PROGRESS: at 8.82% examples, 92928 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:15,682 : INFO : EPOCH 4 - PROGRESS: at 10.56% examples, 93471 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:16,798 : INFO : EPOCH 4 - PROGRESS: at 12.34% examples, 92626 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-27 20:14:17,846 : INFO : EPOCH 4 - PROGRESS: at 14.04% examples, 93491 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:18,899 : INFO : EPOCH 4 - PROGRESS: at 15.55% examples, 92612 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:19,915 : INFO : EPOCH 4 - PROGRESS: at 17.23% examples, 93698 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:21,008 : INFO : EPOCH 4 - PROGRESS: at 19.07% examples, 93310 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:22,167 : INFO : EPOCH 4 - PROGRESS: at 21.04% examples, 93649 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:23,341 : INFO : EPOCH 4 - PROGRESS: at 23.04% examples, 93772 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:24,352 : INFO : EPOCH 4 - PROGRESS: at 24.85% examples, 94454 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:25,405 : INFO : EPOCH 4 - PROGRESS: at 26.38% examples, 93864 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:26,435 : INFO : EPOCH 4 - PROGRESS: at 28.33% examples, 94383 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:27,464 : INFO : EPOCH 4 - PROGRESS: at 29.85% examples, 93984 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:28,495 : INFO : EPOCH 4 - PROGRESS: at 31.50% examples, 93979 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:29,531 : INFO : EPOCH 4 - PROGRESS: at 33.18% examples, 93993 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:30,734 : INFO : EPOCH 4 - PROGRESS: at 35.20% examples, 93923 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:14:31,734 : INFO : EPOCH 4 - PROGRESS: at 37.02% examples, 94414 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:32,762 : INFO : EPOCH 4 - PROGRESS: at 38.61% examples, 94176 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:33,847 : INFO : EPOCH 4 - PROGRESS: at 40.41% examples, 94269 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:35,016 : INFO : EPOCH 4 - PROGRESS: at 42.40% examples, 94338 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:36,231 : INFO : EPOCH 4 - PROGRESS: at 44.36% examples, 94251 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-27 20:14:37,374 : INFO : EPOCH 4 - PROGRESS: at 46.33% examples, 94405 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:38,419 : INFO : EPOCH 4 - PROGRESS: at 48.19% examples, 94653 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:39,430 : INFO : EPOCH 4 - PROGRESS: at 49.95% examples, 94710 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:40,489 : INFO : EPOCH 4 - PROGRESS: at 51.69% examples, 94654 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:41,544 : INFO : EPOCH 4 - PROGRESS: at 53.52% examples, 94831 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:42,568 : INFO : EPOCH 4 - PROGRESS: at 55.32% examples, 94851 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:43,720 : INFO : EPOCH 4 - PROGRESS: at 57.19% examples, 94940 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:44,816 : INFO : EPOCH 4 - PROGRESS: at 58.85% examples, 94754 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-27 20:14:45,853 : INFO : EPOCH 4 - PROGRESS: at 60.66% examples, 94951 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:46,857 : INFO : EPOCH 4 - PROGRESS: at 62.40% examples, 95015 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:47,963 : INFO : EPOCH 4 - PROGRESS: at 64.09% examples, 94843 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:49,004 : INFO : EPOCH 4 - PROGRESS: at 65.89% examples, 94990 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:50,010 : INFO : EPOCH 4 - PROGRESS: at 67.50% examples, 94897 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:51,025 : INFO : EPOCH 4 - PROGRESS: at 69.36% examples, 95095 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:52,053 : INFO : EPOCH 4 - PROGRESS: at 70.93% examples, 94958 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:53,068 : INFO : EPOCH 4 - PROGRESS: at 72.67% examples, 94992 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:54,114 : INFO : EPOCH 4 - PROGRESS: at 74.32% examples, 94956 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:55,261 : INFO : EPOCH 4 - PROGRESS: at 76.37% examples, 94995 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:56,397 : INFO : EPOCH 4 - PROGRESS: at 78.36% examples, 95091 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:57,423 : INFO : EPOCH 4 - PROGRESS: at 80.14% examples, 95249 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:58,423 : INFO : EPOCH 4 - PROGRESS: at 81.90% examples, 95301 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:14:59,447 : INFO : EPOCH 4 - PROGRESS: at 83.60% examples, 95163 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:15:00,516 : INFO : EPOCH 4 - PROGRESS: at 85.37% examples, 95245 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:01,518 : INFO : EPOCH 4 - PROGRESS: at 87.01% examples, 95298 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:02,543 : INFO : EPOCH 4 - PROGRESS: at 88.76% examples, 95310 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:03,584 : INFO : EPOCH 4 - PROGRESS: at 90.41% examples, 95291 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:15:04,607 : INFO : EPOCH 4 - PROGRESS: at 92.09% examples, 95296 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:05,613 : INFO : EPOCH 4 - PROGRESS: at 93.83% examples, 95330 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:06,670 : INFO : EPOCH 4 - PROGRESS: at 95.62% examples, 95291 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:07,677 : INFO : EPOCH 4 - PROGRESS: at 97.41% examples, 95326 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:08,810 : INFO : EPOCH 4 - PROGRESS: at 99.01% examples, 95141 words/s, in_qsize 7, out_qsize 1\n",
            "2020-05-27 20:15:09,110 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-27 20:15:09,126 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-27 20:15:09,156 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-27 20:15:09,200 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-27 20:15:09,201 : INFO : EPOCH - 4 : training on 8035381 raw words (5721832 effective words) took 60.0s, 95435 effective words/s\n",
            "2020-05-27 20:15:10,407 : INFO : EPOCH 5 - PROGRESS: at 1.53% examples, 76069 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:11,469 : INFO : EPOCH 5 - PROGRESS: at 3.63% examples, 90079 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:12,473 : INFO : EPOCH 5 - PROGRESS: at 5.37% examples, 92172 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-27 20:15:13,554 : INFO : EPOCH 5 - PROGRESS: at 7.09% examples, 91860 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:14,593 : INFO : EPOCH 5 - PROGRESS: at 8.72% examples, 92206 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:15,632 : INFO : EPOCH 5 - PROGRESS: at 10.44% examples, 92526 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:16,717 : INFO : EPOCH 5 - PROGRESS: at 12.34% examples, 93129 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:15:17,745 : INFO : EPOCH 5 - PROGRESS: at 13.94% examples, 93334 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:18,774 : INFO : EPOCH 5 - PROGRESS: at 15.55% examples, 93432 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:19,841 : INFO : EPOCH 5 - PROGRESS: at 17.11% examples, 93315 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:20,996 : INFO : EPOCH 5 - PROGRESS: at 19.17% examples, 93717 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:15:22,124 : INFO : EPOCH 5 - PROGRESS: at 21.18% examples, 94211 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:23,272 : INFO : EPOCH 5 - PROGRESS: at 23.17% examples, 94469 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:24,307 : INFO : EPOCH 5 - PROGRESS: at 24.98% examples, 94941 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:25,347 : INFO : EPOCH 5 - PROGRESS: at 26.51% examples, 94383 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:26,469 : INFO : EPOCH 5 - PROGRESS: at 28.60% examples, 94736 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:27,570 : INFO : EPOCH 5 - PROGRESS: at 30.42% examples, 95139 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:28,620 : INFO : EPOCH 5 - PROGRESS: at 31.94% examples, 94585 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:29,772 : INFO : EPOCH 5 - PROGRESS: at 33.90% examples, 94725 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:30,906 : INFO : EPOCH 5 - PROGRESS: at 35.92% examples, 94933 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:32,044 : INFO : EPOCH 5 - PROGRESS: at 37.88% examples, 95125 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:33,095 : INFO : EPOCH 5 - PROGRESS: at 39.68% examples, 95338 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:34,202 : INFO : EPOCH 5 - PROGRESS: at 41.41% examples, 95017 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:35,254 : INFO : EPOCH 5 - PROGRESS: at 43.28% examples, 95226 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:36,325 : INFO : EPOCH 5 - PROGRESS: at 45.14% examples, 95329 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:37,391 : INFO : EPOCH 5 - PROGRESS: at 46.96% examples, 95518 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:38,537 : INFO : EPOCH 5 - PROGRESS: at 48.66% examples, 95082 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:39,547 : INFO : EPOCH 5 - PROGRESS: at 50.58% examples, 95393 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:40,572 : INFO : EPOCH 5 - PROGRESS: at 52.19% examples, 95200 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:41,688 : INFO : EPOCH 5 - PROGRESS: at 54.16% examples, 95403 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:42,802 : INFO : EPOCH 5 - PROGRESS: at 56.02% examples, 95347 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:43,969 : INFO : EPOCH 5 - PROGRESS: at 57.91% examples, 95360 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:44,987 : INFO : EPOCH 5 - PROGRESS: at 59.70% examples, 95575 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:46,007 : INFO : EPOCH 5 - PROGRESS: at 61.28% examples, 95402 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:47,024 : INFO : EPOCH 5 - PROGRESS: at 63.15% examples, 95604 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:48,130 : INFO : EPOCH 5 - PROGRESS: at 64.80% examples, 95400 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:49,152 : INFO : EPOCH 5 - PROGRESS: at 66.62% examples, 95591 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:50,213 : INFO : EPOCH 5 - PROGRESS: at 68.36% examples, 95520 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:51,258 : INFO : EPOCH 5 - PROGRESS: at 70.23% examples, 95654 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:52,294 : INFO : EPOCH 5 - PROGRESS: at 72.06% examples, 95791 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:53,396 : INFO : EPOCH 5 - PROGRESS: at 73.71% examples, 95625 words/s, in_qsize 8, out_qsize 0\n",
            "2020-05-27 20:15:54,567 : INFO : EPOCH 5 - PROGRESS: at 75.88% examples, 95747 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:55,761 : INFO : EPOCH 5 - PROGRESS: at 78.01% examples, 95855 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:56,791 : INFO : EPOCH 5 - PROGRESS: at 79.79% examples, 95999 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:57,800 : INFO : EPOCH 5 - PROGRESS: at 81.54% examples, 96015 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:15:58,906 : INFO : EPOCH 5 - PROGRESS: at 83.47% examples, 95995 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:16:00,036 : INFO : EPOCH 5 - PROGRESS: at 85.37% examples, 96071 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:16:01,049 : INFO : EPOCH 5 - PROGRESS: at 87.13% examples, 96224 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:16:02,068 : INFO : EPOCH 5 - PROGRESS: at 88.76% examples, 96098 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:16:03,224 : INFO : EPOCH 5 - PROGRESS: at 90.64% examples, 96118 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:16:04,326 : INFO : EPOCH 5 - PROGRESS: at 92.46% examples, 96092 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:16:05,487 : INFO : EPOCH 5 - PROGRESS: at 94.48% examples, 96102 words/s, in_qsize 6, out_qsize 1\n",
            "2020-05-27 20:16:06,612 : INFO : EPOCH 5 - PROGRESS: at 96.65% examples, 96297 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:16:07,660 : INFO : EPOCH 5 - PROGRESS: at 98.46% examples, 96361 words/s, in_qsize 7, out_qsize 0\n",
            "2020-05-27 20:16:08,420 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-05-27 20:16:08,428 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-05-27 20:16:08,464 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-05-27 20:16:08,477 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-05-27 20:16:08,478 : INFO : EPOCH - 5 : training on 8035381 raw words (5721985 effective words) took 59.3s, 96539 effective words/s\n",
            "2020-05-27 20:16:08,479 : INFO : training on a 40176905 raw words (28608838 effective words) took 302.2s, 94667 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9min 57s, sys: 765 ms, total: 9min 57s\n",
            "Wall time: 5min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QUZHFsj8RaGo",
        "colab": {}
      },
      "source": [
        "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    \n",
        "    def average_word_vectors(words, model, vocabulary, num_features):\n",
        "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "        nwords = 0.\n",
        "        \n",
        "        for word in words:\n",
        "            if word in vocabulary: \n",
        "                nwords = nwords + 1.\n",
        "                feature_vector = np.add(feature_vector, model.wv[word])\n",
        "        if nwords:\n",
        "            feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "        return feature_vector\n",
        "\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zWfcUVixRaGr",
        "colab": {}
      },
      "source": [
        "# generate averaged word vector features from word2vec model\n",
        "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
        "                                                     num_features=w2v_num_features)\n",
        "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
        "                                                    num_features=w2v_num_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3GMe1dCfRaGw",
        "outputId": "fad86e75-952d-4562-b565-0494008bc1f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec model:> Train features shape: (35000, 300)  Test features shape: (15000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8mSFQ9H0RaGy"
      },
      "source": [
        "## Modeling with deep neural networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-oz9gtU9RaGz"
      },
      "source": [
        "### Building Deep neural network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eCI0uEBIRaGz",
        "colab": {}
      },
      "source": [
        "def construct_deepnn_architecture(num_input_features):\n",
        "    dnn_model = Sequential()\n",
        "    dnn_model.add(Dense(512, input_shape=(num_input_features,)))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(1))\n",
        "    dnn_model.add(Activation('sigmoid'))\n",
        "\n",
        "    dnn_model.compile(loss='binary_crossentropy', optimizer='adam',                 \n",
        "                      metrics=['accuracy'])\n",
        "    return dnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3cQpTH6FRaG1",
        "colab": {}
      },
      "source": [
        "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GBzbo-YlRaG4"
      },
      "source": [
        "### Visualize sample deep architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0fuhzYagRaG5",
        "outputId": "be17e233-c942-44c0-8535-57f5b96f31a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "w2v_dnn.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               154112    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 351,489\n",
            "Trainable params: 351,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s-COslmPRaG8"
      },
      "source": [
        "### Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0kcisAskRaG8",
        "outputId": "615e99c1-89c4-41af-a75e-0e4955d7b74d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "batch_size = 100\n",
        "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=10, batch_size=batch_size, \n",
        "            shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "315/315 [==============================] - 1s 3ms/step - loss: 0.3284 - accuracy: 0.8604 - val_loss: 0.3066 - val_accuracy: 0.8740\n",
            "Epoch 2/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2953 - accuracy: 0.8776 - val_loss: 0.3073 - val_accuracy: 0.8749\n",
            "Epoch 3/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2891 - accuracy: 0.8809 - val_loss: 0.2934 - val_accuracy: 0.8763\n",
            "Epoch 4/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2811 - accuracy: 0.8837 - val_loss: 0.2935 - val_accuracy: 0.8783\n",
            "Epoch 5/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2722 - accuracy: 0.8882 - val_loss: 0.2906 - val_accuracy: 0.8766\n",
            "Epoch 6/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2670 - accuracy: 0.8895 - val_loss: 0.2912 - val_accuracy: 0.8803\n",
            "Epoch 7/10\n",
            "315/315 [==============================] - 1s 3ms/step - loss: 0.2571 - accuracy: 0.8935 - val_loss: 0.2992 - val_accuracy: 0.8800\n",
            "Epoch 8/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2496 - accuracy: 0.8949 - val_loss: 0.3004 - val_accuracy: 0.8749\n",
            "Epoch 9/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2415 - accuracy: 0.9008 - val_loss: 0.3003 - val_accuracy: 0.8757\n",
            "Epoch 10/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2337 - accuracy: 0.9046 - val_loss: 0.3143 - val_accuracy: 0.8771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffaded8e5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7C1gcc4iRaG-",
        "outputId": "cf997aaf-7308-4869-dd12-fff7609a4aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
        "predictions = le.inverse_transform(y_pred) "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-30-bf19a67cc778>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-27 20:16:59,285 : WARNING : From <ipython-input-30-bf19a67cc778>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:289: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2XHEZ-X9RaG_",
        "outputId": "406934f3-c113-4825-b6bd-246227a0ea4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, predictions), index=labels, columns=labels)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.90      0.88      7490\n",
            "    positive       0.89      0.86      0.87      7510\n",
            "\n",
            "    accuracy                           0.88     15000\n",
            "   macro avg       0.88      0.88      0.88     15000\n",
            "weighted avg       0.88      0.88      0.88     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6709</td>\n",
              "      <td>781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1062</td>\n",
              "      <td>6448</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6709       781\n",
              "positive      1062      6448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}