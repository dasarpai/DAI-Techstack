{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Autograd.ipynb","provenance":[{"file_id":"1DUhYf0xbFtpN7EwdYNPqxhEmFDTw9p68","timestamp":1615011883197}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"dq4aMookzPhu"},"source":["import torch\r\n","# The autograd package provides automatic differentiation \r\n","# for all operations on Tensors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rfoCAIlzXb7"},"source":["# requires_grad = True -> tracks all operations on the tensor. \r\n","x = torch.randn(3, requires_grad=True)\r\n","y = x + 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1tiKS4wGx0Ye","executionInfo":{"status":"ok","timestamp":1615022722874,"user_tz":-330,"elapsed":1227,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"a3681994-8562-41c8-88b7-a5e39adb3b2b"},"source":["print (x)\r\n","\r\n","a = y**3\r\n","b = torch.sqrt(a)\r\n","z= torch.exp(b)\r\n","z= z.sum()\r\n","z.backward()\r\n","print(x.grad)\r\n","\r\n","a = y**3\r\n","b = torch.sqrt(a)\r\n","z= torch.exp(b)\r\n","z= z.mean()\r\n","z.backward()\r\n","print(x.grad)\r\n","\r\n","a = y**3\r\n","b = torch.sqrt(a)\r\n","z= torch.exp(b)\r\n","z= z.sum()\r\n","z.backward()\r\n","print(x.grad)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 0.1609,  0.3905, -0.7961], requires_grad=True)\n","tensor([331.4698, 523.3245,  72.8545])\n","tensor([349.0830, 554.4695,  74.9101])\n","tensor([401.9225, 647.9048,  81.0769])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UOA30VlzZ0o","executionInfo":{"status":"ok","timestamp":1615021476799,"user_tz":-330,"elapsed":1873,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"0426734c-bee2-4b56-96d1-67adb6825907"},"source":["# y was created as a result of an operation, so it has a grad_fn attribute.\r\n","# grad_fn: references a Function that has created the Tensor\r\n","print(x) # created by the user -> grad_fn is None\r\n","print(y)\r\n","print(y.grad_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 0.1609,  0.3905, -0.7961], requires_grad=True)\n","tensor([2.1609, 2.3905, 1.2039], grad_fn=<AddBackward0>)\n","<AddBackward0 object at 0x7fe8a798aa10>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3utBBa-vyBK2","executionInfo":{"status":"ok","timestamp":1615022027227,"user_tz":-330,"elapsed":1236,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"cd76f3df-b3b6-453f-eb6a-4ea0e4ebf166"},"source":["z = y * y * 3\r\n","z= z.sum()\r\n","z.backward()\r\n","print(x.grad)\r\n","\r\n","z = y * y * 3\r\n","z= z.mean()\r\n","z.backward()\r\n","print(x.grad)\r\n","\r\n","z = y * y * 3\r\n","z= z.sum()\r\n","z.backward()\r\n","print(x.grad)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([56.1835, 62.1534, 31.3011])\n","tensor([60.5053, 66.9345, 33.7089])\n","tensor([73.4707, 81.2776, 40.9322])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgvitzbCzb8S","executionInfo":{"status":"ok","timestamp":1615021571846,"user_tz":-330,"elapsed":1129,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"b8a6b0f9-3694-442e-8d26-14ddc7aaa154"},"source":["# Do more operations on y\r\n","z = y * y * 3\r\n","print(z)\r\n","z = z.mean()\r\n","print(type(z))\r\n","print(z)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([14.0085, 17.1437,  4.3480], grad_fn=<MulBackward0>)\n","<class 'torch.Tensor'>\n","tensor(11.8334, grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMWK-ohxzeVq","executionInfo":{"status":"ok","timestamp":1615005837688,"user_tz":-330,"elapsed":4104,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"b0c490ad-047c-450f-8be8-8634a442f68c"},"source":["# Let's compute the gradients with backpropagation\r\n","# When we finish our computation we can call .backward() and have all the gradients computed automatically.\r\n","# The gradient for this tensor will be accumulated into .grad attribute.\r\n","# It is the partial derivate of the function w.r.t. the tensor\r\n","\r\n","z.backward()\r\n","print(x.grad) # dz/dx\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([5.8708, 7.6800, 4.9293])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DTt3sfjazg94"},"source":["# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\r\n","# It computes partial derivates while applying the chain rule\r\n","\r\n","# -------------\r\n","# Model with non-scalar output:\r\n","# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \r\n","# specify a gradient argument that is a tensor of matching shape.\r\n","# needed for vector-Jacobian product\r\n","\r\n","x = torch.randn(3, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfryC0rnzpNj","executionInfo":{"status":"ok","timestamp":1615005837692,"user_tz":-330,"elapsed":4092,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"275c2028-57b9-4863-982c-2b3249d5d241"},"source":["y = x * 2\r\n","for _ in range(10):\r\n","    y = y * 2\r\n","\r\n","print(y)\r\n","print(y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([-1087.4006,  1971.4565,  4006.4727], grad_fn=<MulBackward0>)\n","torch.Size([3])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRbT1Rl5zsQd","executionInfo":{"status":"ok","timestamp":1615005837695,"user_tz":-330,"elapsed":4087,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"f291b7cd-f0f1-4c5a-ca01-8700da8c2b67"},"source":["v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\r\n","y.backward(v)\r\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rzJQHPETzvO7","executionInfo":{"status":"ok","timestamp":1615022976928,"user_tz":-330,"elapsed":5781,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"4a311427-7a7d-465e-b7ca-cba827a5b76c"},"source":["# -------------\r\n","# Stop a tensor from tracking history:\r\n","# For example during our training loop when we want to update our weights\r\n","# then this update operation should not be part of the gradient computation\r\n","# - x.requires_grad_(False)\r\n","# - x.detach()\r\n","# - wrap in 'with torch.no_grad():'\r\n","\r\n","# .requires_grad_(...) changes an existing flag in-place.\r\n","a = torch.randn(2, 2)\r\n","print (a)\r\n","print(a.requires_grad)\r\n","b = ((a * 3) / (a - 1))\r\n","print (b)\r\n","print(b.grad_fn)\r\n","print (b.grad)\r\n","a.requires_grad_(True)\r\n","print(a.requires_grad)\r\n","b = (a * a).sum()\r\n","print(b.grad_fn)\r\n","print(b)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0.7428, 0.4621],\n","        [0.1757, 1.4226]])\n","False\n","tensor([[-8.6631, -2.5775],\n","        [-0.6395, 10.0996]])\n","None\n","None\n","True\n","<SumBackward0 object at 0x7fe85607d750>\n","tensor(2.8198, grad_fn=<SumBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6nmvxm1zy4r","executionInfo":{"status":"ok","timestamp":1615005837699,"user_tz":-330,"elapsed":4076,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"26a66d96-a8a7-43c9-d936-ee076548afa7"},"source":["\r\n","# .detach(): get a new Tensor with the same content but no gradient computation:\r\n","a = torch.randn(2, 2, requires_grad=True)\r\n","print(a.requires_grad)\r\n","b = a.detach()\r\n","print(b.requires_grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n","False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkAQq2pmz2Ch","executionInfo":{"status":"ok","timestamp":1615005837701,"user_tz":-330,"elapsed":4071,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"5c023235-fa28-41d4-e5cb-d2c82f1b3053"},"source":["# wrap in 'with torch.no_grad():'\r\n","a = torch.randn(2, 2, requires_grad=True)\r\n","print(a.requires_grad)\r\n","with torch.no_grad():\r\n","    print((x ** 2).requires_grad)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n","False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jr6DPOa4z40K"},"source":["# -------------\r\n","# backward() accumulates the gradient for this tensor into .grad attribute.\r\n","# !!! We need to be careful during optimization !!!\r\n","# Use .zero_() to empty the gradients before a new optimization step!\r\n","weights = torch.ones(4, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ur0jkLD1zj2K","executionInfo":{"status":"ok","timestamp":1615005837705,"user_tz":-330,"elapsed":4061,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"a0dfcc72-9fb9-41db-e1de-0b04d1df045a"},"source":["\r\n","for epoch in range(3):\r\n","    # just a dummy example\r\n","    model_output = (weights*3).sum()\r\n","    model_output.backward()\r\n","    \r\n","    print(weights.grad)\r\n","\r\n","    # optimize model, i.e. adjust weights...\r\n","    with torch.no_grad():\r\n","        weights -= 0.1 * weights.grad\r\n","\r\n","    # this is important! It affects the final weights & output\r\n","    weights.grad.zero_()\r\n","\r\n","print(weights)\r\n","print(model_output)\r\n","\r\n","# Optimizer has zero_grad() method\r\n","# optimizer = torch.optim.SGD([weights], lr=0.1)\r\n","# During training:\r\n","# optimizer.step()\r\n","# optimizer.zero_grad()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n","tensor(4.8000, grad_fn=<SumBackward0>)\n"],"name":"stdout"}]}]}