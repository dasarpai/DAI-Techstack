{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Autograd-org.ipynb","provenance":[{"file_id":"1DUhYf0xbFtpN7EwdYNPqxhEmFDTw9p68","timestamp":1615012028151}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"dq4aMookzPhu"},"source":["import torch\r\n","# The autograd package provides automatic differentiation \r\n","# for all operations on Tensors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rfoCAIlzXb7"},"source":["# requires_grad = True -> tracks all operations on the tensor. \r\n","x = torch.randn(3, requires_grad=True)\r\n","y = x + 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwPuVQmcO38e","executionInfo":{"status":"ok","timestamp":1615012271537,"user_tz":-330,"elapsed":697,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"16540e76-e4f0-4673-eb64-ef58d42ed76f"},"source":["x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.4640, 0.8332, 0.8537], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UOA30VlzZ0o","executionInfo":{"status":"ok","timestamp":1615012254737,"user_tz":-330,"elapsed":1203,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"d4115ff6-0d63-4fe7-9131-db1ac8c1a519"},"source":["# y was created as a result of an operation, so it has a grad_fn attribute.\r\n","# grad_fn: references a Function that has created the Tensor\r\n","print(x) # created by the user -> grad_fn is None\r\n","print(y)\r\n","print(y.grad_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0.4640, 0.8332, 0.8537], requires_grad=True)\n","tensor([2.4640, 2.8332, 2.8537], grad_fn=<AddBackward0>)\n","<AddBackward0 object at 0x7fececec3c50>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgvitzbCzb8S","executionInfo":{"status":"ok","timestamp":1615012417564,"user_tz":-330,"elapsed":1698,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"f25deaa9-9ba3-48fc-b9fc-1b848fb85310"},"source":["# Do more operations on y\r\n","z = y * y * 3\r\n","print(z)\r\n","z = z.mean()\r\n","print(z)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([18.2137, 24.0805, 24.4310], grad_fn=<MulBackward0>)\n","tensor(22.2418, grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMWK-ohxzeVq","executionInfo":{"status":"ok","timestamp":1615012424202,"user_tz":-330,"elapsed":1454,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"637355f8-36cf-4e47-81e4-9fca03ca7322"},"source":["# Let's compute the gradients with backpropagation\r\n","# When we finish our computation we can call .backward() and have all the gradients computed automatically.\r\n","# The gradient for this tensor will be accumulated into .grad attribute.\r\n","# It is the partial derivate of the function w.r.t. the tensor\r\n","\r\n","z.backward()\r\n","print(x.grad) # dz/dx\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([4.9280, 5.6663, 5.7074])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DTt3sfjazg94"},"source":["# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\r\n","# It computes partial derivates while applying the chain rule\r\n","\r\n","# -------------\r\n","# Model with non-scalar output:\r\n","# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \r\n","# specify a gradient argument that is a tensor of matching shape.\r\n","# needed for vector-Jacobian product\r\n","\r\n","x = torch.randn(3, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfryC0rnzpNj","executionInfo":{"status":"ok","timestamp":1615005837692,"user_tz":-330,"elapsed":4092,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"275c2028-57b9-4863-982c-2b3249d5d241"},"source":["y = x * 2\r\n","for _ in range(10):\r\n","    y = y * 2\r\n","\r\n","print(y)\r\n","print(y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([-1087.4006,  1971.4565,  4006.4727], grad_fn=<MulBackward0>)\n","torch.Size([3])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRbT1Rl5zsQd","executionInfo":{"status":"ok","timestamp":1615005837695,"user_tz":-330,"elapsed":4087,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"f291b7cd-f0f1-4c5a-ca01-8700da8c2b67"},"source":["v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\r\n","y.backward(v)\r\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rzJQHPETzvO7","executionInfo":{"status":"ok","timestamp":1615005837697,"user_tz":-330,"elapsed":4082,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"5da99ab5-cd82-4408-871a-ddf848ad2cbd"},"source":["# -------------\r\n","# Stop a tensor from tracking history:\r\n","# For example during our training loop when we want to update our weights\r\n","# then this update operation should not be part of the gradient computation\r\n","# - x.requires_grad_(False)\r\n","# - x.detach()\r\n","# - wrap in 'with torch.no_grad():'\r\n","\r\n","# .requires_grad_(...) changes an existing flag in-place.\r\n","a = torch.randn(2, 2)\r\n","print(a.requires_grad)\r\n","b = ((a * 3) / (a - 1))\r\n","print(b.grad_fn)\r\n","a.requires_grad_(True)\r\n","print(a.requires_grad)\r\n","b = (a * a).sum()\r\n","print(b.grad_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["False\n","None\n","True\n","<SumBackward0 object at 0x7fde6e666910>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H6nmvxm1zy4r","executionInfo":{"status":"ok","timestamp":1615005837699,"user_tz":-330,"elapsed":4076,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"26a66d96-a8a7-43c9-d936-ee076548afa7"},"source":["\r\n","# .detach(): get a new Tensor with the same content but no gradient computation:\r\n","a = torch.randn(2, 2, requires_grad=True)\r\n","print(a.requires_grad)\r\n","b = a.detach()\r\n","print(b.requires_grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n","False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkAQq2pmz2Ch","executionInfo":{"status":"ok","timestamp":1615005837701,"user_tz":-330,"elapsed":4071,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"5c023235-fa28-41d4-e5cb-d2c82f1b3053"},"source":["# wrap in 'with torch.no_grad():'\r\n","a = torch.randn(2, 2, requires_grad=True)\r\n","print(a.requires_grad)\r\n","with torch.no_grad():\r\n","    print((x ** 2).requires_grad)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n","False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jr6DPOa4z40K"},"source":["# -------------\r\n","# backward() accumulates the gradient for this tensor into .grad attribute.\r\n","# !!! We need to be careful during optimization !!!\r\n","# Use .zero_() to empty the gradients before a new optimization step!\r\n","weights = torch.ones(4, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ur0jkLD1zj2K","executionInfo":{"status":"ok","timestamp":1615005837705,"user_tz":-330,"elapsed":4061,"user":{"displayName":"usha rengaraju","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilW5hFfGX88ie8T8j8_ZSx4mUGdCwmVpf7vWSKfA=s64","userId":"04805119296002868129"}},"outputId":"a0dfcc72-9fb9-41db-e1de-0b04d1df045a"},"source":["\r\n","for epoch in range(3):\r\n","    # just a dummy example\r\n","    model_output = (weights*3).sum()\r\n","    model_output.backward()\r\n","    \r\n","    print(weights.grad)\r\n","\r\n","    # optimize model, i.e. adjust weights...\r\n","    with torch.no_grad():\r\n","        weights -= 0.1 * weights.grad\r\n","\r\n","    # this is important! It affects the final weights & output\r\n","    weights.grad.zero_()\r\n","\r\n","print(weights)\r\n","print(model_output)\r\n","\r\n","# Optimizer has zero_grad() method\r\n","# optimizer = torch.optim.SGD([weights], lr=0.1)\r\n","# During training:\r\n","# optimizer.step()\r\n","# optimizer.zero_grad()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n","tensor(4.8000, grad_fn=<SumBackward0>)\n"],"name":"stdout"}]}]}