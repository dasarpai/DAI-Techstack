{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/howto/corpus.html#corpus-reader-objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ieer to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\ieer.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"ieer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CategorizedTaggedCorpusReader in '.../corpora/brown' (not loaded yet)>\n",
      "<BracketParseCorpusReader in '.../corpora/treebank/combined' (not loaded yet)>\n",
      "<WordListCorpusReader in '.../corpora/names' (not loaded yet)>\n",
      "<PlaintextCorpusReader in '.../corpora/inaugural' (not loaded yet)>\n"
     ]
    }
   ],
   "source": [
    "import nltk.corpus\n",
    "# The Brown corpus:\n",
    "print(str(nltk.corpus.brown).replace('\\\\\\\\','/'))\n",
    "#<CategorizedTaggedCorpusReader in '.../corpora/brown'...>\n",
    "# The Penn Treebank Corpus:\n",
    "print(str(nltk.corpus.treebank).replace('\\\\\\\\','/'))\n",
    "#<BracketParseCorpusReader in '.../corpora/treebank/combined'...>\n",
    "# The Name Genders Corpus:\n",
    "print(str(nltk.corpus.names).replace('\\\\\\\\','/'))\n",
    "#<WordListCorpusReader in '.../corpora/names'...>\n",
    "# The Inaugural Address Corpus:\n",
    "print(str(nltk.corpus.inaugural).replace('\\\\\\\\','/'))\n",
    "#<PlaintextCorpusReader in '.../corpora/inaugural'...>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wsj_0001.mrg',\n",
       " 'wsj_0002.mrg',\n",
       " 'wsj_0003.mrg',\n",
       " 'wsj_0004.mrg',\n",
       " 'wsj_0005.mrg',\n",
       " 'wsj_0006.mrg',\n",
       " 'wsj_0007.mrg',\n",
       " 'wsj_0008.mrg',\n",
       " 'wsj_0009.mrg',\n",
       " 'wsj_0010.mrg',\n",
       " 'wsj_0011.mrg',\n",
       " 'wsj_0012.mrg',\n",
       " 'wsj_0013.mrg',\n",
       " 'wsj_0014.mrg',\n",
       " 'wsj_0015.mrg',\n",
       " 'wsj_0016.mrg',\n",
       " 'wsj_0017.mrg',\n",
       " 'wsj_0018.mrg',\n",
       " 'wsj_0019.mrg',\n",
       " 'wsj_0020.mrg',\n",
       " 'wsj_0021.mrg',\n",
       " 'wsj_0022.mrg',\n",
       " 'wsj_0023.mrg',\n",
       " 'wsj_0024.mrg',\n",
       " 'wsj_0025.mrg',\n",
       " 'wsj_0026.mrg',\n",
       " 'wsj_0027.mrg',\n",
       " 'wsj_0028.mrg',\n",
       " 'wsj_0029.mrg',\n",
       " 'wsj_0030.mrg',\n",
       " 'wsj_0031.mrg',\n",
       " 'wsj_0032.mrg',\n",
       " 'wsj_0033.mrg',\n",
       " 'wsj_0034.mrg',\n",
       " 'wsj_0035.mrg',\n",
       " 'wsj_0036.mrg',\n",
       " 'wsj_0037.mrg',\n",
       " 'wsj_0038.mrg',\n",
       " 'wsj_0039.mrg',\n",
       " 'wsj_0040.mrg',\n",
       " 'wsj_0041.mrg',\n",
       " 'wsj_0042.mrg',\n",
       " 'wsj_0043.mrg',\n",
       " 'wsj_0044.mrg',\n",
       " 'wsj_0045.mrg',\n",
       " 'wsj_0046.mrg',\n",
       " 'wsj_0047.mrg',\n",
       " 'wsj_0048.mrg',\n",
       " 'wsj_0049.mrg',\n",
       " 'wsj_0050.mrg',\n",
       " 'wsj_0051.mrg',\n",
       " 'wsj_0052.mrg',\n",
       " 'wsj_0053.mrg',\n",
       " 'wsj_0054.mrg',\n",
       " 'wsj_0055.mrg',\n",
       " 'wsj_0056.mrg',\n",
       " 'wsj_0057.mrg',\n",
       " 'wsj_0058.mrg',\n",
       " 'wsj_0059.mrg',\n",
       " 'wsj_0060.mrg',\n",
       " 'wsj_0061.mrg',\n",
       " 'wsj_0062.mrg',\n",
       " 'wsj_0063.mrg',\n",
       " 'wsj_0064.mrg',\n",
       " 'wsj_0065.mrg',\n",
       " 'wsj_0066.mrg',\n",
       " 'wsj_0067.mrg',\n",
       " 'wsj_0068.mrg',\n",
       " 'wsj_0069.mrg',\n",
       " 'wsj_0070.mrg',\n",
       " 'wsj_0071.mrg',\n",
       " 'wsj_0072.mrg',\n",
       " 'wsj_0073.mrg',\n",
       " 'wsj_0074.mrg',\n",
       " 'wsj_0075.mrg',\n",
       " 'wsj_0076.mrg',\n",
       " 'wsj_0077.mrg',\n",
       " 'wsj_0078.mrg',\n",
       " 'wsj_0079.mrg',\n",
       " 'wsj_0080.mrg',\n",
       " 'wsj_0081.mrg',\n",
       " 'wsj_0082.mrg',\n",
       " 'wsj_0083.mrg',\n",
       " 'wsj_0084.mrg',\n",
       " 'wsj_0085.mrg',\n",
       " 'wsj_0086.mrg',\n",
       " 'wsj_0087.mrg',\n",
       " 'wsj_0088.mrg',\n",
       " 'wsj_0089.mrg',\n",
       " 'wsj_0090.mrg',\n",
       " 'wsj_0091.mrg',\n",
       " 'wsj_0092.mrg',\n",
       " 'wsj_0093.mrg',\n",
       " 'wsj_0094.mrg',\n",
       " 'wsj_0095.mrg',\n",
       " 'wsj_0096.mrg',\n",
       " 'wsj_0097.mrg',\n",
       " 'wsj_0098.mrg',\n",
       " 'wsj_0099.mrg',\n",
       " 'wsj_0100.mrg',\n",
       " 'wsj_0101.mrg',\n",
       " 'wsj_0102.mrg',\n",
       " 'wsj_0103.mrg',\n",
       " 'wsj_0104.mrg',\n",
       " 'wsj_0105.mrg',\n",
       " 'wsj_0106.mrg',\n",
       " 'wsj_0107.mrg',\n",
       " 'wsj_0108.mrg',\n",
       " 'wsj_0109.mrg',\n",
       " 'wsj_0110.mrg',\n",
       " 'wsj_0111.mrg',\n",
       " 'wsj_0112.mrg',\n",
       " 'wsj_0113.mrg',\n",
       " 'wsj_0114.mrg',\n",
       " 'wsj_0115.mrg',\n",
       " 'wsj_0116.mrg',\n",
       " 'wsj_0117.mrg',\n",
       " 'wsj_0118.mrg',\n",
       " 'wsj_0119.mrg',\n",
       " 'wsj_0120.mrg',\n",
       " 'wsj_0121.mrg',\n",
       " 'wsj_0122.mrg',\n",
       " 'wsj_0123.mrg',\n",
       " 'wsj_0124.mrg',\n",
       " 'wsj_0125.mrg',\n",
       " 'wsj_0126.mrg',\n",
       " 'wsj_0127.mrg',\n",
       " 'wsj_0128.mrg',\n",
       " 'wsj_0129.mrg',\n",
       " 'wsj_0130.mrg',\n",
       " 'wsj_0131.mrg',\n",
       " 'wsj_0132.mrg',\n",
       " 'wsj_0133.mrg',\n",
       " 'wsj_0134.mrg',\n",
       " 'wsj_0135.mrg',\n",
       " 'wsj_0136.mrg',\n",
       " 'wsj_0137.mrg',\n",
       " 'wsj_0138.mrg',\n",
       " 'wsj_0139.mrg',\n",
       " 'wsj_0140.mrg',\n",
       " 'wsj_0141.mrg',\n",
       " 'wsj_0142.mrg',\n",
       " 'wsj_0143.mrg',\n",
       " 'wsj_0144.mrg',\n",
       " 'wsj_0145.mrg',\n",
       " 'wsj_0146.mrg',\n",
       " 'wsj_0147.mrg',\n",
       " 'wsj_0148.mrg',\n",
       " 'wsj_0149.mrg',\n",
       " 'wsj_0150.mrg',\n",
       " 'wsj_0151.mrg',\n",
       " 'wsj_0152.mrg',\n",
       " 'wsj_0153.mrg',\n",
       " 'wsj_0154.mrg',\n",
       " 'wsj_0155.mrg',\n",
       " 'wsj_0156.mrg',\n",
       " 'wsj_0157.mrg',\n",
       " 'wsj_0158.mrg',\n",
       " 'wsj_0159.mrg',\n",
       " 'wsj_0160.mrg',\n",
       " 'wsj_0161.mrg',\n",
       " 'wsj_0162.mrg',\n",
       " 'wsj_0163.mrg',\n",
       " 'wsj_0164.mrg',\n",
       " 'wsj_0165.mrg',\n",
       " 'wsj_0166.mrg',\n",
       " 'wsj_0167.mrg',\n",
       " 'wsj_0168.mrg',\n",
       " 'wsj_0169.mrg',\n",
       " 'wsj_0170.mrg',\n",
       " 'wsj_0171.mrg',\n",
       " 'wsj_0172.mrg',\n",
       " 'wsj_0173.mrg',\n",
       " 'wsj_0174.mrg',\n",
       " 'wsj_0175.mrg',\n",
       " 'wsj_0176.mrg',\n",
       " 'wsj_0177.mrg',\n",
       " 'wsj_0178.mrg',\n",
       " 'wsj_0179.mrg',\n",
       " 'wsj_0180.mrg',\n",
       " 'wsj_0181.mrg',\n",
       " 'wsj_0182.mrg',\n",
       " 'wsj_0183.mrg',\n",
       " 'wsj_0184.mrg',\n",
       " 'wsj_0185.mrg',\n",
       " 'wsj_0186.mrg',\n",
       " 'wsj_0187.mrg',\n",
       " 'wsj_0188.mrg',\n",
       " 'wsj_0189.mrg',\n",
       " 'wsj_0190.mrg',\n",
       " 'wsj_0191.mrg',\n",
       " 'wsj_0192.mrg',\n",
       " 'wsj_0193.mrg',\n",
       " 'wsj_0194.mrg',\n",
       " 'wsj_0195.mrg',\n",
       " 'wsj_0196.mrg',\n",
       " 'wsj_0197.mrg',\n",
       " 'wsj_0198.mrg',\n",
       " 'wsj_0199.mrg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.treebank.fileids() # doctest: +ELLIPSIS\n",
    "#['wsj_0001.mrg', 'wsj_0002.mrg', 'wsj_0003.mrg', 'wsj_0004.mrg', ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt',\n",
       " '2013-Obama.txt',\n",
       " '2017-Trump.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.inaugural.fileids() # doctest: +ELLIPSIS\n",
    "#['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':']], [['Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ['On', 'the', 'one', 'hand', ',', 'I', 'was', 'summoned', 'by', 'my', 'Country', ',', 'whose', 'voice', 'I', 'can', 'never', 'hear', 'but', 'with', 'veneration', 'and', 'love', ',', 'from', 'a', 'retreat', 'which', 'I', 'had', 'chosen', 'with', 'the', 'fondest', 'predilection', ',', 'and', ',', 'in', 'my', 'flattering', 'hopes', ',', 'with', 'an', 'immutable', 'decision', ',', 'as', 'the', 'asylum', 'of', 'my', 'declining', 'years', '--', 'a', 'retreat', 'which', 'was', 'rendered', 'every', 'day', 'more', 'necessary', 'as', 'well', 'as', 'more', 'dear', 'to', 'me', 'by', 'the', 'addition', 'of', 'habit', 'to', 'inclination', ',', 'and', 'of', 'frequent', 'interruptions', 'in', 'my', 'health', 'to', 'the', 'gradual', 'waste', 'committed', 'on', 'it', 'by', 'time', '.'], ['On', 'the', 'other', 'hand', ',', 'the', 'magnitude', 'and', 'difficulty', 'of', 'the', 'trust', 'to', 'which', 'the', 'voice', 'of', 'my', 'country', 'called', 'me', ',', 'being', 'sufficient', 'to', 'awaken', 'in', 'the', 'wisest', 'and', 'most', 'experienced', 'of', 'her', 'citizens', 'a', 'distrustful', 'scrutiny', 'into', 'his', 'qualifications', ',', 'could', 'not', 'but', 'overwhelm', 'with', 'despondence', 'one', 'who', '(', 'inheriting', 'inferior', 'endowments', 'from', 'nature', 'and', 'unpracticed', 'in', 'the', 'duties', 'of', 'civil', 'administration', ')', 'ought', 'to', 'be', 'peculiarly', 'conscious', 'of', 'his', 'own', 'deficiencies', '.'], ['In', 'this', 'conflict', 'of', 'emotions', 'all', 'I', 'dare', 'aver', 'is', 'that', 'it', 'has', 'been', 'my', 'faithful', 'study', 'to', 'collect', 'my', 'duty', 'from', 'a', 'just', 'appreciation', 'of', 'every', 'circumstance', 'by', 'which', 'it', 'might', 'be', 'affected', '.'], ['All', 'I', 'dare', 'hope', 'is', 'that', 'if', ',', 'in', 'executing', 'this', 'task', ',', 'I', 'have', 'been', 'too', 'much', 'swayed', 'by', 'a', 'grateful', 'remembrance', 'of', 'former', 'instances', ',', 'or', 'by', 'an', 'affectionate', 'sensibility', 'to', 'this', 'transcendent', 'proof', 'of', 'the', 'confidence', 'of', 'my', 'fellow', 'citizens', ',', 'and', 'have', 'thence', 'too', 'little', 'consulted', 'my', 'incapacity', 'as', 'well', 'as', 'disinclination', 'for', 'the', 'weighty', 'and', 'untried', 'cares', 'before', 'me', ',', 'my', 'error', 'will', 'be', 'palliated', 'by', 'the', 'motives', 'which', 'mislead', 'me', ',', 'and', 'its', 'consequences', 'be', 'judged', 'by', 'my', 'country', 'with', 'some', 'share', 'of', 'the', 'partiality', 'in', 'which', 'they', 'originated', '.']], ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural.raw('1789-Washington.txt') # doctest: +ELLIPSIS\n",
    "#'Fellow-Citizens of the Senate ...'\n",
    "inaugural.words('1789-Washington.txt')\n",
    "#['Fellow', '-', 'Citizens', 'of', 'the', ...]\n",
    "inaugural.sents('1789-Washington.txt') # doctest: +ELLIPSIS\n",
    "#[['Fellow', '-', 'Citizens'...], ['Among', 'the', 'vicissitudes'...]...]\n",
    "inaugural.paras('1789-Washington.txt') # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "#[[['Fellow', '-', 'Citizens'...]],\n",
    "#[['Among', 'the', 'vicissitudes'...],\n",
    "#  ['On', 'the', 'one', 'hand', ',', 'I'...]...]...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1538+147 == 1685\n"
     ]
    }
   ],
   "source": [
    "l1 = len(inaugural.words('1789-Washington.txt'))\n",
    "l2 = len(inaugural.words('1793-Washington.txt'))\n",
    "l3 = len(inaugural.words(['1789-Washington.txt', '1793-Washington.txt']))\n",
    "print('%s+%s == %s' % (l1, l2, l3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149797"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inaugural.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C-Span Inaugural Address Corpus\\n\\nUS presidential inaugural addresses 1789-2017\\n\\n(Thanks to Kathleen Ahrens for compiling this corpus from\\nthe C-Span sources.)\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.readme()[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.abc.words()\n",
    "#['PM', 'denies', 'knowledge', 'of', 'AWB', ...]\n",
    "nltk.corpus.genesis.words()\n",
    "#[u'In', u'the', u'beginning', u'God', u'created', ...]\n",
    "nltk.corpus.gutenberg.words(fileids='austen-emma.txt')\n",
    "#['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ...]\n",
    "nltk.corpus.inaugural.words()\n",
    "#['Fellow', '-', 'Citizens', 'of', 'the', ...]\n",
    "nltk.corpus.state_union.words()\n",
    "#['PRESIDENT', 'HARRY', 'S', '.', 'TRUMAN', \"'\", ...]\n",
    "nltk.corpus.webtext.words()\n",
    "#['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "*************\n",
      " [('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "*************\n",
      " [['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n",
      "*************\n",
      " [[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n",
      "*************\n",
      " [[['It', 'is', 'not', 'news', 'that', 'Nathan', 'Milstein', 'is', 'a', 'wizard', 'of', 'the', 'violin', '.'], ['Certainly', 'not', 'in', 'Orchestra', 'Hall', 'where', 'he', 'has', 'played', 'countless', 'recitals', ',', 'and', 'where', 'Thursday', 'night', 'he', 'celebrated', 'his', '20th', 'season', 'with', 'the', 'Chicago', 'Symphony', 'Orchestra', ',', 'playing', 'the', 'Brahms', 'Concerto', 'with', 'his', 'own', 'slashing', ',', 'demon-ridden', 'cadenza', 'melting', 'into', 'the', 'high', ',', 'pale', ',', 'pure', 'and', 'lovely', 'song', 'with', 'which', 'a', 'violinist', 'unlocks', 'the', 'heart', 'of', 'the', 'music', ',', 'or', 'forever', 'finds', 'it', 'closed', '.']], [['There', 'was', 'about', 'that', 'song', 'something', 'incandescent', ',', 'for', 'this', 'Brahms', 'was', 'Milstein', 'at', 'white', 'heat', '.'], ['Not', 'the', 'noblest', 'performance', 'we', 'have', 'heard', 'him', 'play', ',', 'or', 'the', 'most', 'spacious', ',', 'or', 'even', 'the', 'most', 'eloquent', '.'], ['Those', 'would', 'be', 'reserved', 'for', 'the', \"orchestra's\", 'great', 'nights', 'when', 'the', 'soloist', 'can', 'surpass', 'himself', '.'], ['This', 'time', 'the', 'orchestra', 'gave', 'him', 'some', 'superb', 'support', 'fired', 'by', 'response', 'to', 'his', 'own', 'high', 'mood', '.'], ['But', 'he', 'had', 'in', 'Walter', 'Hendl', 'a', 'willing', 'conductor', 'able', 'only', 'up', 'to', 'a', 'point', '.']], ...]\n",
      "*************\n",
      " [[[('It', 'PPS'), ('is', 'BEZ'), ('not', '*'), ('news', 'NN'), ('that', 'CS'), ('Nathan', 'NP'), ('Milstein', 'NP'), ('is', 'BEZ'), ('a', 'AT'), ('wizard', 'NN'), ('of', 'IN'), ('the', 'AT'), ('violin', 'NN'), ('.', '.')], [('Certainly', 'RB'), ('not', '*'), ('in', 'IN'), ('Orchestra', 'NN-TL'), ('Hall', 'NN-TL'), ('where', 'WRB'), ('he', 'PPS'), ('has', 'HVZ'), ('played', 'VBN'), ('countless', 'JJ'), ('recitals', 'NNS'), (',', ','), ('and', 'CC'), ('where', 'WRB'), ('Thursday', 'NR'), ('night', 'NN'), ('he', 'PPS'), ('celebrated', 'VBD'), ('his', 'PP$'), ('20th', 'OD'), ('season', 'NN'), ('with', 'IN'), ('the', 'AT'), ('Chicago', 'NP-TL'), ('Symphony', 'NN-TL'), ('Orchestra', 'NN-TL'), (',', ','), ('playing', 'VBG'), ('the', 'AT'), ('Brahms', 'NP-TL'), ('Concerto', 'NN-TL'), ('with', 'IN'), ('his', 'PP$'), ('own', 'JJ'), ('slashing', 'VBG'), (',', ','), ('demon-ridden', 'JJ'), ('cadenza', 'NN'), ('melting', 'VBG'), ('into', 'IN'), ('the', 'AT'), ('high', 'JJ'), (',', ','), ('pale', 'JJ'), (',', ','), ('pure', 'JJ'), ('and', 'CC'), ('lovely', 'JJ'), ('song', 'NN'), ('with', 'IN'), ('which', 'WDT'), ('a', 'AT'), ('violinist', 'NN'), ('unlocks', 'VBZ'), ('the', 'AT'), ('heart', 'NN'), ('of', 'IN'), ('the', 'AT'), ('music', 'NN'), (',', ','), ('or', 'CC'), ('forever', 'RB'), ('finds', 'VBZ'), ('it', 'PPO'), ('closed', 'VBN'), ('.', '.')]], [[('There', 'EX'), ('was', 'BEDZ'), ('about', 'IN'), ('that', 'DT'), ('song', 'NN'), ('something', 'PN'), ('incandescent', 'JJ'), (',', ','), ('for', 'CS'), ('this', 'DT'), ('Brahms', 'NP'), ('was', 'BEDZ'), ('Milstein', 'NP'), ('at', 'IN'), ('white', 'JJ'), ('heat', 'NN'), ('.', '.')], [('Not', '*'), ('the', 'AT'), ('noblest', 'JJT'), ('performance', 'NN'), ('we', 'PPSS'), ('have', 'HV'), ('heard', 'VBN'), ('him', 'PPO'), ('play', 'VB'), (',', ','), ('or', 'CC'), ('the', 'AT'), ('most', 'QL'), ('spacious', 'JJ'), (',', ','), ('or', 'CC'), ('even', 'RB'), ('the', 'AT'), ('most', 'QL'), ('eloquent', 'JJ'), ('.', '.')], [('Those', 'DTS'), ('would', 'MD'), ('be', 'BE'), ('reserved', 'VBN'), ('for', 'IN'), ('the', 'AT'), (\"orchestra's\", 'NN$'), ('great', 'JJ'), ('nights', 'NNS'), ('when', 'WRB'), ('the', 'AT'), ('soloist', 'NN'), ('can', 'MD'), ('surpass', 'VB'), ('himself', 'PPL'), ('.', '.')], [('This', 'DT'), ('time', 'NN'), ('the', 'AT'), ('orchestra', 'NN'), ('gave', 'VBD'), ('him', 'PPO'), ('some', 'DTI'), ('superb', 'JJ'), ('support', 'NN'), ('fired', 'VBN'), ('by', 'IN'), ('response', 'NN'), ('to', 'IN'), ('his', 'PP$'), ('own', 'JJ'), ('high', 'JJ'), ('mood', 'NN'), ('.', '.')], [('But', 'CC'), ('he', 'PPS'), ('had', 'HVD'), ('in', 'IN'), ('Walter', 'NP'), ('Hendl', 'NP'), ('a', 'AT'), ('willing', 'JJ'), ('conductor', 'NN'), ('able', 'JJ'), ('only', 'RB'), ('up', 'IN'), ('to', 'IN'), ('a', 'AT'), ('point', 'NN'), ('.', '.')]], ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "print(brown.words())\n",
    "#['The', 'Fulton', 'County', 'Grand', 'Jury', ...]\n",
    "print(\"*************\\n\",brown.tagged_words())\n",
    "#[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
    "print(\"*************\\n\",brown.sents()) # doctest: +ELLIPSIS\n",
    "#[['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]\n",
    "print(\"*************\\n\",brown.tagged_sents()) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "#[[('The', 'AT'), ('Fulton', 'NP-TL')...],\n",
    "# [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR')...]...]\n",
    "print(\"*************\\n\",brown.paras(categories='reviews')) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "#[[['It', 'is', 'not', 'news', 'that', 'Nathan', 'Milstein'...],\n",
    "#  ['Certainly', 'not', 'in', 'Orchestra', 'Hall', 'where'...]],\n",
    "# [['There', 'was', 'about', 'that', 'song', 'something', ...],\n",
    "#  ['Not', 'the', 'noblest', 'performance', 'we', 'have', ...], ...], ...]\n",
    "print(\"*************\\n\",brown.tagged_paras(categories='reviews')) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "#[[[('It', 'PPS'), ('is', 'BEZ'), ('not', '*'), ...],\n",
    "#  [('Certainly', 'RB'), ('not', '*'), ('in', 'IN'), ...]],\n",
    "# [[('There', 'EX'), ('was', 'BEDZ'), ('about', 'IN'), ...],\n",
    "#  [('Not', '*'), ('the', 'AT'), ('noblest', 'JJT'), ...], ...], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['মহিষের', 'সন্তান', ':', 'তোড়া', 'উপজাতি', '৷', ...]\n",
      "[('মহিষের', 'NN'), ('সন্তান', 'NN'), (':', 'SYM'), ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import indian\n",
    "print(indian.words()) # doctest: +SKIP\n",
    "#['\\xe0\\xa6\\xae\\xe0\\xa6\\xb9\\xe0\\xa6\\xbf\\...',\n",
    "# '\\xe0\\xa6\\xb8\\xe0\\xa6\\xa8\\xe0\\xa7\\x8d\\xe0...', ...]\n",
    "print(indian.tagged_words()) # doctest: +SKIP\n",
    "#[('\\xe0\\xa6\\xae\\xe0\\xa6\\xb9\\xe0\\xa6\\xbf...', 'NN'),\n",
    "# ('\\xe0\\xa6\\xb8\\xe0\\xa6\\xa8\\xe0\\xa7\\x8d\\xe0...', 'NN'), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]\n",
      "[('Confidence', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ...]\n"
     ]
    }
   ],
   "source": [
    "print(brown.tagged_sents(tagset='universal')) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "#[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ...],\n",
    "# [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ...]...]\n",
    "from nltk.corpus import conll2000, switchboard\n",
    "print(conll2000.tagged_words(tagset='universal')) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "#[('Confidence', 'NOUN'), ('in', 'ADP'), ...]\n",
    "#Use nltk.app.pos_concordance() to access a GUI for searching tagged corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Confidence', 'in', 'the', 'pound', 'is', 'widely', 'expected', 'to', 'take', 'another', 'sharp', 'dive', 'if', 'trade', 'figures', 'for', 'September', ',', 'due', 'for', 'release', 'tomorrow', ',', 'fail', 'to', 'show', 'a', 'substantial', 'improvement', 'from', 'July', 'and', 'August', \"'s\", 'near-record', 'deficits', '.'], ['Chancellor', 'of', 'the', 'Exchequer', 'Nigel', 'Lawson', \"'s\", 'restated', 'commitment', 'to', 'a', 'firm', 'monetary', 'policy', 'has', 'helped', 'to', 'prevent', 'a', 'freefall', 'in', 'sterling', 'over', 'the', 'past', 'week', '.'], ...]\n",
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  (PP in/IN)\n",
      "  (NP the/DT pound/NN)\n",
      "  (VP is/VBZ widely/RB expected/VBN to/TO take/VB)\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  if/IN\n",
      "  (NP trade/NN figures/NNS)\n",
      "  (PP for/IN)\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  (PP for/IN)\n",
      "  (NP release/NN)\n",
      "  (NP tomorrow/NN)\n",
      "  ,/,\n",
      "  (VP fail/VB to/TO show/VB)\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  (PP from/IN)\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.)\n",
      "(S\n",
      "  Chancellor/NNP\n",
      "  (PP of/IN)\n",
      "  (NP the/DT Exchequer/NNP)\n",
      "  (NP Nigel/NNP Lawson/NNP)\n",
      "  (NP 's/POS restated/VBN commitment/NN)\n",
      "  (PP to/TO)\n",
      "  (NP a/DT firm/NN monetary/JJ policy/NN)\n",
      "  (VP has/VBZ helped/VBN to/TO prevent/VB)\n",
      "  (NP a/DT freefall/NN)\n",
      "  (PP in/IN)\n",
      "  (NP sterling/NN)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT past/JJ week/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Chunked Corpora\n",
    "#The CoNLL corpora also provide chunk structures, which are encoded as flat trees. The CoNLL 2000 Corpus includes phrasal chunks; and the CoNLL 2002 Corpus includes named entity chunks.\n",
    "\n",
    "from nltk.corpus import conll2000, conll2002\n",
    "print(conll2000.sents()) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "#[['Confidence', 'in', 'the', 'pound', 'is', 'widely', ...],\n",
    "# ['Chancellor', 'of', 'the', 'Exchequer', ...], ...]\n",
    "for tree in conll2000.chunked_sents()[:2]:\n",
    "     print(tree) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sao', 'Paulo', '(', 'Brasil', ')', ',', '23', 'may', '(', 'EFECOM', ')', '.'], ['-'], ...]\n",
      "(S\n",
      "  (LOC Sao/NC Paulo/VMI)\n",
      "  (/Fpa\n",
      "  (LOC Brasil/NC)\n",
      "  )/Fpt\n",
      "  ,/Fc\n",
      "  23/Z\n",
      "  may/NC\n",
      "  (/Fpa\n",
      "  (ORG EFECOM/NP)\n",
      "  )/Fpt\n",
      "  ./Fp)\n",
      "(S -/Fg)\n"
     ]
    }
   ],
   "source": [
    "print(conll2002.sents()) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "for tree in conll2002.chunked_sents()[:2]:\n",
    "    print(tree) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['(DT The)',\n",
       "  \"(Lemma('group.n.01.group') (NE (NNP Fulton County Grand Jury)))\",\n",
       "  \"(Lemma('state.v.01.say') (VB said))\",\n",
       "  \"(Lemma('friday.n.01.Friday') (NN Friday))\",\n",
       "  '(DT an)',\n",
       "  \"(Lemma('probe.n.01.investigation') (NN investigation))\",\n",
       "  '(IN of)',\n",
       "  \"(Lemma('atlanta.n.01.Atlanta') (NN Atlanta))\",\n",
       "  \"(POS 's)\",\n",
       "  \"(Lemma('late.s.03.recent') (JJ recent))\",\n",
       "  \"(Lemma('primary.n.01.primary_election') (NN primary election))\",\n",
       "  \"(Lemma('produce.v.04.produce') (VB produced))\",\n",
       "  '(None ``)',\n",
       "  '(DT no)',\n",
       "  \"(Lemma('evidence.n.01.evidence') (NN evidence))\",\n",
       "  \"(None '')\",\n",
       "  '(IN that)',\n",
       "  '(DT any)',\n",
       "  \"(Lemma('abnormality.n.04.irregularity') (NN irregularities))\",\n",
       "  \"(Lemma('happen.v.01.take_place') (VB took place))\",\n",
       "  '(None .)'],\n",
       " ['(DT The)',\n",
       "  \"(Lemma('jury.n.01.jury') (NN jury))\",\n",
       "  \"(Lemma('far.r.02.far') (RB further))\",\n",
       "  \"(Lemma('state.v.01.say') (VB said))\",\n",
       "  '(IN in)',\n",
       "  \"(Lemma('term.n.02.term') (NN term))\",\n",
       "  \"(Lemma('end.n.02.end') (NN end))\",\n",
       "  \"(Lemma('presentment.n.01.presentment') (NN presentments))\",\n",
       "  '(IN that)',\n",
       "  '(DT the)',\n",
       "  \"(Lemma('group.n.01.group') (NE (NNP City Executive Committee)))\",\n",
       "  '(None ,)',\n",
       "  '(WDT which)',\n",
       "  \"(Lemma('own.v.01.have') (VB had))\",\n",
       "  \"(Lemma('overall.s.02.overall') (JJ over-all))\",\n",
       "  \"(Lemma('mission.n.03.charge') (NN charge))\",\n",
       "  '(IN of)',\n",
       "  '(DT the)',\n",
       "  \"(Lemma('election.n.01.election') (NN election))\",\n",
       "  '(None ,)',\n",
       "  '(None ``)',\n",
       "  \"(Lemma('deserve.v.01.deserve') (VB deserves))\",\n",
       "  '(DT the)',\n",
       "  \"(Lemma('praise.n.01.praise') (NN praise))\",\n",
       "  '(CC and)',\n",
       "  \"(Lemma('thanks.n.01.thanks') (NN thanks))\",\n",
       "  '(IN of)',\n",
       "  '(DT the)',\n",
       "  \"(Lemma('location.n.01.location') (NE (NNP City of Atlanta)))\",\n",
       "  \"(None '')\",\n",
       "  '(IN for)',\n",
       "  '(DT the)',\n",
       "  \"(Lemma('manner.n.01.manner') (NN manner))\",\n",
       "  '(RB in)',\n",
       "  '(RB which)',\n",
       "  '(DT the)',\n",
       "  \"(Lemma('election.n.01.election') (NN election))\",\n",
       "  '(VBD was)',\n",
       "  \"(Lemma('conduct.v.01.conduct') (VB conducted))\",\n",
       "  '(None .)']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import semcor\n",
    "semcor.words()\n",
    "#['The', 'Fulton', 'County', 'Grand', 'Jury', ...]\n",
    "semcor.chunks()\n",
    "#[['The'], ['Fulton', 'County', 'Grand', 'Jury'], ...]\n",
    "semcor.sents() # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "#[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...],\n",
    "#['The', 'jury', 'further', 'said', ...], ...]\n",
    "semcor.chunk_sents() # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "#[[['The'], ['Fulton', 'County', 'Grand', 'Jury'], ['said'], ...\n",
    "#['.']], [['The'], ['jury'], ['further'], ['said'], ... ['.']], ...]\n",
    "list(map(str, semcor.tagged_chunks(tag='both')[:3]))\n",
    "#['(DT The)', \"(Lemma('group.n.01.group') (NE (NNP Fulton County Grand Jury)))\", \"(Lemma('state.v.01.say') (VB said))\"]\n",
    "[[str(c) for c in s] for s in semcor.tagged_sents(tag='both')[:2]]\n",
    "#[['(DT The)', \"(Lemma('group.n.01.group') (NE (NNP Fulton County Grand Jury)))\", ...\n",
    "# '(None .)'], ['(DT The)', ... '(None .)']]\n",
    "#The IEER corpus is another chunked corpus. This corpus is unusual in that each corpus item contains multiple documents. (This reflects the fact that each corpus file contains multiple documents.) The IEER corpus defines the parsed_docs method, which returns the documents in a given item as IEERDocument objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('DT', ['The']), Tree(Lemma('group.n.01.group'), [Tree('NE', [Tree('NNP', ['Fulton', 'County', 'Grand', 'Jury'])])]), ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor.tagged_chunks(tag='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(DT The)',\n",
       " \"(Lemma('group.n.01.group') (NE (NNP Fulton County Grand Jury)))\",\n",
       " \"(Lemma('state.v.01.say') (VB said))\"]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(str, semcor.tagged_chunks(tag='both')[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The IEER corpus is another chunked corpus. This corpus is unusual in that each corpus item contains multiple documents. (This reflects the fact that each corpus file contains multiple documents.) The IEER corpus defines the parsed_docs method, which returns the documents in a given item as IEERDocument objects:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IEERDocument APW19980314.0391: 'Kenyans protest tax hikes'>\n",
      "APW19980314.0391\n",
      "NEWS STORY\n",
      "03/14/1998 10:36:00\n",
      "(DOCUMENT Kenyans protest tax hikes)\n",
      "(DOCUMENT\n",
      "  (LOCATION NAIROBI)\n",
      "  ,\n",
      "  (LOCATION Kenya)\n",
      "  (\n",
      "  (ORGANIZATION AP)\n",
      "  )\n",
      "  _\n",
      "  (CARDINAL Thousands)\n",
      "  of\n",
      "  laborers,\n",
      "  students\n",
      "  and\n",
      "  opposition\n",
      "  politicians\n",
      "  on\n",
      "  (DATE Saturday)\n",
      "  protested\n",
      "  tax\n",
      "  hikes\n",
      "  imposed\n",
      "  by\n",
      "  their\n",
      "  cash-strapped\n",
      "  government,\n",
      "  which\n",
      "  they\n",
      "  accused\n",
      "  of\n",
      "  failing\n",
      "  to\n",
      "  provide\n",
      "  basic\n",
      "  services.\n",
      "  Beneath\n",
      "  a\n",
      "  scorching\n",
      "  sun,\n",
      "  they\n",
      "  sang\n",
      "  anti-government\n",
      "  songs\n",
      "  and\n",
      "  chanted\n",
      "  ``\n",
      "  (PERSON Moi)\n",
      "  must\n",
      "  go,''\n",
      "  showing\n",
      "  their\n",
      "  derision\n",
      "  for\n",
      "  President\n",
      "  (PERSON Daniel arap Moi)\n",
      "  ,\n",
      "  (LOCATION Kenya)\n",
      "  's\n",
      "  ruler\n",
      "  for\n",
      "  (DURATION 20 years)\n",
      "  .\n",
      "  By\n",
      "  voice\n",
      "  vote,\n",
      "  the\n",
      "  (CARDINAL 5,000)\n",
      "  protesters\n",
      "  approved\n",
      "  a\n",
      "  resolution\n",
      "  calling\n",
      "  for\n",
      "  the\n",
      "  government\n",
      "  to\n",
      "  scrap\n",
      "  new\n",
      "  taxes,\n",
      "  convene\n",
      "  a\n",
      "  convention\n",
      "  to\n",
      "  write\n",
      "  a\n",
      "  new\n",
      "  Constitution,\n",
      "  stop\n",
      "  harassing\n",
      "  students\n",
      "  and\n",
      "  street\n",
      "  vendors,\n",
      "  and\n",
      "  halt\n",
      "  ethnic\n",
      "  violence.\n",
      "  If\n",
      "  the\n",
      "  government\n",
      "  doesn't\n",
      "  respond\n",
      "  to\n",
      "  the\n",
      "  demands,\n",
      "  workers\n",
      "  should\n",
      "  go\n",
      "  on\n",
      "  strike\n",
      "  (DATE April 3)\n",
      "  ,\n",
      "  said\n",
      "  (PERSON Kivutha Kibwana)\n",
      "  ,\n",
      "  of\n",
      "  the\n",
      "  (ORGANIZATION National Convention Assembly)\n",
      "  ,\n",
      "  a\n",
      "  group\n",
      "  of\n",
      "  opposition\n",
      "  political,\n",
      "  church\n",
      "  and\n",
      "  civic\n",
      "  leaders\n",
      "  who\n",
      "  organized\n",
      "  the\n",
      "  rally.\n",
      "  Although\n",
      "  the\n",
      "  crowd\n",
      "  cheered\n",
      "  its\n",
      "  support,\n",
      "  the\n",
      "  assembly's\n",
      "  calls\n",
      "  for\n",
      "  general\n",
      "  strikes\n",
      "  have\n",
      "  failed\n",
      "  in\n",
      "  the\n",
      "  past.\n",
      "  Kenyans\n",
      "  are\n",
      "  most\n",
      "  angered\n",
      "  by\n",
      "  tax\n",
      "  increases\n",
      "  announced\n",
      "  earlier\n",
      "  this\n",
      "  month\n",
      "  to\n",
      "  plug\n",
      "  a\n",
      "  widening\n",
      "  budget\n",
      "  deficit.\n",
      "  The\n",
      "  price\n",
      "  of\n",
      "  gas\n",
      "  and\n",
      "  diesel\n",
      "  went\n",
      "  up,\n",
      "  and\n",
      "  tax\n",
      "  deductions\n",
      "  were\n",
      "  revoked\n",
      "  on\n",
      "  gifts\n",
      "  to\n",
      "  charities\n",
      "  and\n",
      "  nonprofit\n",
      "  organizations.\n",
      "  Faced\n",
      "  with\n",
      "  a\n",
      "  strike\n",
      "  by\n",
      "  bank\n",
      "  workers,\n",
      "  (ORGANIZATION Finance)\n",
      "  Minister\n",
      "  (PERSON Simeon Nyachae)\n",
      "  delayed\n",
      "  plans\n",
      "  to\n",
      "  raise\n",
      "  taxes\n",
      "  on\n",
      "  cut-rate\n",
      "  loans\n",
      "  provided\n",
      "  by\n",
      "  employers.\n",
      "  Several\n",
      "  politicians\n",
      "  have\n",
      "  charged\n",
      "  that\n",
      "  the\n",
      "  high\n",
      "  taxes\n",
      "  Kenyans\n",
      "  already\n",
      "  pay\n",
      "  go\n",
      "  into\n",
      "  the\n",
      "  pockets\n",
      "  of\n",
      "  government\n",
      "  officials\n",
      "  or\n",
      "  wasteful\n",
      "  projects,\n",
      "  and\n",
      "  not\n",
      "  into\n",
      "  providing\n",
      "  essential\n",
      "  services\n",
      "  and\n",
      "  repairing\n",
      "  crumbling\n",
      "  infrastructure.\n",
      "  ``I\n",
      "  would\n",
      "  like\n",
      "  (PERSON Moi)\n",
      "  to\n",
      "  explain\n",
      "  and\n",
      "  bring\n",
      "  back\n",
      "  the\n",
      "  money\n",
      "  he\n",
      "  has\n",
      "  stolen,''\n",
      "  said\n",
      "  (PERSON Kenneth Matiba)\n",
      "  ,\n",
      "  who\n",
      "  ran\n",
      "  second\n",
      "  to\n",
      "  (PERSON Moi)\n",
      "  in\n",
      "  the\n",
      "  (DATE 1992)\n",
      "  election.\n",
      "  The\n",
      "  (ORGANIZATION International Monetary Fund)\n",
      "  has\n",
      "  demanded\n",
      "  spending\n",
      "  cuts\n",
      "  and\n",
      "  tax\n",
      "  hikes,\n",
      "  warning\n",
      "  that\n",
      "  in\n",
      "  the\n",
      "  absence\n",
      "  of\n",
      "  corrective\n",
      "  measures,\n",
      "  (LOCATION Kenya)\n",
      "  's\n",
      "  overall\n",
      "  budget\n",
      "  deficit\n",
      "  for\n",
      "  (DATE 1997)\n",
      "  -\n",
      "  (DATE 98)\n",
      "  will\n",
      "  reach\n",
      "  (PERCENT 3.9 percent)\n",
      "  of\n",
      "  the\n",
      "  gross\n",
      "  domestic\n",
      "  product\n",
      "  _\n",
      "  more\n",
      "  than\n",
      "  double\n",
      "  the\n",
      "  goal\n",
      "  of\n",
      "  (PERCENT 1.7 percent)\n",
      "  .\n",
      "  The\n",
      "  (ORGANIZATION IMF)\n",
      "  last\n",
      "  year\n",
      "  withheld\n",
      "  a\n",
      "  (MONEY dlrs 220 million)\n",
      "  loan\n",
      "  from\n",
      "  (LOCATION Kenya)\n",
      "  ,\n",
      "  citing\n",
      "  official\n",
      "  corruption\n",
      "  and\n",
      "  mismanagement.\n",
      "  In\n",
      "  contrast\n",
      "  to\n",
      "  other\n",
      "  rallies\n",
      "  which\n",
      "  have\n",
      "  drawn\n",
      "  mostly\n",
      "  male\n",
      "  students,\n",
      "  (DATE Saturday)\n",
      "  's\n",
      "  rally\n",
      "  also\n",
      "  attracted\n",
      "  laborers,\n",
      "  businessmen\n",
      "  and\n",
      "  women.\n",
      "  They\n",
      "  demonstrated\n",
      "  at\n",
      "  the\n",
      "  (LOCATION Kamukunji Grounds)\n",
      "  ,\n",
      "  a\n",
      "  grassy\n",
      "  field\n",
      "  in\n",
      "  (LOCATION Nairobi)\n",
      "  where\n",
      "  Kenyans\n",
      "  have\n",
      "  often\n",
      "  gathered\n",
      "  to\n",
      "  show\n",
      "  their\n",
      "  dissatisfaction\n",
      "  with\n",
      "  the\n",
      "  government.\n",
      "  ``We,\n",
      "  the\n",
      "  people\n",
      "  of\n",
      "  (LOCATION Kenya)\n",
      "  ,\n",
      "  have\n",
      "  met\n",
      "  here\n",
      "  several\n",
      "  times.\n",
      "  We\n",
      "  have\n",
      "  been\n",
      "  beaten,\n",
      "  we\n",
      "  have\n",
      "  shed\n",
      "  blood,\n",
      "  we\n",
      "  have\n",
      "  purchased\n",
      "  the\n",
      "  right\n",
      "  to\n",
      "  meet\n",
      "  here\n",
      "  today\n",
      "  with\n",
      "  our\n",
      "  blood,''\n",
      "  said\n",
      "  (PERSON John Munuve)\n",
      "  ,\n",
      "  an\n",
      "  assembly\n",
      "  leader.\n",
      "  The\n",
      "  protest\n",
      "  was\n",
      "  peaceful,\n",
      "  and\n",
      "  no\n",
      "  police\n",
      "  were\n",
      "  deployed.\n",
      "  In\n",
      "  (DATE July)\n",
      "  ,\n",
      "  police\n",
      "  killed\n",
      "  more\n",
      "  than\n",
      "  a\n",
      "  (CARDINAL dozen)\n",
      "  demonstrators\n",
      "  pressing\n",
      "  for\n",
      "  constitutional\n",
      "  reforms\n",
      "  before\n",
      "  elections\n",
      "  in\n",
      "  (DATE December)\n",
      "  ,\n",
      "  in\n",
      "  which\n",
      "  (PERSON Moi)\n",
      "  won\n",
      "  a\n",
      "  fifth,\n",
      "  (DURATION five-year)\n",
      "  term.\n",
      "  (cm-kjd))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import ieer\n",
    "ieer.fileids() # doctest: +NORMALIZE_WHITESPACE\n",
    "#['APW_19980314', 'APW_19980424', 'APW_19980429',\n",
    "# 'NYT_19980315', 'NYT_19980403', 'NYT_19980407']\n",
    "docs = ieer.parsed_docs('APW_19980314')\n",
    "print(docs[0])\n",
    "#<IEERDocument APW19980314.0391: 'Kenyans protest tax hikes'>\n",
    "print(docs[0].docno)\n",
    "#APW19980314.0391\n",
    "print(docs[0].doctype)\n",
    "#NEWS STORY\n",
    "print(docs[0].date_time)\n",
    "#03/14/1998 10:36:00\n",
    "print(docs[0].headline)\n",
    "#(DOCUMENT Kenyans protest tax hikes)\n",
    "print(docs[0].text) # doctest: +ELLIPSIS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Treebank corpora provide a syntactic parse for each sentence. The NLTK data package includes a 10% sample of the Penn Treebank (in treebank), as well as the Sinica Treebank (in sinica_treebank).\n",
    "\n",
    "Reading the Penn Treebank (Wall Street Journal sample):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wsj_0001.mrg', 'wsj_0002.mrg', 'wsj_0003.mrg', 'wsj_0004.mrg', 'wsj_0005.mrg', 'wsj_0006.mrg', 'wsj_0007.mrg', 'wsj_0008.mrg', 'wsj_0009.mrg', 'wsj_0010.mrg', 'wsj_0011.mrg', 'wsj_0012.mrg', 'wsj_0013.mrg', 'wsj_0014.mrg', 'wsj_0015.mrg', 'wsj_0016.mrg', 'wsj_0017.mrg', 'wsj_0018.mrg', 'wsj_0019.mrg', 'wsj_0020.mrg', 'wsj_0021.mrg', 'wsj_0022.mrg', 'wsj_0023.mrg', 'wsj_0024.mrg', 'wsj_0025.mrg', 'wsj_0026.mrg', 'wsj_0027.mrg', 'wsj_0028.mrg', 'wsj_0029.mrg', 'wsj_0030.mrg', 'wsj_0031.mrg', 'wsj_0032.mrg', 'wsj_0033.mrg', 'wsj_0034.mrg', 'wsj_0035.mrg', 'wsj_0036.mrg', 'wsj_0037.mrg', 'wsj_0038.mrg', 'wsj_0039.mrg', 'wsj_0040.mrg', 'wsj_0041.mrg', 'wsj_0042.mrg', 'wsj_0043.mrg', 'wsj_0044.mrg', 'wsj_0045.mrg', 'wsj_0046.mrg', 'wsj_0047.mrg', 'wsj_0048.mrg', 'wsj_0049.mrg', 'wsj_0050.mrg', 'wsj_0051.mrg', 'wsj_0052.mrg', 'wsj_0053.mrg', 'wsj_0054.mrg', 'wsj_0055.mrg', 'wsj_0056.mrg', 'wsj_0057.mrg', 'wsj_0058.mrg', 'wsj_0059.mrg', 'wsj_0060.mrg', 'wsj_0061.mrg', 'wsj_0062.mrg', 'wsj_0063.mrg', 'wsj_0064.mrg', 'wsj_0065.mrg', 'wsj_0066.mrg', 'wsj_0067.mrg', 'wsj_0068.mrg', 'wsj_0069.mrg', 'wsj_0070.mrg', 'wsj_0071.mrg', 'wsj_0072.mrg', 'wsj_0073.mrg', 'wsj_0074.mrg', 'wsj_0075.mrg', 'wsj_0076.mrg', 'wsj_0077.mrg', 'wsj_0078.mrg', 'wsj_0079.mrg', 'wsj_0080.mrg', 'wsj_0081.mrg', 'wsj_0082.mrg', 'wsj_0083.mrg', 'wsj_0084.mrg', 'wsj_0085.mrg', 'wsj_0086.mrg', 'wsj_0087.mrg', 'wsj_0088.mrg', 'wsj_0089.mrg', 'wsj_0090.mrg', 'wsj_0091.mrg', 'wsj_0092.mrg', 'wsj_0093.mrg', 'wsj_0094.mrg', 'wsj_0095.mrg', 'wsj_0096.mrg', 'wsj_0097.mrg', 'wsj_0098.mrg', 'wsj_0099.mrg', 'wsj_0100.mrg', 'wsj_0101.mrg', 'wsj_0102.mrg', 'wsj_0103.mrg', 'wsj_0104.mrg', 'wsj_0105.mrg', 'wsj_0106.mrg', 'wsj_0107.mrg', 'wsj_0108.mrg', 'wsj_0109.mrg', 'wsj_0110.mrg', 'wsj_0111.mrg', 'wsj_0112.mrg', 'wsj_0113.mrg', 'wsj_0114.mrg', 'wsj_0115.mrg', 'wsj_0116.mrg', 'wsj_0117.mrg', 'wsj_0118.mrg', 'wsj_0119.mrg', 'wsj_0120.mrg', 'wsj_0121.mrg', 'wsj_0122.mrg', 'wsj_0123.mrg', 'wsj_0124.mrg', 'wsj_0125.mrg', 'wsj_0126.mrg', 'wsj_0127.mrg', 'wsj_0128.mrg', 'wsj_0129.mrg', 'wsj_0130.mrg', 'wsj_0131.mrg', 'wsj_0132.mrg', 'wsj_0133.mrg', 'wsj_0134.mrg', 'wsj_0135.mrg', 'wsj_0136.mrg', 'wsj_0137.mrg', 'wsj_0138.mrg', 'wsj_0139.mrg', 'wsj_0140.mrg', 'wsj_0141.mrg', 'wsj_0142.mrg', 'wsj_0143.mrg', 'wsj_0144.mrg', 'wsj_0145.mrg', 'wsj_0146.mrg', 'wsj_0147.mrg', 'wsj_0148.mrg', 'wsj_0149.mrg', 'wsj_0150.mrg', 'wsj_0151.mrg', 'wsj_0152.mrg', 'wsj_0153.mrg', 'wsj_0154.mrg', 'wsj_0155.mrg', 'wsj_0156.mrg', 'wsj_0157.mrg', 'wsj_0158.mrg', 'wsj_0159.mrg', 'wsj_0160.mrg', 'wsj_0161.mrg', 'wsj_0162.mrg', 'wsj_0163.mrg', 'wsj_0164.mrg', 'wsj_0165.mrg', 'wsj_0166.mrg', 'wsj_0167.mrg', 'wsj_0168.mrg', 'wsj_0169.mrg', 'wsj_0170.mrg', 'wsj_0171.mrg', 'wsj_0172.mrg', 'wsj_0173.mrg', 'wsj_0174.mrg', 'wsj_0175.mrg', 'wsj_0176.mrg', 'wsj_0177.mrg', 'wsj_0178.mrg', 'wsj_0179.mrg', 'wsj_0180.mrg', 'wsj_0181.mrg', 'wsj_0182.mrg', 'wsj_0183.mrg', 'wsj_0184.mrg', 'wsj_0185.mrg', 'wsj_0186.mrg', 'wsj_0187.mrg', 'wsj_0188.mrg', 'wsj_0189.mrg', 'wsj_0190.mrg', 'wsj_0191.mrg', 'wsj_0192.mrg', 'wsj_0193.mrg', 'wsj_0194.mrg', 'wsj_0195.mrg', 'wsj_0196.mrg', 'wsj_0197.mrg', 'wsj_0198.mrg', 'wsj_0199.mrg']\n",
      "['A', 'form', 'of', 'asbestos', 'once', 'used', '*', ...]\n",
      "[('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ...]\n",
      "(S\n",
      "  (S-TPC-1\n",
      "    (NP-SBJ\n",
      "      (NP (NP (DT A) (NN form)) (PP (IN of) (NP (NN asbestos))))\n",
      "      (RRC\n",
      "        (ADVP-TMP (RB once))\n",
      "        (VP\n",
      "          (VBN used)\n",
      "          (NP (-NONE- *))\n",
      "          (S-CLR\n",
      "            (NP-SBJ (-NONE- *))\n",
      "            (VP\n",
      "              (TO to)\n",
      "              (VP\n",
      "                (VB make)\n",
      "                (NP (NNP Kent) (NN cigarette) (NNS filters))))))))\n",
      "    (VP\n",
      "      (VBZ has)\n",
      "      (VP\n",
      "        (VBN caused)\n",
      "        (NP\n",
      "          (NP (DT a) (JJ high) (NN percentage))\n",
      "          (PP (IN of) (NP (NN cancer) (NNS deaths)))\n",
      "          (PP-LOC\n",
      "            (IN among)\n",
      "            (NP\n",
      "              (NP (DT a) (NN group))\n",
      "              (PP\n",
      "                (IN of)\n",
      "                (NP\n",
      "                  (NP (NNS workers))\n",
      "                  (RRC\n",
      "                    (VP\n",
      "                      (VBN exposed)\n",
      "                      (NP (-NONE- *))\n",
      "                      (PP-CLR (TO to) (NP (PRP it)))\n",
      "                      (ADVP-TMP\n",
      "                        (NP\n",
      "                          (QP (RBR more) (IN than) (CD 30))\n",
      "                          (NNS years))\n",
      "                        (IN ago))))))))))))\n",
      "  (, ,)\n",
      "  (NP-SBJ (NNS researchers))\n",
      "  (VP (VBD reported) (SBAR (-NONE- 0) (S (-NONE- *T*-1))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "print(treebank.fileids()) # doctest: +ELLIPSIS\n",
    "#['wsj_0001.mrg', 'wsj_0002.mrg', 'wsj_0003.mrg', 'wsj_0004.mrg', ...]\n",
    "print(treebank.words('wsj_0003.mrg'))\n",
    "#['A', 'form', 'of', 'asbestos', 'once', 'used', ...]\n",
    "print(treebank.tagged_words('wsj_0003.mrg'))\n",
    "#[('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ...]\n",
    "print(treebank.parsed_sents('wsj_0003.mrg')[0]) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have access to a full installation of the Penn Treebank, NLTK can be configured to load it as well. Download the ptb package, and in the directory nltk_data/corpora/ptb place the BROWN and WSJ directories of the Treebank installation (symlinks work as well). Then use the ptb module instead of treebank:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ptb to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package ptb is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"ptb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import ptb\n",
    "print(ptb.fileids()) # doctest: +SKIP\n",
    "#['BROWN/CF/CF01.MRG', 'BROWN/CF/CF02.MRG', 'BROWN/CF/CF03.MRG', 'BROWN/CF/CF04.MRG', ...]\n",
    "#print(ptb.words('WSJ/00/WSJ_0003.MRG')) # doctest: +SKIP\n",
    "#['A', 'form', 'of', 'asbestos', 'once', 'used', '*', ...]\n",
    "#print(ptb.tagged_words('WSJ/00/WSJ_0003.MRG')) # doctest: +SKIP\n",
    "#[('A', 'DT'), ('form', 'NN'), ('of', 'IN'), ...]\n",
    "#...and so forth, like treebank but with extended fileids. Categories specified in allcats.txt can be used to filter by genre; they consist of news (for WSJ articles) and names of the Brown subcategories (fiction, humor, romance, etc.):\n",
    "\n",
    "#ptb.categories() # doctest: +SKIP\n",
    "#['adventure', 'belles_lettres', 'fiction', 'humor', 'lore', 'mystery', 'news', 'romance', 'science_fiction']\n",
    "#print(ptb.fileids('news')) # doctest: +SKIP\n",
    "#['WSJ/00/WSJ_0001.MRG', 'WSJ/00/WSJ_0002.MRG', 'WSJ/00/WSJ_0003.MRG', ...]\n",
    "#print(ptb.words(categories=['humor','fiction'])) # doctest: +SKIP\n",
    "#['Thirty-three', 'Scotty', 'did', 'not', 'go', 'back', ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As PropBank and NomBank depend on the (WSJ portion of the) Penn Treebank, the modules propbank_ptb and nombank_ptb are provided for access to a full PTB installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sinica_treebank to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['一'], ['友情'], ['嘉珍', '和', '我', '住在', '同一條', '巷子'], ...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\sinica_treebank.zip.\n"
     ]
    }
   ],
   "source": [
    "#Reading the Sinica Treebank:\n",
    "nltk.download(\"sinica_treebank\")\n",
    "from nltk.corpus import sinica_treebank\n",
    "print(sinica_treebank.sents()) # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\admin\\\\AppData\\\\Local\\\\Temp\\\\tmp_t4232ty.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mD:\\Users\\admin\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\admin\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    802\u001b[0m                 ).split()\n\u001b[0;32m    803\u001b[0m             )\n\u001b[1;32m--> 804\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\admin\\\\AppData\\\\Local\\\\Temp\\\\tmp_t4232ty.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('NP', [Tree('N', [Tree('Nba', ['嘉珍']), Tree('Caa', ['和']), Tree('Nhaa', ['我'])])]), Tree('VC1', ['住在']), Tree('NP', [Tree('DM', ['同一條']), Tree('Nab', ['巷子'])])])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinica_treebank.parsed_sents()[2] # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2007 to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['El',\n",
       " 'aumento',\n",
       " 'del',\n",
       " 'índice',\n",
       " 'de',\n",
       " 'desempleo',\n",
       " 'estadounidense',\n",
       " 'fortaleció',\n",
       " 'hoy',\n",
       " 'considerablemente',\n",
       " 'al',\n",
       " 'euro',\n",
       " ',',\n",
       " 'que',\n",
       " 'a',\n",
       " 'las',\n",
       " '15.35',\n",
       " 'GMT',\n",
       " 'se',\n",
       " 'cotizaba',\n",
       " 'en',\n",
       " 'el',\n",
       " 'mercado',\n",
       " 'de',\n",
       " 'divisas',\n",
       " 'de',\n",
       " 'Fráncfort',\n",
       " 'a',\n",
       " '0,9452_dólares',\n",
       " ',',\n",
       " 'frente_a',\n",
       " 'los',\n",
       " '0,9349_dólares',\n",
       " 'de',\n",
       " 'esta',\n",
       " 'mañana',\n",
       " '.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"conll2007\")\n",
    "from nltk.corpus import conll2007\n",
    "conll2007.sents('esp.train')[0] # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also provides a corpus reader for the York-Toronto-Helsinki Parsed Corpus of Old English Prose (YCOE); but the corpus itself is not included in the NLTK data package. If you install it yourself, you can use NLTK to access it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ycoe to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\ycoe.zip.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ycoe' from 'nltk.corpus' (D:\\Users\\admin\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-a9748fdbab36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ycoe\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mycoe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mycoe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsed_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cocuraC'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# doctest: +SKIP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ycoe' from 'nltk.corpus' (D:\\Users\\admin\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\__init__.py)"
     ]
    }
   ],
   "source": [
    "nltk.download(\"ycoe\")\n",
    "from nltk.corpus import ycoe\n",
    "for tree in ycoe.parsed_sents('cocuraC')[:4]:\n",
    "    print(tree) # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['en', 'en-basic']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word Lists and Lexicons\n",
    "#The NLTK data package also includes a number of lexicons and word lists. These are accessed just like text corpora. The following examples illustrate the use of the wordlist corpora:\n",
    "nltk.download(\"words\")\n",
    "from nltk.corpus import names, stopwords, words\n",
    "words.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\names.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abagael',\n",
       " 'Abagail',\n",
       " 'Abbe',\n",
       " 'Abbey',\n",
       " 'Abbi',\n",
       " 'Abbie',\n",
       " 'Abby',\n",
       " 'Abigael',\n",
       " 'Abigail',\n",
       " 'Abigale',\n",
       " 'Abra',\n",
       " 'Acacia',\n",
       " 'Ada',\n",
       " 'Adah',\n",
       " 'Adaline',\n",
       " 'Adara',\n",
       " 'Addie',\n",
       " 'Addis',\n",
       " 'Adel',\n",
       " 'Adela',\n",
       " 'Adelaide',\n",
       " 'Adele',\n",
       " 'Adelice',\n",
       " 'Adelina',\n",
       " 'Adelind',\n",
       " 'Adeline',\n",
       " 'Adella',\n",
       " 'Adelle',\n",
       " 'Adena',\n",
       " 'Adey',\n",
       " 'Adi',\n",
       " 'Adiana',\n",
       " 'Adina',\n",
       " 'Adora',\n",
       " 'Adore',\n",
       " 'Adoree',\n",
       " 'Adorne',\n",
       " 'Adrea',\n",
       " 'Adria',\n",
       " 'Adriaens',\n",
       " 'Adrian',\n",
       " 'Adriana',\n",
       " 'Adriane',\n",
       " 'Adrianna',\n",
       " 'Adrianne',\n",
       " 'Adrien',\n",
       " 'Adriena',\n",
       " 'Adrienne',\n",
       " 'Aeriel',\n",
       " 'Aeriela',\n",
       " 'Aeriell',\n",
       " 'Ag',\n",
       " 'Agace',\n",
       " 'Agata',\n",
       " 'Agatha',\n",
       " 'Agathe',\n",
       " 'Aggi',\n",
       " 'Aggie',\n",
       " 'Aggy',\n",
       " 'Agna',\n",
       " 'Agnella',\n",
       " 'Agnes',\n",
       " 'Agnese',\n",
       " 'Agnesse',\n",
       " 'Agneta',\n",
       " 'Agnola',\n",
       " 'Agretha',\n",
       " 'Aida',\n",
       " 'Aidan',\n",
       " 'Aigneis',\n",
       " 'Aila',\n",
       " 'Aile',\n",
       " 'Ailee',\n",
       " 'Aileen',\n",
       " 'Ailene',\n",
       " 'Ailey',\n",
       " 'Aili',\n",
       " 'Ailina',\n",
       " 'Ailyn',\n",
       " 'Aime',\n",
       " 'Aimee',\n",
       " 'Aimil',\n",
       " 'Aina',\n",
       " 'Aindrea',\n",
       " 'Ainslee',\n",
       " 'Ainsley',\n",
       " 'Ainslie',\n",
       " 'Ajay',\n",
       " 'Alaine',\n",
       " 'Alameda',\n",
       " 'Alana',\n",
       " 'Alanah',\n",
       " 'Alane',\n",
       " 'Alanna',\n",
       " 'Alayne',\n",
       " 'Alberta',\n",
       " 'Albertina',\n",
       " 'Albertine',\n",
       " 'Albina',\n",
       " 'Alecia',\n",
       " 'Aleda',\n",
       " 'Aleece',\n",
       " 'Aleecia',\n",
       " 'Aleen',\n",
       " 'Alejandra',\n",
       " 'Alejandrina',\n",
       " 'Alena',\n",
       " 'Alene',\n",
       " 'Alessandra',\n",
       " 'Aleta',\n",
       " 'Alethea',\n",
       " 'Alex',\n",
       " 'Alexa',\n",
       " 'Alexandra',\n",
       " 'Alexandrina',\n",
       " 'Alexi',\n",
       " 'Alexia',\n",
       " 'Alexina',\n",
       " 'Alexine',\n",
       " 'Alexis',\n",
       " 'Alfie',\n",
       " 'Alfreda',\n",
       " 'Ali',\n",
       " 'Alia',\n",
       " 'Alica',\n",
       " 'Alice',\n",
       " 'Alicea',\n",
       " 'Alicia',\n",
       " 'Alida',\n",
       " 'Alidia',\n",
       " 'Alina',\n",
       " 'Aline',\n",
       " 'Alis',\n",
       " 'Alisa',\n",
       " 'Alisha',\n",
       " 'Alison',\n",
       " 'Alissa',\n",
       " 'Alisun',\n",
       " 'Alix',\n",
       " 'Aliza',\n",
       " 'Alla',\n",
       " 'Alleen',\n",
       " 'Allegra',\n",
       " 'Allene',\n",
       " 'Alli',\n",
       " 'Allianora',\n",
       " 'Allie',\n",
       " 'Allina',\n",
       " 'Allis',\n",
       " 'Allison',\n",
       " 'Allissa',\n",
       " 'Allsun',\n",
       " 'Ally',\n",
       " 'Allyce',\n",
       " 'Allyn',\n",
       " 'Allys',\n",
       " 'Allyson',\n",
       " 'Alma',\n",
       " 'Almeda',\n",
       " 'Almeria',\n",
       " 'Almeta',\n",
       " 'Almira',\n",
       " 'Almire',\n",
       " 'Aloise',\n",
       " 'Aloisia',\n",
       " 'Aloysia',\n",
       " 'Alpa',\n",
       " 'Alta',\n",
       " 'Althea',\n",
       " 'Alvera',\n",
       " 'Alvina',\n",
       " 'Alvinia',\n",
       " 'Alvira',\n",
       " 'Alyce',\n",
       " 'Alyda',\n",
       " 'Alys',\n",
       " 'Alysa',\n",
       " 'Alyse',\n",
       " 'Alysia',\n",
       " 'Alyson',\n",
       " 'Alyss',\n",
       " 'Alyssa',\n",
       " 'Amabel',\n",
       " 'Amabelle',\n",
       " 'Amalea',\n",
       " 'Amalee',\n",
       " 'Amaleta',\n",
       " 'Amalia',\n",
       " 'Amalie',\n",
       " 'Amalita',\n",
       " 'Amalle',\n",
       " 'Amanda',\n",
       " 'Amandi',\n",
       " 'Amandie',\n",
       " 'Amandy',\n",
       " 'Amara',\n",
       " 'Amargo',\n",
       " 'Amata',\n",
       " 'Amber',\n",
       " 'Amberly',\n",
       " 'Ambrosia',\n",
       " 'Ambur',\n",
       " 'Ame',\n",
       " 'Amelia',\n",
       " 'Amelie',\n",
       " 'Amelina',\n",
       " 'Ameline',\n",
       " 'Amelita',\n",
       " 'Ami',\n",
       " 'Amie',\n",
       " 'Amity',\n",
       " 'Ammamaria',\n",
       " 'Amy',\n",
       " 'Ana',\n",
       " 'Anabel',\n",
       " 'Anabella',\n",
       " 'Anabelle',\n",
       " 'Anais',\n",
       " 'Analiese',\n",
       " 'Analise',\n",
       " 'Anallese',\n",
       " 'Anallise',\n",
       " 'Anastasia',\n",
       " 'Anastasie',\n",
       " 'Anastassia',\n",
       " 'Anatola',\n",
       " 'Andee',\n",
       " 'Andi',\n",
       " 'Andie',\n",
       " 'Andra',\n",
       " 'Andrea',\n",
       " 'Andreana',\n",
       " 'Andree',\n",
       " 'Andrei',\n",
       " 'Andria',\n",
       " 'Andriana',\n",
       " 'Andriette',\n",
       " 'Andromache',\n",
       " 'Andromeda',\n",
       " 'Andy',\n",
       " 'Anestassia',\n",
       " 'Anet',\n",
       " 'Anett',\n",
       " 'Anetta',\n",
       " 'Anette',\n",
       " 'Ange',\n",
       " 'Angel',\n",
       " 'Angela',\n",
       " 'Angele',\n",
       " 'Angelia',\n",
       " 'Angelica',\n",
       " 'Angelika',\n",
       " 'Angelina',\n",
       " 'Angeline',\n",
       " 'Angelique',\n",
       " 'Angelita',\n",
       " 'Angelle',\n",
       " 'Angie',\n",
       " 'Angil',\n",
       " 'Angy',\n",
       " 'Ania',\n",
       " 'Anica',\n",
       " 'Anissa',\n",
       " 'Anita',\n",
       " 'Anitra',\n",
       " 'Anja',\n",
       " 'Anjanette',\n",
       " 'Anjela',\n",
       " 'Ann',\n",
       " 'Ann-Mari',\n",
       " 'Ann-Marie',\n",
       " 'Anna',\n",
       " 'Anna-Diana',\n",
       " 'Anna-Diane',\n",
       " 'Anna-Maria',\n",
       " 'Annabal',\n",
       " 'Annabel',\n",
       " 'Annabela',\n",
       " 'Annabell',\n",
       " 'Annabella',\n",
       " 'Annabelle',\n",
       " 'Annadiana',\n",
       " 'Annadiane',\n",
       " 'Annalee',\n",
       " 'Annalena',\n",
       " 'Annaliese',\n",
       " 'Annalisa',\n",
       " 'Annalise',\n",
       " 'Annalyse',\n",
       " 'Annamari',\n",
       " 'Annamaria',\n",
       " 'Annamarie',\n",
       " 'Anne',\n",
       " 'Anne-Corinne',\n",
       " 'Anne-Mar',\n",
       " 'Anne-Marie',\n",
       " 'Annecorinne',\n",
       " 'Anneliese',\n",
       " 'Annelise',\n",
       " 'Annemarie',\n",
       " 'Annetta',\n",
       " 'Annette',\n",
       " 'Anni',\n",
       " 'Annice',\n",
       " 'Annie',\n",
       " 'Annissa',\n",
       " 'Annmaria',\n",
       " 'Annmarie',\n",
       " 'Annnora',\n",
       " 'Annora',\n",
       " 'Anny',\n",
       " 'Anselma',\n",
       " 'Ansley',\n",
       " 'Anstice',\n",
       " 'Anthe',\n",
       " 'Anthea',\n",
       " 'Anthia',\n",
       " 'Antoinette',\n",
       " 'Antonella',\n",
       " 'Antonetta',\n",
       " 'Antonia',\n",
       " 'Antonie',\n",
       " 'Antonietta',\n",
       " 'Antonina',\n",
       " 'Anya',\n",
       " 'Aphrodite',\n",
       " 'Appolonia',\n",
       " 'April',\n",
       " 'Aprilette',\n",
       " 'Ara',\n",
       " 'Arabel',\n",
       " 'Arabela',\n",
       " 'Arabele',\n",
       " 'Arabella',\n",
       " 'Arabelle',\n",
       " 'Arda',\n",
       " 'Ardath',\n",
       " 'Ardeen',\n",
       " 'Ardelia',\n",
       " 'Ardelis',\n",
       " 'Ardella',\n",
       " 'Ardelle',\n",
       " 'Arden',\n",
       " 'Ardene',\n",
       " 'Ardenia',\n",
       " 'Ardine',\n",
       " 'Ardis',\n",
       " 'Ardith',\n",
       " 'Ardra',\n",
       " 'Ardyce',\n",
       " 'Ardys',\n",
       " 'Ardyth',\n",
       " 'Aretha',\n",
       " 'Ariadne',\n",
       " 'Ariana',\n",
       " 'Arianne',\n",
       " 'Aridatha',\n",
       " 'Ariel',\n",
       " 'Ariela',\n",
       " 'Ariella',\n",
       " 'Arielle',\n",
       " 'Arlana',\n",
       " 'Arlee',\n",
       " 'Arleen',\n",
       " 'Arlen',\n",
       " 'Arlena',\n",
       " 'Arlene',\n",
       " 'Arleta',\n",
       " 'Arlette',\n",
       " 'Arleyne',\n",
       " 'Arlie',\n",
       " 'Arliene',\n",
       " 'Arlina',\n",
       " 'Arlinda',\n",
       " 'Arline',\n",
       " 'Arly',\n",
       " 'Arlyn',\n",
       " 'Arlyne',\n",
       " 'Aryn',\n",
       " 'Ashely',\n",
       " 'Ashlee',\n",
       " 'Ashleigh',\n",
       " 'Ashlen',\n",
       " 'Ashley',\n",
       " 'Ashli',\n",
       " 'Ashlie',\n",
       " 'Ashly',\n",
       " 'Asia',\n",
       " 'Astra',\n",
       " 'Astrid',\n",
       " 'Astrix',\n",
       " 'Atalanta',\n",
       " 'Athena',\n",
       " 'Athene',\n",
       " 'Atlanta',\n",
       " 'Atlante',\n",
       " 'Auberta',\n",
       " 'Aubine',\n",
       " 'Aubree',\n",
       " 'Aubrette',\n",
       " 'Aubrey',\n",
       " 'Aubrie',\n",
       " 'Aubry',\n",
       " 'Audi',\n",
       " 'Audie',\n",
       " 'Audra',\n",
       " 'Audre',\n",
       " 'Audrey',\n",
       " 'Audrie',\n",
       " 'Audry',\n",
       " 'Audrye',\n",
       " 'Audy',\n",
       " 'Augusta',\n",
       " 'Auguste',\n",
       " 'Augustina',\n",
       " 'Augustine',\n",
       " 'Aura',\n",
       " 'Aurea',\n",
       " 'Aurel',\n",
       " 'Aurelea',\n",
       " 'Aurelia',\n",
       " 'Aurelie',\n",
       " 'Auria',\n",
       " 'Aurie',\n",
       " 'Aurilia',\n",
       " 'Aurlie',\n",
       " 'Auroora',\n",
       " 'Aurora',\n",
       " 'Aurore',\n",
       " 'Austin',\n",
       " 'Austina',\n",
       " 'Austine',\n",
       " 'Ava',\n",
       " 'Aveline',\n",
       " 'Averil',\n",
       " 'Averyl',\n",
       " 'Avie',\n",
       " 'Avis',\n",
       " 'Aviva',\n",
       " 'Avivah',\n",
       " 'Avril',\n",
       " 'Avrit',\n",
       " 'Ayn',\n",
       " 'Bab',\n",
       " 'Babara',\n",
       " 'Babette',\n",
       " 'Babita',\n",
       " 'Babs',\n",
       " 'Bambi',\n",
       " 'Bambie',\n",
       " 'Bamby',\n",
       " 'Barb',\n",
       " 'Barbabra',\n",
       " 'Barbara',\n",
       " 'Barbara-Anne',\n",
       " 'Barbaraanne',\n",
       " 'Barbe',\n",
       " 'Barbee',\n",
       " 'Barbette',\n",
       " 'Barbey',\n",
       " 'Barbi',\n",
       " 'Barbie',\n",
       " 'Barbra',\n",
       " 'Barby',\n",
       " 'Bari',\n",
       " 'Barrie',\n",
       " 'Barry',\n",
       " 'Basia',\n",
       " 'Bathsheba',\n",
       " 'Batsheva',\n",
       " 'Bea',\n",
       " 'Beatrice',\n",
       " 'Beatrisa',\n",
       " 'Beatrix',\n",
       " 'Beatriz',\n",
       " 'Beau',\n",
       " 'Bebe',\n",
       " 'Becca',\n",
       " 'Becka',\n",
       " 'Becki',\n",
       " 'Beckie',\n",
       " 'Becky',\n",
       " 'Bee',\n",
       " 'Beilul',\n",
       " 'Beitris',\n",
       " 'Bekki',\n",
       " 'Bel',\n",
       " 'Belia',\n",
       " 'Belicia',\n",
       " 'Belinda',\n",
       " 'Belita',\n",
       " 'Bell',\n",
       " 'Bella',\n",
       " 'Bellamy',\n",
       " 'Bellanca',\n",
       " 'Belle',\n",
       " 'Bellina',\n",
       " 'Belva',\n",
       " 'Belvia',\n",
       " 'Bendite',\n",
       " 'Benedetta',\n",
       " 'Benedicta',\n",
       " 'Benedikta',\n",
       " 'Benetta',\n",
       " 'Benita',\n",
       " 'Benni',\n",
       " 'Bennie',\n",
       " 'Benny',\n",
       " 'Benoite',\n",
       " 'Berenice',\n",
       " 'Beret',\n",
       " 'Berget',\n",
       " 'Berna',\n",
       " 'Bernadene',\n",
       " 'Bernadette',\n",
       " 'Bernadina',\n",
       " 'Bernadine',\n",
       " 'Bernardina',\n",
       " 'Bernardine',\n",
       " 'Bernelle',\n",
       " 'Bernete',\n",
       " 'Bernetta',\n",
       " 'Bernette',\n",
       " 'Berni',\n",
       " 'Bernice',\n",
       " 'Bernie',\n",
       " 'Bernita',\n",
       " 'Berny',\n",
       " 'Berri',\n",
       " 'Berrie',\n",
       " 'Berry',\n",
       " 'Bert',\n",
       " 'Berta',\n",
       " 'Berte',\n",
       " 'Bertha',\n",
       " 'Berthe',\n",
       " 'Berti',\n",
       " 'Bertie',\n",
       " 'Bertina',\n",
       " 'Bertine',\n",
       " 'Berty',\n",
       " 'Beryl',\n",
       " 'Beryle',\n",
       " 'Bess',\n",
       " 'Bessie',\n",
       " 'Bessy',\n",
       " 'Beth',\n",
       " 'Bethanne',\n",
       " 'Bethany',\n",
       " 'Bethena',\n",
       " 'Bethina',\n",
       " 'Betsey',\n",
       " 'Betsy',\n",
       " 'Betta',\n",
       " 'Bette',\n",
       " 'Bette-Ann',\n",
       " 'Betteann',\n",
       " 'Betteanne',\n",
       " 'Betti',\n",
       " 'Bettie',\n",
       " 'Bettina',\n",
       " 'Bettine',\n",
       " 'Betty',\n",
       " 'Bettye',\n",
       " 'Beulah',\n",
       " 'Bev',\n",
       " 'Beverie',\n",
       " 'Beverlee',\n",
       " 'Beverlie',\n",
       " 'Beverly',\n",
       " 'Bevvy',\n",
       " 'Bianca',\n",
       " 'Bianka',\n",
       " 'Biddy',\n",
       " 'Bidget',\n",
       " 'Bill',\n",
       " 'Billi',\n",
       " 'Billie',\n",
       " 'Billy',\n",
       " 'Binni',\n",
       " 'Binnie',\n",
       " 'Binny',\n",
       " 'Bird',\n",
       " 'Birdie',\n",
       " 'Birgit',\n",
       " 'Birgitta',\n",
       " 'Blair',\n",
       " 'Blaire',\n",
       " 'Blake',\n",
       " 'Blakelee',\n",
       " 'Blakeley',\n",
       " 'Blanca',\n",
       " 'Blanch',\n",
       " 'Blancha',\n",
       " 'Blanche',\n",
       " 'Blinni',\n",
       " 'Blinnie',\n",
       " 'Blinny',\n",
       " 'Bliss',\n",
       " 'Blisse',\n",
       " 'Blithe',\n",
       " 'Blondell',\n",
       " 'Blondelle',\n",
       " 'Blondie',\n",
       " 'Blondy',\n",
       " 'Blythe',\n",
       " 'Bo',\n",
       " 'Bobbette',\n",
       " 'Bobbi',\n",
       " 'Bobbie',\n",
       " 'Bobby',\n",
       " 'Bobette',\n",
       " 'Bobina',\n",
       " 'Bobine',\n",
       " 'Bobinette',\n",
       " 'Bonita',\n",
       " 'Bonnee',\n",
       " 'Bonni',\n",
       " 'Bonnie',\n",
       " 'Bonny',\n",
       " 'Brana',\n",
       " 'Brandais',\n",
       " 'Brande',\n",
       " 'Brandea',\n",
       " 'Brandi',\n",
       " 'Brandice',\n",
       " 'Brandie',\n",
       " 'Brandise',\n",
       " 'Brandy',\n",
       " 'Brea',\n",
       " 'Breanne',\n",
       " 'Brear',\n",
       " 'Bree',\n",
       " 'Breena',\n",
       " 'Bren',\n",
       " 'Brena',\n",
       " 'Brenda',\n",
       " 'Brenn',\n",
       " 'Brenna',\n",
       " 'Brett',\n",
       " 'Bria',\n",
       " 'Briana',\n",
       " 'Brianna',\n",
       " 'Brianne',\n",
       " 'Bride',\n",
       " 'Bridget',\n",
       " 'Bridgett',\n",
       " 'Bridgette',\n",
       " 'Bridie',\n",
       " 'Brier',\n",
       " 'Brietta',\n",
       " 'Brigid',\n",
       " 'Brigida',\n",
       " 'Brigit',\n",
       " 'Brigitta',\n",
       " 'Brigitte',\n",
       " 'Brina',\n",
       " 'Briney',\n",
       " 'Briny',\n",
       " 'Brit',\n",
       " 'Brita',\n",
       " 'Britaney',\n",
       " 'Britani',\n",
       " 'Briteny',\n",
       " 'Britney',\n",
       " 'Britni',\n",
       " 'Britt',\n",
       " 'Britta',\n",
       " 'Brittan',\n",
       " 'Brittany',\n",
       " 'Britte',\n",
       " 'Brittney',\n",
       " 'Brook',\n",
       " 'Brooke',\n",
       " 'Brooks',\n",
       " 'Brunella',\n",
       " 'Brunhilda',\n",
       " 'Brunhilde',\n",
       " 'Bryana',\n",
       " 'Bryn',\n",
       " 'Bryna',\n",
       " 'Brynn',\n",
       " 'Brynna',\n",
       " 'Brynne',\n",
       " 'Buffy',\n",
       " 'Bunni',\n",
       " 'Bunnie',\n",
       " 'Bunny',\n",
       " 'Burta',\n",
       " 'Cabrina',\n",
       " 'Cacilia',\n",
       " 'Cacilie',\n",
       " 'Caitlin',\n",
       " 'Caitrin',\n",
       " 'Cal',\n",
       " 'Calida',\n",
       " 'Calla',\n",
       " 'Calley',\n",
       " 'Calli',\n",
       " 'Callida',\n",
       " 'Callie',\n",
       " 'Cally',\n",
       " 'Calypso',\n",
       " 'Cam',\n",
       " 'Camala',\n",
       " 'Camel',\n",
       " 'Camella',\n",
       " 'Camellia',\n",
       " 'Cameo',\n",
       " 'Cami',\n",
       " 'Camila',\n",
       " 'Camile',\n",
       " 'Camilla',\n",
       " 'Camille',\n",
       " 'Cammi',\n",
       " 'Cammie',\n",
       " 'Cammy',\n",
       " 'Canada',\n",
       " 'Candace',\n",
       " 'Candi',\n",
       " 'Candice',\n",
       " 'Candida',\n",
       " 'Candide',\n",
       " 'Candie',\n",
       " 'Candis',\n",
       " 'Candra',\n",
       " 'Candy',\n",
       " 'Cappella',\n",
       " 'Caprice',\n",
       " 'Cara',\n",
       " 'Caralie',\n",
       " 'Caren',\n",
       " 'Carena',\n",
       " 'Caresa',\n",
       " 'Caressa',\n",
       " 'Caresse',\n",
       " 'Carey',\n",
       " 'Cari',\n",
       " 'Caria',\n",
       " 'Carie',\n",
       " 'Caril',\n",
       " 'Carilyn',\n",
       " 'Carin',\n",
       " 'Carina',\n",
       " 'Carine',\n",
       " 'Cariotta',\n",
       " 'Carissa',\n",
       " 'Carita',\n",
       " 'Caritta',\n",
       " 'Carla',\n",
       " 'Carlee',\n",
       " 'Carleen',\n",
       " 'Carlen',\n",
       " 'Carlena',\n",
       " 'Carlene',\n",
       " 'Carley',\n",
       " 'Carli',\n",
       " 'Carlie',\n",
       " 'Carlin',\n",
       " 'Carlina',\n",
       " 'Carline',\n",
       " 'Carlisle',\n",
       " 'Carlita',\n",
       " 'Carlota',\n",
       " 'Carlotta',\n",
       " 'Carly',\n",
       " 'Carlye',\n",
       " 'Carlyn',\n",
       " 'Carlynn',\n",
       " 'Carlynne',\n",
       " 'Carma',\n",
       " 'Carmel',\n",
       " 'Carmela',\n",
       " 'Carmelia',\n",
       " 'Carmelina',\n",
       " 'Carmelita',\n",
       " 'Carmella',\n",
       " 'Carmelle',\n",
       " 'Carmen',\n",
       " 'Carmina',\n",
       " 'Carmine',\n",
       " 'Carmita',\n",
       " 'Carmon',\n",
       " 'Caro',\n",
       " 'Carol',\n",
       " 'Carol-Jean',\n",
       " 'Carola',\n",
       " 'Carolan',\n",
       " 'Carolann',\n",
       " 'Carole',\n",
       " 'Carolee',\n",
       " 'Caroleen',\n",
       " 'Carolie',\n",
       " 'Carolin',\n",
       " 'Carolina',\n",
       " 'Caroline',\n",
       " 'Caroljean',\n",
       " 'Carolyn',\n",
       " 'Carolyne',\n",
       " 'Carolynn',\n",
       " 'Caron',\n",
       " 'Carree',\n",
       " 'Carri',\n",
       " 'Carrie',\n",
       " 'Carrissa',\n",
       " 'Carrol',\n",
       " 'Carroll',\n",
       " 'Carry',\n",
       " 'Cary',\n",
       " 'Caryl',\n",
       " 'Caryn',\n",
       " 'Casandra',\n",
       " 'Casey',\n",
       " 'Casi',\n",
       " 'Casia',\n",
       " 'Casie',\n",
       " 'Cass',\n",
       " 'Cassandra',\n",
       " 'Cassandre',\n",
       " 'Cassandry',\n",
       " 'Cassaundra',\n",
       " 'Cassey',\n",
       " 'Cassi',\n",
       " 'Cassie',\n",
       " 'Cassondra',\n",
       " 'Cassy',\n",
       " 'Cat',\n",
       " 'Catarina',\n",
       " 'Cate',\n",
       " 'Caterina',\n",
       " 'Catha',\n",
       " 'Catharina',\n",
       " 'Catharine',\n",
       " 'Cathe',\n",
       " 'Cathee',\n",
       " 'Catherin',\n",
       " 'Catherina',\n",
       " 'Catherine',\n",
       " 'Cathi',\n",
       " 'Cathie',\n",
       " 'Cathleen',\n",
       " 'Cathlene',\n",
       " 'Cathrin',\n",
       " 'Cathrine',\n",
       " 'Cathryn',\n",
       " 'Cathy',\n",
       " 'Cathyleen',\n",
       " 'Cati',\n",
       " 'Catie',\n",
       " 'Catina',\n",
       " 'Catlaina',\n",
       " 'Catlee',\n",
       " 'Catlin',\n",
       " 'Catrina',\n",
       " 'Catriona',\n",
       " 'Caty',\n",
       " 'Cayla',\n",
       " 'Cecelia',\n",
       " 'Cecil',\n",
       " 'Cecile',\n",
       " 'Ceciley',\n",
       " 'Cecilia',\n",
       " 'Cecilla',\n",
       " 'Cecily',\n",
       " 'Ceil',\n",
       " 'Cele',\n",
       " 'Celene',\n",
       " 'Celesta',\n",
       " 'Celeste',\n",
       " 'Celestia',\n",
       " 'Celestina',\n",
       " 'Celestine',\n",
       " 'Celestyn',\n",
       " 'Celestyna',\n",
       " 'Celia',\n",
       " 'Celie',\n",
       " 'Celina',\n",
       " 'Celinda',\n",
       " 'Celine',\n",
       " 'Celinka',\n",
       " 'Celisse',\n",
       " 'Celle',\n",
       " 'Cesya',\n",
       " 'Chad',\n",
       " 'Chanda',\n",
       " 'Chandal',\n",
       " 'Chandra',\n",
       " 'Channa',\n",
       " 'Chantal',\n",
       " 'Chantalle',\n",
       " 'Charil',\n",
       " 'Charin',\n",
       " 'Charis',\n",
       " 'Charissa',\n",
       " 'Charisse',\n",
       " 'Charita',\n",
       " 'Charity',\n",
       " 'Charla',\n",
       " 'Charlean',\n",
       " 'Charleen',\n",
       " 'Charlena',\n",
       " 'Charlene',\n",
       " 'Charline',\n",
       " 'Charlot',\n",
       " 'Charlott',\n",
       " 'Charlotta',\n",
       " 'Charlotte',\n",
       " 'Charmain',\n",
       " 'Charmaine',\n",
       " 'Charmane',\n",
       " 'Charmian',\n",
       " 'Charmine',\n",
       " 'Charmion',\n",
       " 'Charo',\n",
       " 'Charyl',\n",
       " 'Chastity',\n",
       " 'Chelsae',\n",
       " 'Chelsea',\n",
       " 'Chelsey',\n",
       " 'Chelsie',\n",
       " 'Chelsy',\n",
       " 'Cher',\n",
       " 'Chere',\n",
       " 'Cherey',\n",
       " 'Cheri',\n",
       " 'Cherianne',\n",
       " 'Cherice',\n",
       " 'Cherida',\n",
       " 'Cherie',\n",
       " 'Cherilyn',\n",
       " 'Cherilynn',\n",
       " 'Cherin',\n",
       " 'Cherise',\n",
       " 'Cherish',\n",
       " 'Cherlyn',\n",
       " 'Cherri',\n",
       " 'Cherrita',\n",
       " 'Cherry',\n",
       " 'Chery',\n",
       " 'Cherye',\n",
       " 'Cheryl',\n",
       " 'Cheslie',\n",
       " 'Chiarra',\n",
       " 'Chickie',\n",
       " 'Chicky',\n",
       " 'Chiquita',\n",
       " 'Chloe',\n",
       " 'Chloette',\n",
       " 'Chloris',\n",
       " 'Chris',\n",
       " 'Chriss',\n",
       " 'Chrissa',\n",
       " 'Chrissie',\n",
       " 'Chrissy',\n",
       " 'Christa',\n",
       " 'Christabel',\n",
       " 'Christabella',\n",
       " 'Christabelle',\n",
       " 'Christal',\n",
       " 'Christalle',\n",
       " 'Christan',\n",
       " 'Christean',\n",
       " 'Christel',\n",
       " 'Christen',\n",
       " 'Christi',\n",
       " 'Christian',\n",
       " 'Christiana',\n",
       " 'Christiane',\n",
       " 'Christie',\n",
       " 'Christin',\n",
       " 'Christina',\n",
       " 'Christine',\n",
       " 'Christy',\n",
       " 'Christyna',\n",
       " 'Chrysa',\n",
       " 'Chrysler',\n",
       " 'Chrystal',\n",
       " 'Chryste',\n",
       " 'Chrystel',\n",
       " 'Ciara',\n",
       " 'Cicely',\n",
       " 'Cicily',\n",
       " 'Ciel',\n",
       " 'Cilka',\n",
       " 'Cinda',\n",
       " 'Cindee',\n",
       " 'Cindelyn',\n",
       " 'Cinderella',\n",
       " 'Cindi',\n",
       " 'Cindie',\n",
       " 'Cindra',\n",
       " 'Cindy',\n",
       " 'Cinnamon',\n",
       " 'Cissie',\n",
       " 'Cissy',\n",
       " 'Clair',\n",
       " 'Claire',\n",
       " 'Clara',\n",
       " 'Clarabelle',\n",
       " 'Clare',\n",
       " ...]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.words('en') # doctest: +ELLIPSIS\n",
    "#['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', ...]\n",
    "stopwords.fileids() # doctest: +ELLIPSIS\n",
    "#['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', ...]\n",
    "stopwords.words('portuguese') # doctest: +ELLIPSIS\n",
    "#['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', ...]\n",
    "names.fileids()\n",
    "#['female.txt', 'male.txt']\n",
    "names.words('male.txt') # doctest: +ELLIPSIS\n",
    "#['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', ...]\n",
    "names.words('female.txt') # doctest: +ELLIPSIS\n",
    "#['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Giovanni',\n",
       " 'Giraldo',\n",
       " 'Giraud',\n",
       " 'Giuseppe',\n",
       " 'Glen',\n",
       " 'Glenn',\n",
       " 'Glynn',\n",
       " 'Godard',\n",
       " 'Godart',\n",
       " 'Goddard',\n",
       " 'Goddart',\n",
       " 'Godfree',\n",
       " 'Godfrey',\n",
       " 'Godfry',\n",
       " 'Godwin',\n",
       " 'Gomer',\n",
       " 'Gonzales',\n",
       " 'Gonzalo',\n",
       " 'Goober',\n",
       " 'Goose',\n",
       " 'Gordan',\n",
       " 'Gordie',\n",
       " 'Gordon',\n",
       " 'Grace',\n",
       " 'Grady',\n",
       " 'Graehme',\n",
       " 'Graeme',\n",
       " 'Graham',\n",
       " 'Graig',\n",
       " 'Grant',\n",
       " 'Granville',\n",
       " 'Greg',\n",
       " 'Gregg',\n",
       " 'Greggory',\n",
       " 'Gregor',\n",
       " 'Gregorio',\n",
       " 'Gregory',\n",
       " 'Gretchen',\n",
       " 'Griff',\n",
       " 'Griffin',\n",
       " 'Griffith',\n",
       " 'Griswold',\n",
       " 'Grove',\n",
       " 'Grover',\n",
       " 'Guido',\n",
       " 'Guillaume',\n",
       " 'Guillermo',\n",
       " 'Gunner',\n",
       " 'Gunter',\n",
       " 'Gunther',\n",
       " 'Gus',\n",
       " 'Gustaf',\n",
       " 'Gustav',\n",
       " 'Gustave',\n",
       " 'Gustavo',\n",
       " 'Gustavus',\n",
       " 'Guthrey',\n",
       " 'Guthrie',\n",
       " 'Guthry',\n",
       " 'Guy',\n",
       " 'Hadleigh',\n",
       " 'Hadley',\n",
       " 'Hadrian',\n",
       " 'Hagan',\n",
       " 'Hagen',\n",
       " 'Hailey',\n",
       " 'Hakeem',\n",
       " 'Hakim',\n",
       " 'Hal',\n",
       " 'Hale',\n",
       " 'Haleigh',\n",
       " 'Haley',\n",
       " 'Hall',\n",
       " 'Hallam',\n",
       " 'Halvard',\n",
       " 'Ham',\n",
       " 'Hamel',\n",
       " 'Hamid',\n",
       " 'Hamil',\n",
       " 'Hamilton',\n",
       " 'Hamish',\n",
       " 'Hamlen',\n",
       " 'Hamlet',\n",
       " 'Hamlin',\n",
       " 'Hammad',\n",
       " 'Hamnet',\n",
       " 'Han',\n",
       " 'Hanan',\n",
       " 'Hanford',\n",
       " 'Hank',\n",
       " 'Hannibal',\n",
       " 'Hans',\n",
       " 'Hans-Peter',\n",
       " 'Hansel',\n",
       " 'Hanson',\n",
       " 'Harald',\n",
       " 'Harcourt',\n",
       " 'Hari',\n",
       " 'Harlan',\n",
       " 'Harland',\n",
       " 'Harley',\n",
       " 'Harlin',\n",
       " 'Harman',\n",
       " 'Harmon',\n",
       " 'Harold',\n",
       " 'Harris',\n",
       " 'Harrison',\n",
       " 'Harrold',\n",
       " 'Harry',\n",
       " 'Hart',\n",
       " 'Hartley',\n",
       " 'Hartwell',\n",
       " 'Harv',\n",
       " 'Harvard',\n",
       " 'Harvey',\n",
       " 'Harvie',\n",
       " 'Harwell',\n",
       " 'Hasheem',\n",
       " 'Hashim',\n",
       " 'Haskel',\n",
       " 'Haskell',\n",
       " 'Hassan',\n",
       " 'Hastings',\n",
       " 'Hasty',\n",
       " 'Haven',\n",
       " 'Hayden',\n",
       " 'Haydon',\n",
       " 'Hayes',\n",
       " 'Hayward',\n",
       " 'Haywood',\n",
       " 'Hazel',\n",
       " 'Heath',\n",
       " 'Heathcliff',\n",
       " 'Hebert',\n",
       " 'Hector',\n",
       " 'Heinrich',\n",
       " 'Heinz',\n",
       " 'Helmuth',\n",
       " 'Henderson',\n",
       " 'Hendrick',\n",
       " 'Hendrik',\n",
       " 'Henri',\n",
       " 'Henrie',\n",
       " 'Henrik',\n",
       " 'Henrique',\n",
       " 'Henry',\n",
       " 'Herb',\n",
       " 'Herbert',\n",
       " 'Herbie',\n",
       " 'Herby',\n",
       " 'Hercule',\n",
       " 'Hercules',\n",
       " 'Herculie',\n",
       " 'Herman',\n",
       " 'Hermann',\n",
       " 'Hermon',\n",
       " 'Hermy',\n",
       " 'Hernando',\n",
       " 'Herold',\n",
       " 'Herrick',\n",
       " 'Herrmann',\n",
       " 'Hersch',\n",
       " 'Herschel',\n",
       " 'Hersh',\n",
       " 'Hershel',\n",
       " 'Herve',\n",
       " 'Hervey',\n",
       " 'Hew',\n",
       " 'Hewe',\n",
       " 'Hewet',\n",
       " 'Hewett',\n",
       " 'Hewie',\n",
       " 'Hewitt',\n",
       " 'Heywood',\n",
       " 'Hezekiah',\n",
       " 'Higgins',\n",
       " 'Hilary',\n",
       " 'Hilbert',\n",
       " 'Hill',\n",
       " 'Hillard',\n",
       " 'Hillary',\n",
       " 'Hillel',\n",
       " 'Hillery',\n",
       " 'Hilliard',\n",
       " 'Hilton',\n",
       " 'Hiralal',\n",
       " 'Hiram',\n",
       " 'Hiro',\n",
       " 'Hirsch',\n",
       " 'Hobart',\n",
       " 'Hodge',\n",
       " 'Hogan',\n",
       " 'Hollis',\n",
       " 'Holly',\n",
       " 'Homer',\n",
       " 'Horace',\n",
       " 'Horacio',\n",
       " 'Horatio',\n",
       " 'Horatius',\n",
       " 'Horst']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.words('male.txt')[1000:1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CMU Pronunciation Dictionary corpus contains pronounciation transcriptions for over 100,000 words. It can be accessed as a list of entries (where each entry consists of a word, an identifier, and a transcription) or as a dictionary from words to lists of transcriptions. Transcriptions are encoded as tuples of phoneme strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\cmudict.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('acetate', ['AE1', 'S', 'AH0', 'T', 'EY2', 'T']), ('acetic', ['AH0', 'S', 'EH1', 'T', 'IH0', 'K']), ('acetic', ['AH0', 'S', 'IY1', 'T', 'IH0', 'K']), ('aceto', ['AA0', 'S', 'EH1', 'T', 'OW0']), ('acetochlor', ['AA0', 'S', 'EH1', 'T', 'OW0', 'K', 'L', 'AO2', 'R']), ('acetone', ['AE1', 'S', 'AH0', 'T', 'OW2', 'N'])]\n",
      "[['N', 'AE1', 'CH', 'ER0', 'AH0', 'L'], ['L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH'], ['T', 'UW1', 'L'], ['K', 'IH1', 'T']]\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"cmudict\")\n",
    "from nltk.corpus import cmudict\n",
    "print(cmudict.entries()[653:659]) # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "#[('acetate', ['AE1', 'S', 'AH0', 'T', 'EY2', 'T']),\n",
    "#('acetic', ['AH0', 'S', 'EH1', 'T', 'IH0', 'K']),\n",
    "#('acetic', ['AH0', 'S', 'IY1', 'T', 'IH0', 'K']),\n",
    "#('aceto', ['AA0', 'S', 'EH1', 'T', 'OW0']),\n",
    "#('acetochlor', ['AA0', 'S', 'EH1', 'T', 'OW0', 'K', 'L', 'AO2', 'R']),\n",
    "#('acetone', ['AE1', 'S', 'AH0', 'T', 'OW2', 'N'])]\n",
    "# Load the entire cmudict corpus into a Python dictionary:\n",
    "transcr = cmudict.dict()\n",
    "print([transcr[w][0] for w in 'Natural Language Tool Kit'.lower().split()]) # doctest: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading SentiWordNet: Package 'SentiWordNet' not\n",
      "[nltk_data]     found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"SentiWordNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet\n",
    "Please see the separate WordNet howto.\n",
    "\n",
    "FrameNet\n",
    "Please see the separate FrameNet howto.\n",
    "\n",
    "PropBank\n",
    "Please see the separate PropBank howto.\n",
    "\n",
    "SentiWordNet\n",
    "Please see the separate SentiWordNet howto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to C:\\Users\\admin/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['acq',\n",
       " 'alum',\n",
       " 'barley',\n",
       " 'bop',\n",
       " 'carcass',\n",
       " 'castor-oil',\n",
       " 'cocoa',\n",
       " 'coconut',\n",
       " 'coconut-oil',\n",
       " 'coffee',\n",
       " 'copper',\n",
       " 'copra-cake',\n",
       " 'corn',\n",
       " 'cotton',\n",
       " 'cotton-oil',\n",
       " 'cpi',\n",
       " 'cpu',\n",
       " 'crude',\n",
       " 'dfl',\n",
       " 'dlr',\n",
       " 'dmk',\n",
       " 'earn',\n",
       " 'fuel',\n",
       " 'gas',\n",
       " 'gnp',\n",
       " 'gold',\n",
       " 'grain',\n",
       " 'groundnut',\n",
       " 'groundnut-oil',\n",
       " 'heat',\n",
       " 'hog',\n",
       " 'housing',\n",
       " 'income',\n",
       " 'instal-debt',\n",
       " 'interest',\n",
       " 'ipi',\n",
       " 'iron-steel',\n",
       " 'jet',\n",
       " 'jobs',\n",
       " 'l-cattle',\n",
       " 'lead',\n",
       " 'lei',\n",
       " 'lin-oil',\n",
       " 'livestock',\n",
       " 'lumber',\n",
       " 'meal-feed',\n",
       " 'money-fx',\n",
       " 'money-supply',\n",
       " 'naphtha',\n",
       " 'nat-gas',\n",
       " 'nickel',\n",
       " 'nkr',\n",
       " 'nzdlr',\n",
       " 'oat',\n",
       " 'oilseed',\n",
       " 'orange',\n",
       " 'palladium',\n",
       " 'palm-oil',\n",
       " 'palmkernel',\n",
       " 'pet-chem',\n",
       " 'platinum',\n",
       " 'potato',\n",
       " 'propane',\n",
       " 'rand',\n",
       " 'rape-oil',\n",
       " 'rapeseed',\n",
       " 'reserves',\n",
       " 'retail',\n",
       " 'rice',\n",
       " 'rubber',\n",
       " 'rye',\n",
       " 'ship',\n",
       " 'silver',\n",
       " 'sorghum',\n",
       " 'soy-meal',\n",
       " 'soy-oil',\n",
       " 'soybean',\n",
       " 'strategic-metal',\n",
       " 'sugar',\n",
       " 'sun-meal',\n",
       " 'sun-oil',\n",
       " 'sunseed',\n",
       " 'tea',\n",
       " 'tin',\n",
       " 'trade',\n",
       " 'veg-oil',\n",
       " 'wheat',\n",
       " 'wpi',\n",
       " 'yen',\n",
       " 'zinc']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"movie_reviews\")\n",
    "nltk.download(\"reuters\")\n",
    "from nltk.corpus import brown, movie_reviews, reuters\n",
    "brown.categories() # doctest: +NORMALIZE_WHITESPACE\n",
    "#['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor',\n",
    "#'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
    "\n",
    "movie_reviews.categories()\n",
    "#['neg', 'pos']\n",
    "\n",
    "reuters.categories() # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n",
    "#['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',\n",
    "#'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',\n",
    "#'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/15618',\n",
       " 'test/15649',\n",
       " 'test/15676',\n",
       " 'test/15728',\n",
       " 'test/15871',\n",
       " 'test/15875',\n",
       " 'test/15952',\n",
       " 'test/17767',\n",
       " 'test/17769',\n",
       " 'test/18024',\n",
       " 'test/18263',\n",
       " 'test/18908',\n",
       " 'test/19275',\n",
       " 'test/19668',\n",
       " 'training/10175',\n",
       " 'training/1067',\n",
       " 'training/11208',\n",
       " 'training/11316',\n",
       " 'training/11885',\n",
       " 'training/12428',\n",
       " 'training/13099',\n",
       " 'training/13744',\n",
       " 'training/13795',\n",
       " 'training/13852',\n",
       " 'training/13856',\n",
       " 'training/1652',\n",
       " 'training/1970',\n",
       " 'training/2044',\n",
       " 'training/2171',\n",
       " 'training/2172',\n",
       " 'training/2191',\n",
       " 'training/2217',\n",
       " 'training/2232',\n",
       " 'training/3132',\n",
       " 'training/3324',\n",
       " 'training/395',\n",
       " 'training/4280',\n",
       " 'training/4296',\n",
       " 'training/5',\n",
       " 'training/501',\n",
       " 'training/5467',\n",
       " 'training/5610',\n",
       " 'training/5640',\n",
       " 'training/6626',\n",
       " 'training/7205',\n",
       " 'training/7579',\n",
       " 'training/8213',\n",
       " 'training/8257',\n",
       " 'training/8759',\n",
       " 'training/9865',\n",
       " 'training/9958']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This method has an optional argument that specifies a document or a list of documents, \n",
    "#allowing us to map from (one or more) documents to (one or more) categories:\n",
    "\n",
    "brown.categories('ca01')\n",
    "#['news']\n",
    "brown.categories(['ca01','cb01'])\n",
    "#['editorial', 'news']\n",
    "reuters.categories('training/9865')\n",
    "#['barley', 'corn', 'grain', 'wheat']\n",
    "reuters.categories(['training/9865', 'training/9880'])\n",
    "#['barley', 'corn', 'grain', 'money-fx', 'wheat']\n",
    "#We can go back the other way using the optional argument of the fileids() method:\n",
    "\n",
    "reuters.fileids('barley') # doctest: +ELLIPSIS\n",
    "#['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In addition to mapping between categories and documents, these corpora permit direct access to their contents via the categories. Instead of accessing a subset of a corpus by specifying one or more fileids, we can identify one or more categories, e.g.:\n",
    "\n",
    "brown.tagged_words(categories='news')\n",
    "#Note that it is an error to specify both documents and categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In the context of a text categorization system, we can easily test if the category assigned to a document is correct as follows:\n",
    "\n",
    "def classify(doc): return 'news'   # Trivial classifier\n",
    "doc = 'ca01'\n",
    "classify(doc) in brown.categories(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package comparative_sentences to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\comparative_sentences.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['its',\n",
       " 'fast-forward',\n",
       " 'and',\n",
       " 'rewind',\n",
       " 'work',\n",
       " 'much',\n",
       " 'more',\n",
       " 'smoothly',\n",
       " 'and',\n",
       " 'consistently',\n",
       " 'than',\n",
       " 'those',\n",
       " 'of',\n",
       " 'other',\n",
       " 'models',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'had',\n",
       " '.']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A list of sentences from various sources, especially reviews and articles. Each line contains one sentence; sentences were separated by using a sentence tokenizer. Comparative sentences have been annotated with their type, entities, features and keywords.\n",
    "nltk.download(\"comparative_sentences\")\n",
    "from nltk.corpus import comparative_sentences\n",
    "comparison = comparative_sentences.comparisons()[0]\n",
    "comparison.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\opinion_lexicon.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"opinion_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'aborts',\n",
       " 'abrade',\n",
       " 'abrasive',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abscond',\n",
       " 'absence',\n",
       " 'absent-minded',\n",
       " 'absentee',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'absurdness',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'abusive',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'accidental',\n",
       " 'accost',\n",
       " 'accursed',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accusingly',\n",
       " 'acerbate']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "opinion_lexicon.words()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'aborts',\n",
       " 'abrade',\n",
       " 'abrasive',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abscond',\n",
       " 'absence',\n",
       " 'absent-minded',\n",
       " 'absentee',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'absurdness',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'abusive',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'accidental',\n",
       " 'accost',\n",
       " 'accursed',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accusingly',\n",
       " 'acerbate']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_lexicon.negative()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a+',\n",
       " 'abound',\n",
       " 'abounds',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'accessable',\n",
       " 'accessible',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'acclamation',\n",
       " 'accolade',\n",
       " 'accolades',\n",
       " 'accommodative',\n",
       " 'accomodative',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'achievable',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achievible',\n",
       " 'acumen',\n",
       " 'adaptable',\n",
       " 'adaptive',\n",
       " 'adequate',\n",
       " 'adjustable',\n",
       " 'admirable',\n",
       " 'admirably',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admirer',\n",
       " 'admiring',\n",
       " 'admiringly',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adored',\n",
       " 'adorer']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_lexicon.positive()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'a+',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(opinion_lexicon.words())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ppattach to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\ppattach.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PPAttachment(sent='0', verb='join', noun1='board', prep='as', noun2='director', attachment='V'), PPAttachment(sent='1', verb='is', noun1='chairman', prep='of', noun2='N.V.', attachment='N'), ...]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The Prepositional Phrase Attachment corpus is a corpus of prepositional phrase attachment decisions. \n",
    "#Each instance in the corpus is encoded as a PPAttachment object:\n",
    "nltk.download(\"ppattach\")\n",
    "from nltk.corpus import ppattach\n",
    "ppattach.attachments('training') # doctest: +NORMALIZE_WHITESPAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package product_reviews_1 to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package product_reviews_1 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('canon powershot g3', '+3'),\n",
       " ('use', '+2'),\n",
       " ('picture', '+2'),\n",
       " ('picture quality', '+1'),\n",
       " ('picture quality', '+1'),\n",
       " ('camera', '+2'),\n",
       " ('use', '+2'),\n",
       " ('feature', '+1'),\n",
       " ('picture quality', '+3'),\n",
       " ('use', '+1'),\n",
       " ('option', '+1')]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#product_reviews_1 and product_reviews_2\n",
    "#These two datasets respectively contain annotated customer reviews of 5 and 9 products from amazon.com.\n",
    "nltk.download(\"product_reviews_1\")\n",
    "\n",
    "from nltk.corpus import product_reviews_1\n",
    "camera_reviews = product_reviews_1.reviews('Canon_G3.txt')\n",
    "review = camera_reviews[0]\n",
    "\n",
    "review.sents()[0]\n",
    "\n",
    "review.features()\n",
    "\n",
    "#It is also possible to reach the same information directly from the stream:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('canon powershot g3', '+3'), ('use', '+2'), ...]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_reviews_1.features('Canon_G3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 24 1.6\n"
     ]
    }
   ],
   "source": [
    "n_reviews = len([(feat,score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])\n",
    "tot = sum([int(score) for (feat,score) in product_reviews_1.features('Canon_G3.txt') if feat=='picture'])\n",
    "\n",
    "mean = float(tot)/n_reviews\n",
    "print(n_reviews, tot, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package pros_cons to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\pros_cons.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['East', 'batteries', '!', 'On', '-', 'off', 'switch', 'too', 'easy', 'to', 'maneuver', '.'], ['Eats', '...', 'no', ',', 'GULPS', 'batteries'], ...]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A list of pros/cons sentences for determining context (aspect) dependent sentiment words, which are then applied to sentiment analysis of comparative sentences.\n",
    "nltk.download(\"pros_cons\")\n",
    "from nltk.corpus import pros_cons\n",
    "pros_cons.sents(categories='Cons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Easy', 'to', 'use', ',', 'economical', '!', ...]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pros_cons.words('IntegratedPros.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When', 'several', 'minutes', 'had', 'passed', 'and', ...]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The Brown Corpus, annotated with WordNet senses.\n",
    "\n",
    "from nltk.corpus import semcor\n",
    "semcor.words('brown2/tagfiles/br-n12.xml')  # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When cmd=ignore pos=WRB \n",
      "several cmd=done lemma=several lexsn=5:00:00:some(a):00 pos=JJ wnsn=1 \n",
      "minutes cmd=done lemma=minute lexsn=1:28:00:: pos=NN wnsn=1 \n",
      "had cmd=done ot=notag pos=VBD \n",
      "passed cmd=done lemma=pass lexsn=2:38:03:: pos=VB wnsn=4 \n",
      "and cmd=ignore pos=CC \n",
      "Curt cmd=done lemma=person lexsn=1:03:00:: pn=person pos=NNP rdf=person wnsn=1 \n",
      "had cmd=done ot=notag pos=VBD \n",
      "n't cmd=done lemma=n't lexsn=4:02:00:: pos=RB wnsn=0 \n",
      "emerged cmd=done lemma=emerge lexsn=2:30:00:: pos=VB wnsn=1 \n",
      "from cmd=ignore pos=IN \n",
      "the cmd=ignore pos=DT \n",
      "livery_stable cmd=done lemma=livery_stable lexsn=1:06:00:: pos=NN wnsn=1 \n",
      ", \n",
      "Brenner cmd=done lemma=person lexsn=1:03:00:: pn=person pos=NNP rdf=person wnsn=1 \n",
      "re-entered cmd=done lemma=re-enter lexsn=2:38:00:: pos=VB wnsn=1 \n",
      "the cmd=ignore pos=DT \n",
      "hotel cmd=done lemma=hotel lexsn=1:06:00:: pos=NN wnsn=1 \n",
      "and cmd=ignore pos=CC \n",
      "faced cmd=done lemma=face lexsn=2:42:02:: pos=VB wnsn=4 \n",
      "Summers cmd=done lemma=person lexsn=1:03:00:: pn=person pos=NNP rdf=person wnsn=1 \n",
      "across cmd=ignore pos=IN \n",
      "the cmd=ignore pos=DT \n",
      "counter cmd=done lemma=counter lexsn=1:06:00:: pos=NN wnsn=1 \n",
      ". \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sent = semcor.xml('brown2/tagfiles/br-n12.xml').findall('context/p/s')[0]\n",
    "for wordform in sent.getchildren():\n",
    "    print(wordform.text, end=' ')\n",
    "    for key in sorted(wordform.keys()):\n",
    "        print(key + '=' + wordform.get(key), end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package senseval to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\senseval.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#senseval\n",
    "#The Senseval 2 corpus is a word sense disambiguation corpus. Each item in the corpus corresponds to a single ambiguous word. For each of these words, the corpus contains a list of instances, corresponding to occurrences of that word. Each instance provides the word; a list of word senses that apply to the word occurrence; and the word's context.\n",
    "nltk.download(\"senseval\")\n",
    "from nltk.corpus import senseval\n",
    "senseval.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         declines in |  interest | rates .         -> interest_6\n",
      "  indicate declining |  interest | rates because   -> interest_6\n",
      "       in short-term |  interest | rates .         -> interest_6\n",
      "                 4 % |  interest | in this         -> interest_5\n",
      "        company with | interests | in the          -> interest_5\n",
      "              , plus |  interest | .               -> interest_6\n",
      "             set the |  interest | rate on         -> interest_6\n",
      "              's own |  interest | , prompted      -> interest_4\n",
      "       principal and |  interest | is the          -> interest_6\n",
      "        increase its |  interest | to 70           -> interest_5\n"
     ]
    }
   ],
   "source": [
    "for inst in senseval.instances('interest.pos')[:10]:\n",
    "     p = inst.position\n",
    "     left = ' '.join(w for (w,t) in inst.context[p-2:p])\n",
    "     word = ' '.join(w for (w,t) in inst.context[p:p+1])\n",
    "     right = ' '.join(w for (w,t) in inst.context[p+1:p+3])\n",
    "     senses = ' '.join(inst.senses)\n",
    "     print('%20s |%10s | %-15s -> %s' % (left, word, right, senses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
