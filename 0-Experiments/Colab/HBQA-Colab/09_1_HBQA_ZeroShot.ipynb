{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34518,"status":"ok","timestamp":1697685070422,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"},"user_tz":-330},"id":"_bwe_CP7MZhJ","outputId":"f7031d71-8798-4dc2-845d-a8877ad18371"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"xhMxInMCGaYC"},"source":["# Zeroshot\n","- T5\n","- Flan-T5\n","- GPT2\n","- DistilBERT\n","- RoBERTa\n","- Llama2 (with PEFT & LoRA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bvglx-JnK6mf"},"outputs":[],"source":["predmodels_list = {\"0\" :\"T5:t5\", \"1\" : \"Flan-T5:flant5\", \"2\":\"GPT2:gpt2\",\"3\":\"DistilBERT:distilbert\",\n","                   \"4\":\"RoBERTa:roberta\",\"5\":\"Llama2:llama2\", \"6\":\"BERTSquad:bert\", \"7\":\"LongFormer:longformer\"}\n","\n","predict_now=True\n","embed_now=True\n","compute_metrics_now=True"]},{"cell_type":"markdown","metadata":{"id":"kxePEJraGPSv"},"source":["# Example Questions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_A-z0xwGSUQ"},"outputs":[],"source":["q1 = \"What predicament does Yudhishthira face, and how does he seek guidance to resolve it?\"\n","\n","ra1 = \"Yudhishthira faces the predicament of being unable to support the Brahmanas who are following him as he departs for the forest. \\\n","He seeks guidance to resolve this dilemma by approaching his priest, Dhaumya, and inquiring about the appropriate course of action.\"\n","\n","ra1 = ra1.replace('\\n','')\n","\n","c1 = \"\"\"Section III\n","\"Vaisampayana said, 'Yudhishthira the son of Kunti, thus addressed by Saunaka, approached his priest and in the midst of his brothers said,\n","'The Brahmanas versed in the Vedas are following me who am departing for the forest. Afflicted with many calamities I am unable to support them.\n","I cannot abandon them, nor have I the power to offer them sustenance: Tell me, O holy one, what should be done by me in such a pass.'\n","\"Vaisampayana said, 'After reflecting for a moment seeking to find out the (proper) course by his yoga powers, Dhaumya, that foremost of all virtuous men,\n","addressed Yudhishthira, in these words, 'In days of old, all living beings that had been created were sorely afflicted with hunger.\n","And like a father (unto all of them), Savita (the sun) took compassion upon them. And going first into the northern declension,\n","the sun drew up water by his rays, and coming back to the southern declension, stayed over the earth, with his heat centered in himself.\n","And while the sun so stayed over the earth, the lord of the vegetable world (the moon), converting the effects of the solar heat (vapours) into clouds\n","and pouring them down in the shape of water, caused plants to spring up. Thus it is the sun himself, who, drenched by the lunar influence, is transformed,\n","upon the sprouting of seeds, into holy vegetable furnished with the six tastes. And it is these which constitute the food of all creatures upon the earth.\n","Thus the food that supporteth the lives of creatures is instinct with solar energy, and the sun is, therefore, the father of all creatures.\n","Do thou, hence, O Yudhishthira, take refuge even in him. All illustrious monarchs of pure descent and deeds are known to have delivered their people by practising high asceticism.\n","The great Karttavirya, and Vainya and Nahusha, had all, by virtue of ascetic meditation preceded by vows, delivered their people from heavy afflictions.\n","Therefore, O virtuous one, as thou art purified by the acts do thou likewise, entering upon a file of austerities. O Bharata, virtuously support the regenerate ones.'\n","\"Janamejaya said, 'How did that bull among the Kurus, king Yudhishthira, for the sake of the Brahmanas adore the sun of wonderful appearance?\"\n","\"Vaisampayana said, 'Listen attentively, O king, purifying thyself and withdrawing thy mind from every other thing. And, O king of kings, appoint thou a time.\n","I will tell thee everything in detail, And, O illustrious one, listen to the one hundred and eight names (of the sun)\n","as they were disclosed of old by Dhaumya to the high-souled son of Pritha. Dhaumya said, 'Surya, Aryaman, Bhaga, Twastri, Pusha, Arka, Savitri.\n","Ravi, Gabhastimat, Aja, Kala, Mrityu, Dhatri, Prabhakara, Prithibi, Apa, Teja, Kha, Vayu, the sole stay, Soma, Vrihaspati, Sukra, Budha, Angaraka,\n","Indra, Vivaswat, Diptanshu, Suchi, Sauri, Sanaichara, Brahma, Vishnu, Rudra, Skanda, Vaisravana, Yama, Vaidyutagni, Jatharagni, Aindhna, Tejasampati,\n","Dharmadhwaja, Veda-karttri, Vedanga, Vedavahana, Krita, Treta, Dwapara, Kali, full of every impurity, Kala, Kastha, Muhurtta, Kshapa, Yama, and Kshana;\n","Samvatsara-kara, Aswattha, Kalachakra, Bibhavasu, Purusha, Saswata, Yogin, Vyaktavyakta, Sanatana, Kaladhyaksha, Prajadhyaksha, Viswakarma, Tamounda,\n","Varuna, Sagara, Ansu, Jimuta, Jivana, Arihan, Bhutasraya, Bhutapati, Srastri, Samvartaka, Vanhi, Sarvadi, Alolupa, Ananta, Kapila, Bhanu, Kamada,\n","Sarvatomukha, Jaya, Visala, Varada, Manas, Suparna, Bhutadi, Sighraga, Prandharana, Dhanwantari, Dhumaketu, Adideva, Aditisuta, Dwadasatman, Aravindaksha,\n","Pitri, Matri, Pitamaha, Swarga-dwara, Prajadwara, Mokshadwara, Tripistapa, Dehakarti, Prasantatman, Viswatman, Viswatomukha, Characharatman, Sukhsmatman,\n","the merciful Maitreya. These are the hundred and eight names of Surya of immeasurable energy, as told by the self-create (Brahma). For the acquisition of prosperity,\n","I bow down to thee, O Bhaskara, blazing like unto gold or fire, who is worshipped of the gods and the Pitris and the Yakshas, and who is adored by Asuras, Nisacharas,\n","and Siddhas. He that with fixed attention reciteth this hymn at sunrise, obtaineth wife and offspring and riches and the memory of his former existence,\n","and by reciting this hymn a person attaineth patience and memory. Let a man concentrating his mind, recite this hymn. By doing so,\n","he shall be proof against grief and forest-fire and ocean and every object of desire shall be his.'\n","\"Vaisampayana continued, 'Having heard from Dhaumya these words suitable to the occasion, Yudhishthira the just, with heart concentrated within itself and purifying it duly,\n","became engaged in austere meditation, moved by the desire of supporting the Brahmanas. And worshipping the maker of day with offerings of flowers and other articles,\n","the king performed his ablutions. And standing in the stream, he turned his face towards the god of day. And touching the water of the Ganges\n","the virtuous Yudhishthira with senses under complete control and depending upon air alone for his sustenance, stood there with rapt soul engaged in pranayama.\n","And having purified himself and restrained his speech, he began to sing the hymn of praise (to the sun).'\n","'Yudhishthira said, \"Thou art, O sun, the eye of the universe. Thou art the soul of all corporeal existences. Thou art the origin of all things.\n","Thou art the embodiment of the acts of all religious men. Thou art the refuge of those versed in the Sankhya philosophy (the mysteries of the 1.\"\"\"\n","\n","c1 = c1.replace('\\n','')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_1in5rC6GdwA"},"outputs":[],"source":["q2 = \"What is the significance of performing the Agnihotra, and what consequences are mentioned for neglecting it?\"\n","\n","ra2 = \"\"\"Performing the Agnihotra is considered important for eternal morality. Neglecting it, as mentioned in the text, leads to the consumption of sin.\n","Those who do not perform the Agnihotra are said to suffer the consequences, including not waiting upon bulls and neglecting their kinsmen, guests, friends, sons, wives, and servants.\"\"\"\n","\n","ra2 = ra2.replace('\\n','')\n","\n","c2 = \"\"\"Even this is eternal morality. They that perform not the Agnihotra  not wait upon bulls, nor cherish their kinsmen and guests and friends and\n","sons and wives and servants, are consumed with sin for such neglect. None should cook his food for himself alone and none should slay an animal without dedicating it to the gods,\n","the pitris, and guests. Nor should one eat of that food which hath not been duly dedicated to the gods and pitris. By scattering food on the earth, morning and evening,\n","for (the behoof of) dogs and Chandalas and birds, should a person perform the Viswedeva sacrifice.  He that eateth the Vighasa, is regarded as eating ambrosia.\n","What remaineth in a sacrifice after dedication to the gods and the pitris is regarded as ambrosia; and what remaineth after feeding the guest is called Vighasa\n","and is equivalent to ambrosia itself. Feeding a guest is equivalent to a sacrifice, and the pleasant looks the host casteth upon the guest, the attention he devoteth to him,\n","the sweet words in which he addresseth him, the respect he payeth by following him, and the food and drink with which he treateth him, are the five Dakshinas\n","in that sacrifice. He who giveth without stint food to a fatigued wayfarer never seen before, obtaineth merit that is great, and he who leading a domestic life,\n","followeth such practices, acquireth religious merit that is said to be very great. O Brahmana, what is thy opinion on this?\"\n","\"Saunaka said, 'Alas, this world is full of contradictions! That which shameth the good, gratifieth the wicked! Alas, moved by ignorance and passion\n","and slaves of their own senses, even fools perform many acts of (apparent merit) to gratify in after-life their appetites! With eyes open are these\n","men led astray by their seducing senses, even as a charioteer, who hath lost his senses, by restive and wicked steeds! When any of the six senses\n","findeth its particular object, the desire springeth up in the heart to enjoy that particular object. And thus when one's heart proceedeth to enjoy the\n","objects of any particular sense a wish is entertained which in its turn giveth birth to a resolve. And finally, like unto an insect falling into a flame from\n","love of light, the man falleth into the fire of temptation, pierced by the shafts of the object of enjoyment discharged by the desire constituting the seed of the resolve!\n","And thenceforth blinded by sensual pleasure which he seeketh without stint, and steeped in dark ignorance and folly which he mistaketh for a state of happiness, he\n","knoweth not himself! And like unto a wheel that is incessantly rolling, every creature, from ignorance and deed and desire, falleth into various states in this world,\n","wandering from one birth to another, and rangeth the entire circle of existences from a Brahma to the point of a blade of grass, now in water, now on land, and now against in the air!\n","'This then is the career of those that are without knowledge. Listen now to the course of the wise they that are intent on profitable virtue,\n","and are desirous of emancipation! The Vedas enjoin act but renounce (interest in) action. Therefore, shouldst thou act, renouncing Abhimana,  performance of sacrifices, study (of the Vedas),\n","gifts, penance, truth (in both speech and act), forgiveness, subduing the senses, and renunciation of desire,--these have been declared to be the eight (cardinal) duties constituting the true path.\n","Of these, the four first pave the way to the world of the pitris. And these should be practised without Abhimana. The four last are always observed by the pious, to attain the heaven of the gods.\n","And the pure in spirit should ever follow these eight paths. Those who wish to subdue the world for purpose of salvation, should ever act fully renouncing motives, effectually subduing their senses,\n","rigidly observing particular vows, devotedly serving their preceptors, austerely regulating their fare, diligently studying the Vedas, renouncing action as mean and restraining their hearts.\n","By renouncing desire and aversion the gods have attained prosperity. It is by virtue of their wealth of yoga  that the Rudras, and the Sadhyas, and the Adityas and the Vasus, and the twin Aswins,\n","rule the creatures. Therefore, O son of Kunti, like unto them, do thou, O Bharata, entirely refraining from action with motive, strive to attain success in yoga and by ascetic austerities.\n","Thou hast already achieved such success so far as thy debts to thy ancestors, both male and female concerned, and that success also which is derived from action (sacrifices). Do thou,\n","for serving the regenerate ones endeavour to attain success in penances. Those that are crowned with ascetic success, can, by virtue of that success, do whatever they list; do thou, therefore,\n","practising asceticism realise all thy wishes.\"\"\"\n","\n","c2 = c2.replace('\\n','')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"6RPNSfNWjH2b","outputId":"763f7fa9-df39-4793-c212-b85349a5bccd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement huggingface-cli (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for huggingface-cli\u001b[0m\u001b[31m\n","\u001b[0m\n","    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n","    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n","    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n","    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n","    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n","    \n","    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n","    Setting a new token will erase the existing one.\n","    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","Token: "]}],"source":["# @title huggingface login\n","!pip install huggingface-cli\n","\n","!huggingface-cli login"]},{"cell_type":"markdown","metadata":{"id":"V4yo_L89jF_k"},"source":[]},{"cell_type":"markdown","metadata":{"id":"2jZzbP7b8iw1"},"source":["# Install Necessary Libararies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VacrsXBLqEc"},"outputs":[],"source":["!pip install -Uq transformers\n","!pip install -Uq evaluate\n","!pip install -Uq SentencePiece\n","!pip install -Uq sentence-transformers\n","!pip install rouge-score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbDoP7u6NSeJ"},"outputs":[],"source":["# !pip install accelerate>=0.20.1\n","# !pip install transformers[torch]\n","# # You need to restart the kernel after this step"]},{"cell_type":"markdown","metadata":{"id":"sCruv7vhoP2w"},"source":["# Load Libraries and Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YtLfF1XwKOIh"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import json\n","import ast\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.utils.data import Dataset, DataLoader, RandomSampler\n","from torch.nn.functional import cosine_similarity\n","import torch.nn.functional as F\n","\n","from sentence_transformers import SentenceTransformer, util\n","\n","import tensorflow as tf\n","\n","# import spacy\n","# import string\n","\n","from sklearn.model_selection import train_test_split\n","\n","import transformers\n","import evaluate  # Bleu\n","\n","\n","import nltk\n","nltk.download('punkt')\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s63VS02xKOIh"},"outputs":[],"source":["# @title Load Configuraiton\n","import hbqaconfig\n","conf = hbqaconfig.setEnv('colab')\n","# for k,v in conf.items(): print (k,\":\",v)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFfYxOWUM50w"},"outputs":[],"source":["import torch\n","\n","# # Detect and initialize TPU\n","# tpu_available = tf.config.experimental.list_logical_devices(\"TPU\")\n","# if tpu_available:\n","#     print(\"TPU available\")\n","# else:\n","#     print(\"No TPU available\")\n","\n","if torch.cuda.is_available():\n","    DEVICE = torch.device(\"cuda\")\n","else:\n","    DEVICE = torch.device(\"cpu\")\n","\n","print(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"lpBtIrI9M7hv"},"source":["# Load QA Dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"bsJcYRO2ZdMH","executionInfo":{"status":"ok","timestamp":1697685521899,"user_tz":-330,"elapsed":1243,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"outputs":[],"source":["# df = pd.read_csv(datapath + '06-HBQA_Manual_with_Chunk.csv') # for local machine/vedavit colab\n","\n","# Uncomment below code\n","# !pip install -q gdown\n","# import gdown\n","\n","# # Replace the shared link with the actual link to your file\n","# file_url = 'https://drive.google.com/uc?id=1Euvnmp8yJ2LGlL2uDvDjER87PYz9RVvS'\n","# output_path = '/content/hbqa-colab.txt'  # Specify the desired file name and path\n","\n","# gdown.download(file_url, output_path, quiet=False)\n","import pandas as pd\n","df = pd.read_csv(conf['QAGS_FOLDER'] + '06-HBQA_Manual_with_Chunk.csv')\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"1QCj3v0vKOIi","colab":{"base_uri":"https://localhost:8080/","height":165},"executionInfo":{"status":"ok","timestamp":1697685526677,"user_tz":-330,"elapsed":775,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}},"outputId":"8a02dce9-393c-4db8-b54b-c0a03ed457e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["(1102, 11)\n"]},{"output_type":"execute_result","data":{"text/plain":["   Ques_Id  Chunk_Id  Section_Id  \\\n","0        0       389  Book03_002   \n","1        1       390  Book03_003   \n","\n","                                            Question  \\\n","0  What is the significance of performing the Agn...   \n","1  What predicament does Yudhishthira face, and h...   \n","\n","                                          Ref_Answer  \\\n","0  Performing the Agnihotra is considered importa...   \n","1  Yudhishthira faces the predicament of being un...   \n","\n","                                               Chunk  \\\n","0  Even this is eternal morality. They that perfo...   \n","1  Section III\\n\"Vaisampayana said, 'Yudhishthira...   \n","\n","                                           Reference  WordsInQues  WordsInAns  \\\n","0  The significance of the Agnihotra and the cons...           16          50   \n","1  Yudhishthira's predicament and his consultatio...           14          41   \n","\n","   WordsInRef  WordsInChunk  \n","0          50           809  \n","1          53           852  "],"text/html":["\n","  <div id=\"df-79aeefcf-eabd-4282-99b4-ed511d383546\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Ques_Id</th>\n","      <th>Chunk_Id</th>\n","      <th>Section_Id</th>\n","      <th>Question</th>\n","      <th>Ref_Answer</th>\n","      <th>Chunk</th>\n","      <th>Reference</th>\n","      <th>WordsInQues</th>\n","      <th>WordsInAns</th>\n","      <th>WordsInRef</th>\n","      <th>WordsInChunk</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>389</td>\n","      <td>Book03_002</td>\n","      <td>What is the significance of performing the Agn...</td>\n","      <td>Performing the Agnihotra is considered importa...</td>\n","      <td>Even this is eternal morality. They that perfo...</td>\n","      <td>The significance of the Agnihotra and the cons...</td>\n","      <td>16</td>\n","      <td>50</td>\n","      <td>50</td>\n","      <td>809</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>390</td>\n","      <td>Book03_003</td>\n","      <td>What predicament does Yudhishthira face, and h...</td>\n","      <td>Yudhishthira faces the predicament of being un...</td>\n","      <td>Section III\\n\"Vaisampayana said, 'Yudhishthira...</td>\n","      <td>Yudhishthira's predicament and his consultatio...</td>\n","      <td>14</td>\n","      <td>41</td>\n","      <td>53</td>\n","      <td>852</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79aeefcf-eabd-4282-99b4-ed511d383546')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-79aeefcf-eabd-4282-99b4-ed511d383546 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-79aeefcf-eabd-4282-99b4-ed511d383546');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-85a858ad-04f3-4258-bf4f-dc3b33d9da8a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-85a858ad-04f3-4258-bf4f-dc3b33d9da8a')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-85a858ad-04f3-4258-bf4f-dc3b33d9da8a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":12}],"source":["print(df.shape)\n","df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BtQbY4CUzHi"},"outputs":[],"source":["df.set_index('Ques_Id',inplace=True)\n","df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nBJWpcOJZiRc"},"outputs":[],"source":["\n","# df['WordsInChunk'] = df.Chunk.str.split(' ').apply(len)\n","# df['CharInChunk'] = df.Chunk.apply(len)\n","# df.to_csv(r'H:\\My Drive\\HBQA\\Data\\06-HBQA_Manual_with_Chunk.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYZeJ16zoGDQ"},"outputs":[],"source":["# sample code\n","# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","\n","# TOKENIZER = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-ReformerForQuestionAnswering\")\n","# MODEL = AutoModelForQuestionAnswering.from_pretrained(\"hf-internal-testing/tiny-random-ReformerForQuestionAnswering\")\n","# MODEL.to(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"sVUo67UeySRL"},"source":["# Common Setting for Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zt2QQ5J7Ccvn"},"outputs":[],"source":["Question_Len = int(max(len(ques) for ques in df.Question)/4)\n","Answer_Len = int(max(len(ans) for ans in df.Ref_Answer)/4)\n","Chunk_Len = int(max(len(chunk) for chunk in df.Chunk)/4)\n","Chunk_Len, Question_Len,Answer_Len # in Tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCXnNdcA9fsP"},"outputs":[],"source":["Q_LEN =  1500 #256   # Question Length\n","T_LEN =  500 #32  # Target Length\n","BATCH_SIZE = 4\n","# DEVICE = \"cuda:0\""]},{"cell_type":"markdown","metadata":{"id":"A6itUc94KOIl"},"source":["# Load Zeroshot Prediction Model"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"a_6EBav9KOIl"},"outputs":[],"source":["# @title Function to Load Zeroshot Transformers/Models\n","def load_model(predmodel_name):\n","  if predmodel_name==\"t5\":\n","    from transformers import T5Tokenizer,  T5ForConditionalGeneration #, T5Model , T5TokenizerFast\n","    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n","    model.to(DEVICE)\n","  elif predmodel_name==\"gpt2\":\n","    from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n","    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\",  padding_side=\"left\")  #also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n","    tokenizer.pad_token = tokenizer.eos_token\n","    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").to(DEVICE)  #also try gpt2, gpt2-large and gpt2-medium, also gpt2-xl\n","    model.to(DEVICE)\n","  elif predmodel_name==\"bert\":\n","    from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","    model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n","    # model_name = \"t5-base\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n","    model.to(DEVICE)\n","  elif predmodel_name==\"longformer\":\n","    from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n","    model_name = \"valhalla/longformer-base-4096-finetuned-squadv1\"\n","    tokenizer = LongformerTokenizer.from_pretrained(model_name)\n","    model = LongformerForQuestionAnswering.from_pretrained(model_name).to(DEVICE)\n","  elif predmodel_name==\"distilbert\":\n","    # from transformers import DistilBertTokenizer, DistilBertModel\n","    # tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n","    # model = DistilBertModel.from_pretrained('distilbert-base-cased-distilled-squad')\n","    # model.to(DEVICE)\n","\n","    from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","    model_name = 'distilbert-base-cased-distilled-squad'\n","    # model_name = \"t5-base\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n","    model.to(DEVICE)\n","\n","\n","  return tokenizer, model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDpM77EMHhe2","executionInfo":{"status":"ok","timestamp":1697779917125,"user_tz":-330,"elapsed":9,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPE4iqoGGpA_"},"outputs":[],"source":["predmodels_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_elXyC-kEhuE"},"outputs":[],"source":["# predmodel_name = predmodels_list['3'].split(':')[1]\n","# print(predmodel_name)\n","# tokenizer, model = load_model(predmodel_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pyg3Qnok-gEl"},"outputs":[],"source":["# @title Select prediction model\n","predmodel_name = predmodels_list['6'].split(':')[1]\n","print(predmodel_name)\n","tokenizer, model = load_model(predmodel_name)"]},{"cell_type":"markdown","metadata":{"id":"832Ayr0HC95L"},"source":["# Function to predict Answers"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nc2Wc--I2_Qf"},"outputs":[],"source":["# @title answer_from_long_sentence_distilbert\n","def answer_from_long_sentence_distilbert(inputs,model):\n","  # Check if the input sequence length exceeds the model's maximum\n","  if len(inputs[\"input_ids\"][0]) > 512:\n","      # Split the input into segments that fit the model's maximum sequence length\n","      input_ids = inputs[\"input_ids\"][0]\n","      attention_mask = inputs[\"attention_mask\"][0]\n","\n","      # Create a tensor with zeros\n","      pad_len = 512 - len(input_ids) % 512\n","      zeros = torch.zeros(pad_len, dtype=input_ids.dtype)\n","\n","      # Append the zeros to the input_ids tensor and attention_mask\n","      input_ids = torch.cat((input_ids, zeros))\n","      attention_mask = torch.cat((attention_mask, zeros))\n","\n","      start_positions = []\n","      end_positions = []\n","\n","      while len(input_ids) >= 512:\n","          # Extract the first 512 tokens\n","          segment_input_ids = input_ids[:512]\n","          segment_attention_mask = attention_mask[:512]\n","          # print(len(segment_input_ids))\n","\n","          with torch.no_grad():\n","              segment_outputs = model(input_ids=segment_input_ids, attention_mask=segment_attention_mask)\n","\n","          start_logits = segment_outputs.start_logits\n","          end_logits = segment_outputs.end_logits\n","\n","          # Find the answer span within the segment\n","          answer_start = torch.argmax(start_logits)\n","          answer_end = torch.argmax(end_logits) + 1\n","\n","          # Add the segment positions to the overall positions\n","          start_positions.append(answer_start)\n","          end_positions.append(answer_end)\n","\n","          # Remove the processed tokens\n","          input_ids = input_ids[512:]\n","          attention_mask = attention_mask[512:]\n","\n","      # Determine the final answer based on the segment positions\n","      answer_start = min(start_positions)\n","      answer_end = max(end_positions)\n","  else:\n","      # Use the original answer extraction approach\n","      with torch.no_grad():\n","          outputs = model(**inputs)\n","      answer_start = torch.argmax(outputs.start_logits)\n","      answer_end = torch.argmax(outputs.end_logits) + 1\n","\n","\n","  # Get the answer text from the input tokens\n","  answer = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end])\n","\n","  if answer[:5]=='[CLS]':\n","    answer = \"don't know\"\n","\n","  return answer"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"acyGOIHMTRZM"},"outputs":[],"source":["# @title Answer_from_long_sentence_bert\n","\n","# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","# import torch\n","\n","# model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n","# tokenizer = AutoTokenizer.from_pretrained(model_name)\n","# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model.to(device)\n","\n","def sliding_window_tokenize(text, tokenizer, max_chunk_size=512, overlap=20):\n","    tokenized_text = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=1500, truncation=True)\n","    tokens = tokenized_text['input_ids'][0]\n","    print(len(tokens))\n","    attention_mask = tokenized_text['attention_mask']\n","    stride = max_chunk_size - overlap\n","\n","    num_chunks = int(len(tokens)/ stride)+1 #((len(tokens) - max_chunk_size) // stride) + 1\n","\n","    chunks = []\n","    for i in range(num_chunks):\n","        start = i * stride\n","        end = start + max_chunk_size\n","        chunk_tokens = tokens[start:end]\n","        chunk_attention_mask = attention_mask[start:end]\n","        chunks.append((chunk_tokens, chunk_attention_mask))\n","    print(len(chunks))\n","    return chunks\n","\n","def predict_answer1(context, question, tokenizer, model):\n","    tokenized_question = tokenizer(question, return_tensors=\"pt\", padding=\"max_length\", max_length=200, truncation=True)\n","    tokenized_question.to(device)\n","\n","\n","    context_chunks = sliding_window_tokenize(context, tokenizer,  )\n","    print( len(context_chunks))\n","    # print (len(context))\n","\n","    all_predicted_answers = []\n","\n","    for chunk_tokens, chunk_attention_mask in context_chunks:\n","        chunk_tokens = chunk_tokens.to(device)\n","        chunk_attention_mask = chunk_attention_mask.to(device)\n","\n","        # with torch.no_grad():\n","        #     outputs = model(input_ids=chunk_tokens, attention_mask=chunk_attention_mask)\n","\n","        with torch.no_grad():\n","          outputs = model(input_ids=tokenized_context[\"input_ids\"], attention_mask=tokenized_context[\"attention_mask\"])\n","\n","\n","        start_scores = outputs.start_logits\n","        end_scores = outputs.end_logits\n","\n","        answer_start = torch.argmax(start_scores)\n","        answer_end = torch.argmax(end_scores) + 1  # No need to add 50 here\n","\n","        predicted_answer = tokenizer.decode(chunk_tokens[0][answer_start:answer_end])\n","        all_predicted_answers.append(predicted_answer)\n","\n","    # Combine answers from all chunks\n","    combined_answer = \" \".join(all_predicted_answers)\n","\n","    return combined_answer\n","\n","# Example usage\n","# context = \"Your long context here...\"\n","# question = \"Your question here...\"\n","\n","predicted_answer = predict_answer1(c1, q1, tokenizer, model)\n","print(predicted_answer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4Gq3uQTA0DS"},"outputs":[],"source":["# @title Predict Answer Function\n","def predict_answer(context, question, tokenizer, model):\n","    if predmodel_name==\"t5\":\n","      inputs = tokenizer(question, context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n","\n","      input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n","      attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n","\n","      with torch.no_grad():\n","        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=100)\n","\n","      predicted_answer = tokenizer.decode(outputs.flatten(), skip_special_tokens=True)\n","\n","    elif predmodel_name==\"gpt2\":\n","      Q_LEN=1024\n","      # inputs = tokenizer(context, question, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n","      # input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n","      # attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n","      # outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=100)\n","\n","      # Tokenize the input and set pad_token_id\n","      input_text = f\"{context} [SEP] {question}\"\n","      input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=Q_LEN, truncation=True, padding=\"max_length\", pad_to_max_length=True)\n","\n","      # Set pad_token_id to eos_token_id\n","      model.config.pad_token_id = model.config.eos_token_id\n","\n","      # Generate text\n","      with torch.no_grad():\n","        outputs = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n","\n","      predicted_answer = tokenizer.decode(outputs.flatten(), skip_special_tokens=True)\n","    elif predmodel_name==\"longformer\":\n","      tokenized_input = tokenizer(question, context, return_tensors=\"pt\", padding=\"max_length\", max_length=1500, truncation=\"only_second\").to(DEVICE)\n","      with torch.no_grad():\n","          outputs = model(**tokenized_input)\n","\n","      start_scores = outputs.start_logits\n","      end_scores = outputs.end_logits\n","\n","      # Find the answer span using a threshold\n","      threshold = 0.20  # Adjust this threshold as needed\n","      answer_start = torch.argmax(start_scores) if torch.max(start_scores) > threshold else 0\n","      answer_end = torch.argmax(end_scores) + 1 if torch.max(end_scores) > threshold else len(tokenized_input[\"input_ids\"][0])\n","\n","      predicted_answer = tokenizer.decode(tokenized_input[\"input_ids\"][0][answer_start:answer_end])\n","      if predicted_answer[0:3]=='<s>' or len(predicted_answer)<3:\n","        predicted_answer=\"xxx\"\n","\n","    elif predmodel_name==\"bert\":\n","      # inputs = tokenizer(question, context, return_tensors=\"pt\", padding=True, max_length=1500, truncation=\"only_second\")\n","      # predicted_answer = answer_from_long_sentence_bert(inputs,model)\n","\n","      inputs = tokenizer(question, context, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n","      with torch.no_grad():\n","          outputs = model(**inputs)\n","\n","\n","      start_scores = outputs.start_logits\n","      end_scores = outputs.end_logits\n","\n","      answer_start = torch.argmax(start_scores)\n","      answer_end = torch.argmax(end_scores)+50\n","      predicted_answer = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end+1])\n","      if predicted_answer[0:5]=='[SEP]' or len(predicted_answer)<3:\n","        predicted_answer=\"xxx\"\n","\n","\n","\n","    elif predmodel_name==\"distilbert\":\n","      inputs = tokenizer(question, context, return_tensors=\"pt\", padding=True, max_length=1500, truncation=\"only_second\")\n","\n","      predicted_answer = answer_from_long_sentence_distilbert(inputs,model)\n","\n","    return predicted_answer"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"l8GsRLVa4BJt"},"outputs":[],"source":["# @title Example Prediction with a Selcted Model\n","\n","# from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n","# inputs = tokenizer(q2, c2, return_tensors=\"pt\", padding=True, max_length=1500, truncation=\"only_second\")\n","\n","pred_answer = predict_answer(c2, q2, tokenizer,model)\n","print(pred_answer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qjeaHmtjBgfA"},"outputs":[],"source":["# inputs = tokenizer(q2, c2, return_tensors=\"pt\", padding=True, max_length=512, truncation=True)\n","# with torch.no_grad():\n","#     outputs = model(**inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQPkaE-2DPlf"},"outputs":[],"source":["# inputs.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6ULgjX-D1ma"},"outputs":[],"source":["# type(outputs)\n","# outputs.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kWcT2z_ENMtx"},"outputs":[],"source":["# @title Predict Answer Function\n","from IPython.display import clear_output\n","clear_output()\n","\n","from IPython.display import display\n","from IPython.display import HTML\n","\n","if predict_now:\n","  df_pred = pd.DataFrame(columns=['Ques_Id','Question','Ref_Answer','Pred_Answer'])\n","\n","def pred_ans(df_pred, sample=True, verbose=False):\n","  import random\n","  if sample:\n","    qno=random.sample(set(df.index),10)\n","  else:\n","    qno = df.index\n","\n","  total_items = len(qno)\n","  j=0\n","\n","  # progress_bar = tqdm(total=total_items, position=0, leave=True)\n","\n","  progress_bar = tqdm(total=total_items, position=0, leave=True, dynamic_ncols=True)\n","  progress_html = HTML(\"\"\"<progress value=\"0\" max=\"{0}\" style=\"width: 100%\"></progress>\"\"\".format(total_items))\n","\n","\n","  for i  in qno:\n","      progress_bar.update(j)\n","      progress_html.value = \"\"\"<progress value=\"{0}\" max=\"{1}\" style=\"width: 100%\"></progress>\"\"\".format(i, total_items)\n","      progress_bar = tqdm(total=total_items, position=0, leave=True)\n","      ques_id  = df.index[i]\n","      chunk    = df.iloc[i]['Chunk']\n","      # print(chunk)\n","      # chunk = chunk.replace(\"'\",\"\").replace(\"\\\",\"\")\n","      ques     = df.iloc[i]['Question']\n","      ref_ans  = df.iloc[i]['Ref_Answer']\n","\n","      pred_ans = predict_answer(chunk, ques, tokenizer, model)\n","\n","      df_pred.loc[j] = ques_id, ques, ref_ans, pred_ans\n","      j+=1\n","      if verbose:\n","        print('Question  :', ques)\n","        print(\"Ref Answer:\", ref_ans)\n","        print(\"Pred Ans  :\", pred_ans)\n","        print('--------')\n","\n","      # print(f\"Prediting Answer {i}/{df.shape[0]}\")\n","\n","  # progress_bar.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b3tfGlcNWAzX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlVWVIb9WU6q"},"outputs":[],"source":["# @title Start Predicting Answer\n","if predict_now:\n","  pred_ans(df_pred, sample=True, verbose=True)\n","\n","if predict_now:\n","  print('Saving Predictions')\n","  # df_pred.to_csv(datapath + '09.11-' + predmodel_name +'Predicted_Ans-Zeroshot.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nCE-n97SYVda"},"outputs":[],"source":["# @title Load Predicted Asnwer file\n","df_pred = pd.read_csv(datapath + '09.11-' + predmodel_name +'Predicted_Ans-Zeroshot.csv')\n","print(df_pred.shape)\n","df_pred.head(2)\n"]},{"cell_type":"markdown","metadata":{"id":"3YR8gxGwKOIn"},"source":["# Load Embedding Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hJs8L6DKOIn"},"outputs":[],"source":["#Select Model Function\n","\n","# https://www.sbert.net/docs/pretrained_models.html\n","\n","#250MB, multi-qa-distilbert-cos-v1',  Max Sequence Length:\t512, Dimensions:768, Normalized Embeddings:\ttrue\n","#80MB, all-MiniLM-L6-v2, Max Sequence Length:\t256, Dimensions:\t384, Normalized Embeddings:\ttrue\n","#290MB, all-distilroberta-v1, Max Sequence Length:\t512, Dimensions:\t768, Normalized Embeddings:\ttrue\n","#420MB, all-mpnet-base-v2, Max Sequence Length:\t384, Dimensions:\t768, Normalized Embeddings:\ttrue\n","#1.36GB, all-roberta-large-v1, Max Sequence Length:\t256, Dimensions: 1024, Normalized Embeddings:\ttrue\n","\n","def select_embmodel(num):\n","    emb_modelshortlist = ['distilbert','minilm','distilroberta','mpnet','roberta']\n","\n","    emb_modellist = ['multi-qa-distilbert-cos-v1',\n","                'all-MiniLM-L6-v2',\n","                'all-distilroberta-v1',\n","                'multi-qa-mpnet-base-dot-v1',\n","                'all-roberta-large-v1']\n","\n","    embmodelname = emb_modellist[num]\n","    embmodelshort = emb_modelshortlist[num]\n","    embmodelname1 = \"_\" + embmodelname\n","\n","    print (embmodelname,'\\t',embmodelshort,'\\t', embmodelname1)\n","    return embmodelname, embmodelshort, embmodelname1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6n5NSwK0PBfv"},"outputs":[],"source":["embmodelname, embmodelshort, embmodelname1 = select_embmodel(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGzzleoxPWsk"},"outputs":[],"source":["if embed_now:\n","  embmodel = SentenceTransformer(embmodelname)"]},{"cell_type":"markdown","metadata":{"id":"hflQRBBNYOhb"},"source":["# Vectorize Answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vI0565KwYMTr"},"outputs":[],"source":["df_pred =  pd.read_csv(datapath + '09.11-' + predmodel_name +'Predicted_Ans-Zeroshot.csv')\n","\n","df_pred.set_index('Ques_Id',inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"my04KErIUhP8"},"outputs":[],"source":["df_pred.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKV1ytidgDCb"},"outputs":[],"source":["def create_ans_vector():\n","\n","    Ans_Sentences = df_pred.Pred_Answer.tolist()\n","    Ans_Embeddings = embmodel.encode(Ans_Sentences)\n","\n","    print (\"Answer Embedding: \", Ans_Embeddings.shape)\n","    # return (Ques_Embeddings, Ans_Embeddings)\n","\n","    return (Ans_Embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h41dWQUBZDKe"},"outputs":[],"source":["df_pred['Pred_Answer'] = df_pred.Pred_Answer.fillna(\"don't know\")\n","df_pred.loc[df_pred['Pred_Answer'].str.contains(r'\\?\\?\\?'), 'Pred_Answer'] = \"don't know\"\n","\n","# Model couldn't fine any any answer for this\n","df_pred.loc[df_pred['Pred_Answer'].str.contains(r\"don't know\")].shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWy7UqdigGLY"},"outputs":[],"source":["if embed_now:\n","  Ans_Embeddings = create_ans_vector()\n","  df_pred['Pred_Answer_Vector'] = torch.tensor(Ans_Embeddings, dtype=torch.float).tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WtNurcbEctC"},"outputs":[],"source":["if embed_now:\n","  df_pred.to_csv(datapath + '09.11-' + predmodel_name +'Predicted_AnsVec-Zeroshot.csv')\n","\n","# df_pred = pd.read_csv(datapath + '09.11-' + predmodel_name +'Predicted_AnsVec-Zeroshot.csv')\n","print(df_pred.shape)\n","df_pred.head(2)"]},{"cell_type":"markdown","metadata":{"id":"3DFkqupdP78P"},"source":["# Calculate Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwxedCIOKOIo"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from sklearn.metrics import precision_score, recall_score\n","from nltk.tokenize import word_tokenize\n","from rouge_score import rouge_scorer\n","import numpy as np\n","\n","smoother = SmoothingFunction()\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n","\n","def get_nlp_metrics(ques_id, ref_ans, pred_ans):\n","\n","  # Calculate ROUGE-1 and ROUGE-L scores\n","  scores = scorer.score(ref_ans, pred_ans)\n","\n","  # Access individual ROUGE scores\n","  rouge_1_precision = scores['rouge1'].precision\n","  rouge_1_recall = scores['rouge1'].recall\n","  rouge_1_f1 = scores['rouge1'].fmeasure\n","\n","  rouge_l_precision = scores['rougeL'].precision\n","  rouge_l_recall = scores['rougeL'].recall\n","  rouge_l_f1 = scores['rougeL'].fmeasure\n","\n","  rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1,\n","\n","  ref_tokens = word_tokenize(ref_ans)\n","  pred_tokens = word_tokenize(pred_ans)\n","\n","  # Calculate BLEU score for a single sentence\n","  bleu_score = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoother.method2)\n","\n","  # Calculate BLEU score for multiple sentences\n","  # corpus_bleu_score = corpus_bleu([[ref_tokens]], [pred_tokens],  smoothing_function=smoother.method2)\n","\n","  # print(set(ref_tokens).intersection(set(pred_tokens)))\n","  tp = len(set(ref_tokens).intersection(set(pred_tokens)))\n","  precision = tp / len(pred_tokens)\n","  recall =  tp / len(ref_tokens)\n","\n","  return ques_id, bleu_score, rouge_1_precision, rouge_1_recall, rouge_1_f1, rouge_l_precision, rouge_l_recall, rouge_l_f1, precision, recall\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhEK-0Acx_PA"},"outputs":[],"source":["# from datasets import load_metric\n","if compute_metrics_now:\n","  df_metrics = pd.DataFrame(columns = ['Ques_Id','BLEU1', 'ROUGE1_P', 'ROUGE1_R', 'ROUGE1_F1',\n","                              'ROUGEL_P', 'ROUGEL_R', 'ROUGE1_F1', 'Precision', 'Recall'])\n","  N= df_pred.shape[0]\n","\n","  for i in range(N):\n","    # bleu_score1 = calculate_score( df1.iloc[i]['ref_answer'],df1.iloc[i]['pred_answer'])\n","    ques_id = df_pred.index[i]\n","    ref_ans  = df_pred.loc[df_pred.index==ques_id,'Ref_Answer'].values[0]\n","    pred_ans = df_pred.loc[df_pred.index==ques_id,'Pred_Answer'].values[0]\n","    results = get_nlp_metrics(ques_id, ref_ans, pred_ans)\n","    df_metrics.loc[i] = results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lr5975TJx3s"},"outputs":[],"source":["metricsfile = '09.12-' + predmodel_name +'Predicted_Ans_Score_Zeroshot.csv'\n","print(metricsfile)\n","\n","if compute_metrics_now:\n","  df_metrics.to_csv(datapath + metricsfile, index=False)\n","\n","# df_metrics = pd.read_csv(datapath + metricsfile)\n","print(df_metrics.shape)\n","df_metrics.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XnnnOCeInQSy"},"outputs":[],"source":["df_metrics = pd.read_csv(datapath + '09.12-' + predmodel_name +'Predicted_Ans_Score.csv')\n","print(df_metrics.shape)\n","df_metrics.head(3)"]},{"cell_type":"markdown","metadata":{"id":"lYkxrY-LF4HN"},"source":["# Calculate Cosine between Predicted and Reference Answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWGLBQNnwndJ"},"outputs":[],"source":["df_ref_ans = pd.read_csv(datapath + '07.2-HBQA_QA_Vector' + embmodelname1 + '.csv')\n","\n","df_ref_pred= df_pred.merge(df_ref_ans, on = \"Ques_Id\", how=\"inner\")[['Ques_Id','Pred_Answer_Vector','AnsVector_multi-qa-mpnet-base-dot-v1']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3g9g_PKE74G"},"outputs":[],"source":["df_ref_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1RZRZePCdwG"},"outputs":[],"source":["def calculate_cosine(row):\n","\n","    if type(row['Pred_Answer_Vector'])!=type(list()):\n","      predAns_vector_values = ast.literal_eval(row['Pred_Answer_Vector'])\n","    else:\n","      predAns_vector_values = row['Pred_Answer_Vector']\n","\n","    if type(row['AnsVector' + embmodelname1])!=type(list()):\n","      refAns_vector_values = ast.literal_eval(row['AnsVector' + embmodelname1])\n","    else:\n","      refAns_vector_values = row['AnsVector' + embmodelname1]\n","\n","\n","    # Convert the string values to floats\n","    predAns_vector_values = [float(value) for value in predAns_vector_values]\n","    refAns_vector_values = [float(value) for value in refAns_vector_values]\n","\n","    # Convert the lists to PyTorch tensors\n","    predAns_vector_values = torch.tensor(predAns_vector_values).reshape(1, -1)\n","    refAns_vector_values = torch.tensor(refAns_vector_values).reshape(1, -1)\n","\n","    similarity = cosine_similarity(predAns_vector_values, refAns_vector_values)\n","    return similarity.item()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rvUtkwJChL_"},"outputs":[],"source":["df_ref_pred['Cosine'] = df_ref_pred.apply(calculate_cosine, axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dm521szWDEae"},"outputs":[],"source":["df_ref_pred.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QEguQujGD5Nc"},"outputs":[],"source":["df_metrics = df_metrics.merge(df_ref_pred, on=\"Ques_Id\", how=\"left\")[['Ques_Id', 'BLEU1', 'ROUGE1_P', 'ROUGE1_R', 'ROUGE1_F1', 'ROUGEL_P',\n","       'ROUGEL_R', 'ROUGE1_F1.1', 'Precision', 'Recall','Cosine']]"]},{"cell_type":"markdown","metadata":{"id":"G_Ww_QeMGCV6"},"source":["# Save File Metrics & Show Summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTO0g10sEpz4"},"outputs":[],"source":["metricsfile = '09.12-' + predmodel_name +'Predicted_Ans_Score_Zeroshot.csv'\n","print(metricsfile)\n","df_metrics.to_csv(datapath + metricsfile, index=False)\n","df_metrics.head(2)"]},{"cell_type":"markdown","metadata":{"id":"r4vD_07sGAvs"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sTPBYLYFw8F"},"outputs":[],"source":["df_metrics.mean()"]}],"metadata":{"colab":{"collapsed_sections":["xhMxInMCGaYC","kxePEJraGPSv","2jZzbP7b8iw1","sCruv7vhoP2w","lpBtIrI9M7hv","sVUo67UeySRL"],"provenance":[{"file_id":"https://github.com/dasarpai/HBQA/blob/main/09_1_HBQA_ZeroShot.ipynb","timestamp":1697466450190}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}