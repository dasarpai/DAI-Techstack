{"cells":[{"cell_type":"code","source":[],"metadata":{"id":"uvR0q1pE4yMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VacrsXBLqEc"},"outputs":[],"source":["!pip install transformers -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhZbV2OWnuWs"},"outputs":[],"source":["!pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IskCNQeCn5vm"},"outputs":[],"source":["!pip install SentencePiece"]},{"cell_type":"markdown","metadata":{"id":"sCruv7vhoP2w"},"source":["# Load Saved model directly"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3wqesBaSHQY"},"outputs":[],"source":["import tensorflow as tf\n","\n","# Detect and initialize TPU\n","tpu_available = tf.config.experimental.list_logical_devices(\"TPU\")\n","if tpu_available:\n","    print(\"TPU available\")\n","else:\n","    print(\"No TPU available\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yaVUMHxj_jkg"},"outputs":[],"source":["import torch\n","\n","if torch.cuda.is_available():\n","    DEVICE = torch.device(\"cuda\")\n","else:\n","    DEVICE = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1VSaWtRRUbg"},"outputs":[],"source":["DEVICE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYZeJ16zoGDQ"},"outputs":[],"source":["# sample code\n","# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","\n","# TOKENIZER = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-ReformerForQuestionAnswering\")\n","# MODEL = AutoModelForQuestionAnswering.from_pretrained(\"hf-internal-testing/tiny-random-ReformerForQuestionAnswering\")\n","# MODEL.to(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"sVUo67UeySRL"},"source":["# Loading Training Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1Uf713Y8QsS"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxdVz7jQFG2U"},"outputs":[],"source":["df = pd.read_csv('/content/drive/MyDrive/Booksummary/qa_merged.csv')"]},{"cell_type":"code","source":["df"],"metadata":{"id":"qdSgYnT519ED"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zt2QQ5J7Ccvn"},"outputs":[],"source":["Question_Len = max(len(ques) for ques in df.question)\n","Answer_Len = max(len(ans) for ans in df.answer)\n","Question_Len,Answer_Len"]},{"cell_type":"markdown","metadata":{"id":"2pe69lLM442P"},"source":["# Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dsNg38Tz9FQj"},"outputs":[],"source":["import torch\n","import json\n","from tqdm import tqdm\n","import torch.nn as nn\n","from torch.optim import Adam\n","import nltk\n","import spacy\n","import string\n","import evaluate  # Bleu\n","from torch.utils.data import Dataset, DataLoader, RandomSampler\n","import pandas as pd\n","import numpy as np\n","import transformers\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89RaR0jB745I"},"outputs":[],"source":["class QA_Dataset(Dataset):\n","    def __init__(self, tokenizer, dataframe, q_len, t_len):\n","        self.tokenizer = tokenizer\n","        self.q_len = q_len\n","        self.t_len = t_len\n","        self.data = dataframe\n","        self.questions = self.data[\"question\"]\n","        self.context = self.data[\"context\"]\n","        self.answer = self.data['answer']\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        question = self.questions[idx]\n","        context = self.context[idx]\n","        answer = self.answer[idx]\n","\n","        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n","                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n","        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\",\n","                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n","\n","        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n","        labels[labels == 0] = -100\n","\n","        return {\n","            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n","            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n","            \"labels\": labels,\n","            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n","        }"]},{"cell_type":"markdown","metadata":{"id":"Z974XcOXMGtc"},"source":["# Prepare Dataset and Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCXnNdcA9fsP"},"outputs":[],"source":["Q_LEN = Question_Len # 256   # Question Length\n","T_LEN = Answer_Len #32  # Target Length\n","BATCH_SIZE = 2\n","DEVICE = \"cuda:0\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WY_lp1B19tpc"},"outputs":[],"source":["# Dataloader\n","\n","train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","train_sampler = RandomSampler(train_data.index)\n","val_sampler = RandomSampler(val_data.index)\n","\n","qa_dataset = QA_Dataset(TOKENIZER, data, Q_LEN, T_LEN)\n","\n","train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n","val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"]},{"cell_type":"markdown","metadata":{"id":"oCYSDPpOoVI5"},"source":["# Load Base Model for Finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pyg3Qnok-gEl"},"outputs":[],"source":["# TOKENIZER = T5TokenizerFast.from_pretrained(\"t5-base\")\n","# MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n","# MODEL.to(DEVICE)\n","# OPTIMIZER = Adam(MODEL.parameters(), lr=0.00001)"]},{"cell_type":"markdown","metadata":{"id":"QE0WSRmUL3pI"},"source":["# Start finetune (Training)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDLx6WAf-A3V"},"outputs":[],"source":["# train_loss = 0\n","# val_loss = 0\n","# train_batch_count = 0\n","# val_batch_count = 0\n","\n","# for epoch in range(2):\n","#     MODEL.train()\n","#     for batch in tqdm(train_loader, desc=\"Training batches\"):\n","#         input_ids = batch[\"input_ids\"].to(DEVICE)\n","#         attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","#         labels = batch[\"labels\"].to(DEVICE)\n","#         decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n","\n","#         outputs = MODEL(\n","#                           input_ids=input_ids,\n","#                           attention_mask=attention_mask,\n","#                           labels=labels,\n","#                           decoder_attention_mask=decoder_attention_mask\n","#                         )\n","\n","#         OPTIMIZER.zero_grad()\n","#         outputs.loss.backward()\n","#         OPTIMIZER.step()\n","#         train_loss += outputs.loss.item()\n","#         train_batch_count += 1\n","\n","#     #Evaluation\n","#     MODEL.eval()\n","#     for batch in tqdm(val_loader, desc=\"Validation batches\"):\n","#         input_ids = batch[\"input_ids\"].to(DEVICE)\n","#         attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","#         labels = batch[\"labels\"].to(DEVICE)\n","#         decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n","\n","#         outputs = MODEL(\n","#                           input_ids=input_ids,\n","#                           attention_mask=attention_mask,\n","#                           labels=labels,\n","#                           decoder_attention_mask=decoder_attention_mask\n","#                         )\n","\n","#         OPTIMIZER.zero_grad()\n","#         outputs.loss.backward()\n","#         OPTIMIZER.step()\n","#         val_loss += outputs.loss.item()\n","#         val_batch_count += 1\n","\n","#     print(f\"{epoch+1}/{2} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")"]},{"cell_type":"markdown","metadata":{"id":"H8WhoU_2hdnx"},"source":["# Save Finetuned (trained) Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dRyro45RAk1L"},"outputs":[],"source":["# model_path = \"/content/drive/MyDrive/Booksummary/t5qa_model\"\n","# token_path = \"/content/drive/MyDrive/Booksummary/t5qa_tokenizer\"\n","\n","# MODEL.save_pretrained(model_path)\n","# TOKENIZER.save_pretrained(token_path)\n","\n","# # Saved files\n","# \"\"\"('qa_tokenizer/tokenizer_config.json',\n","#  'qa_tokenizer/special_tokens_map.json',\n","#  'qa_tokenizer/spiece.model',\n","# 'qa_tokenizer/added_tokens.json',\n","# 'qa_tokenizer/tokenizer.json')\"\"\""]},{"cell_type":"markdown","metadata":{"id":"HBit8QnZg4iF"},"source":["# Load Model from Memory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWTckpGQg38e"},"outputs":[],"source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","# Replace 'model_path' with the path to your saved model directory\n","model_path = r\"/content/drive/MyDrive/Booksummary/t5qa_model\"\n","token_path = r\"/content/drive/MyDrive/Booksummary/t5qa_tokenizer\"\n","\n","# Load the corresponding tokenizer\n","TOKENIZER = T5Tokenizer.from_pretrained(token_path)\n","\n","\n","# Load the pre-trained T5 model\n","MODEL = T5ForConditionalGeneration.from_pretrained(model_path)\n","MODEL.to(DEVICE)"]},{"cell_type":"code","source":["# input_text = \"Translate the following English text to French: 'Hello, how are you?'\"\n","\n","# # Tokenize the input text\n","# input_ids = tokenizer.encode(input_text, return_tensors='pt')\n","# input_ids = input_ids.to(DEVICE)\n","\n","# # Generate translated text\n","# translated_ids = MODEL.generate(input_ids)\n","\n","# # Decode the generated IDs back to text\n","# translated_text = TOKENIZER.decode(translated_ids[0], skip_special_tokens=True)\n","\n","# print(\"Translated Text:\", translated_text)\n"],"metadata":{"id":"suQoNy6t2ruM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"832Ayr0HC95L"},"source":["# Predict Answers from t5 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4Gq3uQTA0DS"},"outputs":[],"source":["def predict_answer(context, question):\n","\n","    inputs = TOKENIZER(question, context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n","\n","    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n","    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n","\n","    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n","\n","    predicted_answer = TOKENIZER.decode(outputs.flatten(), skip_special_tokens=True)\n","    return predicted_answer\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHyYhQi6g4GJ"},"outputs":[],"source":["pred_answers=[]\n","ref_answers=[]\n","question=[]\n","N = 5#len(data)\n","for i in range(N):\n","  pred_ans = predict_answer(data.iloc[i]['context'],data.iloc[i]['question'])\n","  pred_answers.append(pred_ans)\n","  ref_answers.append(data.iloc[i]['answer'])\n","  question.append(data.iloc[i]['question'])\n","  print(pred_ans)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WtNurcbEctC"},"outputs":[],"source":["prediction_path =r'/content/drive/MyDrive/Booksummary/t5predicted_ans.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BK1xRpA7gu8m"},"outputs":[],"source":["# df1 = pd.DataFrame({\"question\": question, \"ref_answer\": ref_answers, \"pred_answer\": pred_answers})\n","# df1.index = data.index\n","\n","# val_data_index = val_data.index\n","\n","# df1['train_data']=True\n","\n","# for i in val_data_index:\n","#   df1.loc[i,\"train_data\"]=False\n","\n","# df1.to_csv(prediction_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_q6-PZLvDojv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjoZpo1HhIi_"},"outputs":[],"source":["df1 = pd.read_csv(prediction_path)\n","df1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJrrLq--A8Db"},"outputs":[],"source":["idx=[10,100,201]\n","for i in idx:\n","  pred_ans = predict_answer(data.iloc[i]['context'],data.iloc[i]['question'])\n","  print(\"Question: \" + data.iloc[i]['question'])\n","  print(\"Refrence Ans: \"+ data.iloc[i]['answer'])\n","  print(\"Predicted Answer: \", pred_ans)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tABXVsrWhW03"},"outputs":[],"source":["def calculate_score(ref_answer, predicted_answer):\n","\n","  # bleu = evaluate.load(\"google_bleu\")\n","  # bleu_score1  = bleu.compute(predictions=[predicted_answer], references=[ref_answer])\n","\n","  pred_answer_ids = TOKENIZER.encode(predicted_answer, return_tensors='pt')[0][0]\n","  pred_answer_ids = pred_answer_ids.to(DEVICE)\n","\n","  ref_answer_ids = TOKENIZER.encode(ref_answer, return_tensors='pt')[0][0]\n","  ref_answer_ids = pred_answer_ids.to(DEVICE)\n","\n","  # squad = evaluate.load(\"squad\")\n","  glue_qqp = evaluate.load('glue', 'qqp')\n","\n","\n","  glue_qqp_score1 = glue_qqp.compute(predictions=[pred_answer_ids],\n","                      references=[ref_answer_ids])\n","\n","\n","\n","  return glue_qqp_score1 #squad_score1 #bleu_score1#, squad_score1, glue_score1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhEK-0Acx_PA"},"outputs":[],"source":["bleu_score=[]\n","squad_score=[]\n","glue_qqp_score=[]\n","from datasets import load_metric\n","\n","N= 5 #len(df1)\n","\n","for i in range(N):\n","  # bleu_score1 = calculate_score( df1.iloc[i]['ref_answer'],df1.iloc[i]['pred_answer'])\n","  ref_ans = df1.iloc[i]['ref_answer'],\n","  pred_ans = df1.iloc[i]['pred_answer']\n","  print(ref_ans, '\\n\\n', pred_ans)\n","\n","\n","  pred_answer_ids = TOKENIZER.encode(pred_ans, return_tensors='pt')[0][0]\n","  pred_answer_ids = pred_answer_ids.to(DEVICE)\n","\n","  ref_answer_ids = TOKENIZER.encode(ref_ans, return_tensors='pt')[0][0]\n","  ref_answer_ids = pred_answer_ids.to(DEVICE)\n","\n","\n","  # glue_qqp_score1 = calculate_score(ref_ans,  pred_ans)\n","  glue_qqp = evaluate.load('glue', 'qqp')\n","\n","  glue_qqp_score1 = glue_qqp.compute(predictions=[pred_answer_ids],\n","                      references=[ref_answer_ids])\n","\n","  glue_qqp_score.append(glue_qqp_score1)\n","\n","\n","\n","  squad_metric = load_metric(\"squad_v2\")\n","  squad_score = squad_metric.compute(predictions=pred_answer_ids, references=ref_answer_ids)\n","\n","  squad_score.append(squad_score1)\n","\n","  # glue_score.append(glue_score1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGnwG2ikLPeu"},"outputs":[],"source":["glue_qqp_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QO3QKcIAKA2-"},"outputs":[],"source":["TOKENIZER.encode(pred_ans, return_tensors='pt')[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VW78c6EUQv4H"},"outputs":[],"source":["# predict_answer(data.iloc[i]['context'],data.iloc[i]['question'],data.iloc[i]['answer'],)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VA3y6wBZO9LY"},"outputs":[],"source":["df1 = pd.DataFrame({\"question\": question, \"ref_answer\": ref_answers, \"pred_answer\": pred_answers,  \"bleu_score\":bleu_score})\n","df1.index = data.index\n","\n","val_data_index = val_data.index\n","\n","df1['train_data']=True\n","\n","for i in val_data_index:\n","  df1.loc[i,\"train_data\"]=False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8f10g5VuUXwq"},"outputs":[],"source":["df1.to_csv('/content/drive/MyDrive/Booksummary/t5predicted_ans.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"66Vx5Q8EbWmJ"},"outputs":[],"source":["df1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1WfT-99SNVE"},"outputs":[],"source":["def extract_numeric(dic):\n","    return next(iter(dic.values()))\n","\n","df1['bleu_score'] = df1.bleu_score.apply(extract_numeric)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IxDCAlxrbgbs"},"outputs":[],"source":["df1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqR3ynrCWthX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rk34WOaqW6gl"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6_8GuXfw1yX"},"outputs":[],"source":["# sst2, mnli, mnli_mismatched, mnli_matched, qnli, rte, wnli, cola,stsb, mrpc, qqp, and hans."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HguahnXowLWQ"},"outputs":[],"source":["from evaluate import load\n","glue_metric = load('glue', 'mrpc')  # 'mrpc' or 'qqp'\n","references = [0, 1]\n","predictions = [0, 1]\n","results = glue_metric.compute(predictions=predictions, references=references)\n","print(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGBKBRuewmcl"},"outputs":[],"source":["from evaluate import load\n","glue_metric = load('glue', 'stsb')\n","references = [0., 1., 2., 3., 4., 5.]\n","predictions = [-10., -11., -12., -13., -14., -15.]\n","results = glue_metric.compute(predictions=predictions, references=references)\n","print(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AlyJn-mwp8G"},"outputs":[],"source":["from evaluate import load\n","glue_metric = load('glue', 'cola')\n","references = [0, 1]\n","predictions = [1, 1]\n","results = glue_metric.compute(predictions=predictions, references=references)\n","results"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1rRz6UIWNx8AZ6QRlbaQ-dS_ducopO6_G","timestamp":1689756454860}],"mount_file_id":"1xlayoptfnDVbk0ICX_uDlqw7uh-ScbB0","authorship_tag":"ABX9TyO8gmVcaMKecq4wj4GTugfj"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}