{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1xlayoptfnDVbk0ICX_uDlqw7uh-ScbB0","timestamp":1689782329004},{"file_id":"1rRz6UIWNx8AZ6QRlbaQ-dS_ducopO6_G","timestamp":1689756454860}],"gpuType":"T4","mount_file_id":"1Vm7-VtNOa7yKUqFbPbxesMOHGzt2_O18","authorship_tag":"ABX9TyM4HRh2tyMRG3N3Y+3vQ9cU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"id":"aR5xT6XM4aCZ","executionInfo":{"status":"error","timestamp":1689785424085,"user_tz":-330,"elapsed":732,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}},"outputId":"7c86f21c-9d2e-46f3-a984-5aa2d5c1f162"},"execution_count":74,"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-74-2f79ebd4dd12>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    165\u001b[0m   \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     )\n","\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"]}]},{"cell_type":"code","source":["# Use a pipeline as a high-level helper\n","from transformers import pipeline\n","\n","pipe = pipeline(\"question-answering\", model=\"adityabhat/GPT3\")"],"metadata":{"id":"W_oBRXPe4OHU","executionInfo":{"status":"ok","timestamp":1689785398679,"user_tz":-330,"elapsed":6372,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["# Load model directly\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","\n"],"metadata":{"id":"6hF6EmMd4UdZ","executionInfo":{"status":"ok","timestamp":1689785398680,"user_tz":-330,"elapsed":35,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["# # Load model directly\n","# from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-ReformerForQuestionAnswering\")\n","# model = AutoModelForQuestionAnswering.from_pretrained(\"hf-internal-testing/tiny-random-ReformerForQuestionAnswering\")"],"metadata":{"id":"l4zgB4kmVMyy","executionInfo":{"status":"ok","timestamp":1689785398681,"user_tz":-330,"elapsed":33,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["#!pip install transformers\n","!pip install evaluate"],"metadata":{"id":"G2-GNKTaJ8mA","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"error","timestamp":1689785398682,"user_tz":-330,"elapsed":33,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}},"outputId":"404b236f-63ad-4864-eb4c-71f689e2a2b6"},"execution_count":73,"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-73-5ed9521d8154>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!pip install transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install evaluate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    165\u001b[0m   \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     )\n","\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"]}]},{"cell_type":"code","source":["# !pip install SentencePiece"],"metadata":{"id":"x_HepHhdKNRw","executionInfo":{"status":"aborted","timestamp":1689785398683,"user_tz":-330,"elapsed":32,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"f1Uf713Y8QsS","executionInfo":{"status":"aborted","timestamp":1689785398684,"user_tz":-330,"elapsed":33,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_qafinal = pd.read_csv('/content/drive/MyDrive/Booksummary/QA_Final.csv')\n","\n","df_prompt_qa = pd.read_csv(\"/content/drive/MyDrive/Booksummary/Question_Prompts_with_QA.csv\")\n","\n","df_prompt_qa  = df_prompt_qa.loc[df_prompt_qa.QA.notna()][['Section_Id','Context_Id','Prompt']]\n","df_prompt_qa['Context'] = df_prompt_qa['Prompt'].str.replace('Text: ', \"|\").str.replace('Question: \\n', \"|\").str.split(\"|\").str[1].str.strip()\n","\n","df_prompt_qa = df_prompt_qa[['Section_Id','Context_Id', 'Context']]\n","\n","df_qafinal = df_qafinal.merge(df_prompt_qa, left_on=\"ContextId\", right_on=\"Context_Id\", how=\"left\")\n","df_qafinal['Question'] = df_qafinal.Question.str.replace('Question: ','')\n","df_qafinal['Answer'] = df_qafinal.Answer.str.replace('Answer: ','')\n","data = df_qafinal[['QuesId','Question','Answer','Context']]\n","data.rename(columns={\"Question\":\"question\",\"Answer\":\"answer\",\"Context\":\"context\"},inplace=True)\n","data"],"metadata":{"id":"fxdVz7jQFG2U","executionInfo":{"status":"aborted","timestamp":1689785398685,"user_tz":-330,"elapsed":34,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Question_Len = max(len(ques) for ques in data.question)\n","Answer_Len = max(len(ans) for ans in data.answer)\n","Question_Len,Answer_Len"],"metadata":{"id":"Zt2QQ5J7Ccvn","executionInfo":{"status":"aborted","timestamp":1689785398686,"user_tz":-330,"elapsed":34,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import json\n","from tqdm import tqdm\n","import torch.nn as nn\n","from torch.optim import Adam\n","import nltk\n","import spacy\n","import string\n","import evaluate  # Bleu\n","from torch.utils.data import Dataset, DataLoader, RandomSampler\n","import pandas as pd\n","import numpy as np\n","import transformers\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","# from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"dsNg38Tz9FQj","executionInfo":{"status":"aborted","timestamp":1689785398687,"user_tz":-330,"elapsed":35,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class QA_Dataset(Dataset):\n","    def __init__(self, tokenizer, dataframe, q_len, t_len):\n","        self.tokenizer = tokenizer\n","        self.q_len = q_len\n","        self.t_len = t_len\n","        self.data = dataframe\n","        self.questions = self.data[\"question\"]\n","        self.context = self.data[\"context\"]\n","        self.answer = self.data['answer']\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, idx):\n","        question = self.questions[idx]\n","        context = self.context[idx]\n","        answer = self.answer[idx]\n","\n","        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n","                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n","        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\",\n","                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n","\n","        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n","        labels[labels == 0] = -100\n","\n","        return {\n","            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n","            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n","            \"labels\": labels,\n","            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n","        }"],"metadata":{"id":"89RaR0jB745I","executionInfo":{"status":"aborted","timestamp":1689785398689,"user_tz":-330,"elapsed":37,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","if torch.cuda.is_available():\n","    DEVICE = torch.device(\"cuda\")\n","else:\n","    DEVICE = torch.device(\"cpu\")"],"metadata":{"id":"yaVUMHxj_jkg","executionInfo":{"status":"aborted","timestamp":1689785398690,"user_tz":-330,"elapsed":38,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TOKENIZER = T5TokenizerFast.from_pretrained(\"t5-base\")\n","# MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True)\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","# TOKENIZER = GPT2Tokenizer.from_pretrained(\"adityabhat/GPT3\")\n","# MODEL = GPT2LMHeadModel.from_pretrained(\"adityabhat/GPT3\")\n","MODEL_NAME = \"gpt2\"\n","MODEL = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n","TOKENIZER = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n","\n","MODEL.to(DEVICE)\n","OPTIMIZER = Adam(MODEL.parameters(), lr=0.00001)"],"metadata":{"id":"Pyg3Qnok-gEl","executionInfo":{"status":"aborted","timestamp":1689785398690,"user_tz":-330,"elapsed":38,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Q_LEN = Question_Len # 256   # Question Length\n","T_LEN = Answer_Len #32  # Target Length\n","BATCH_SIZE = 2\n","# DEVICE = \"cuda:0\""],"metadata":{"id":"uCXnNdcA9fsP","executionInfo":{"status":"aborted","timestamp":1689785398691,"user_tz":-330,"elapsed":38,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataloader\n","\n","train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","train_sampler = RandomSampler(train_data.index)\n","val_sampler = RandomSampler(val_data.index)\n","\n","qa_dataset = QA_Dataset(TOKENIZER, data, Q_LEN, T_LEN)\n","\n","train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n","val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"],"metadata":{"id":"WY_lp1B19tpc","executionInfo":{"status":"aborted","timestamp":1689785398691,"user_tz":-330,"elapsed":38,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","train_loss = 0\n","val_loss = 0\n","train_batch_count = 0\n","val_batch_count = 0\n","\n","for epoch in range(2):\n","    MODEL.train()\n","    for batch in tqdm(train_loader, desc=\"Training batches\"):\n","        input_ids = batch[\"input_ids\"].to(DEVICE)\n","        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","        labels = batch[\"input_ids\"].to(DEVICE)\n","\n","\n","        # Forward pass\n","        outputs = MODEL(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","\n","        )\n","\n","        # Compute the loss\n","        loss = outputs.loss\n","\n","        # Backpropagation and optimization\n","        OPTIMIZER.zero_grad()\n","        loss.backward()\n","        OPTIMIZER.step()\n","        train_loss += loss.item()\n","        train_batch_count += 1\n","\n","    # Evaluation\n","    MODEL.eval()\n","    for batch in tqdm(val_loader, desc=\"Validation batches\"):\n","        input_ids = batch[\"input_ids\"].to(DEVICE)\n","        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","        labels = batch[\"input_ids\"].to(DEVICE)\n","\n","\n","        # Forward pass\n","        with torch.no_grad():  # No need to compute gradients during evaluation\n","            outputs = MODEL(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                labels=labels\n","            )\n","\n","        # Compute the loss\n","        loss = outputs.loss\n","        val_loss += loss.item()\n","        val_batch_count += 1\n","\n","    print(f\"{epoch+1}/{2} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss / val_batch_count}\")\n"],"metadata":{"id":"KMY1PR2e6wsx","executionInfo":{"status":"aborted","timestamp":1689785398692,"user_tz":-330,"elapsed":39,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Load the GPT-2 model from the saved directory\n","# MODEL_PATH = \"/content/drive/MyDrive/Booksummary/gpt2qa_model\"\n","# MODEL = GPT2LMHeadModel.from_pretrained(MODEL_PATH)\n","\n","# # Load the GPT-2 tokenizer from the saved directory\n","# TOKENIZER_PATH = \"/content/drive/MyDrive/Booksummary/gpt2qa_tokenizer\"\n","# TOKENIZER = GPT2Tokenizer.from_pretrained(TOKENIZER_PATH)\n"],"metadata":{"id":"XF2lyeO26wdX","executionInfo":{"status":"aborted","timestamp":1689785398693,"user_tz":-330,"elapsed":40,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL.save_pretrained(\"/content/drive/MyDrive/Booksummary/gptqa_model\")\n","TOKENIZER.save_pretrained(\"/content/drive/MyDrive/Booksummary/gpt2qa_tokenizer\")\n","\n","# Saved files\n","\"\"\"('qa_tokenizer/tokenizer_config.json',\n"," 'qa_tokenizer/special_tokens_map.json',\n"," 'qa_tokenizer/spiece.model',\n","'qa_tokenizer/added_tokens.json',\n","'qa_tokenizer/tokenizer.json')\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"dRyro45RAk1L","executionInfo":{"status":"ok","timestamp":1689784033909,"user_tz":-330,"elapsed":3621,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}},"outputId":"8c3f0d8a-abe4-480b-e169-8755004d9c86"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"('qa_tokenizer/tokenizer_config.json',\\n 'qa_tokenizer/special_tokens_map.json',\\n 'qa_tokenizer/spiece.model',\\n'qa_tokenizer/added_tokens.json',\\n'qa_tokenizer/tokenizer.json')\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["def predict_answer(context, question, ref_answer=None):\n","    # Add the padding token to the tokenizer\n","    TOKENIZER.add_special_tokens({'pad_token': '[PAD]'})\n","\n","    inputs = TOKENIZER(question, context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n","\n","    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n","    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n","\n","    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n","\n","    predicted_answer = TOKENIZER.decode(outputs.flatten(), skip_special_tokens=True)\n","\n","    if ref_answer:\n","        # Load the Bleu metric\n","        bleu = evaluate.load(\"google_bleu\")\n","        score = bleu.compute(predictions=[predicted_answer],\n","                            references=[ref_answer])\n","\n","        print(\"Context: \\n\", context)\n","        print(\"\\n\")\n","        print(\"Question: \\n\", question)\n","        return {\n","            \"Reference Answer: \": ref_answer,\n","            \"Predicted Answer: \": predicted_answer,\n","            \"BLEU Score: \": score\n","        }\n","    else:\n","        return predicted_answer\n"],"metadata":{"id":"lTPzt1_-_D6N","executionInfo":{"status":"ok","timestamp":1689785272646,"user_tz":-330,"elapsed":1147,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["DEVICE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r5rstKIWDWHX","executionInfo":{"status":"ok","timestamp":1689785302380,"user_tz":-330,"elapsed":438,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}},"outputId":"946956c1-6cd7-406d-a771-eef65414d687"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["i=10\n","predict_answer(data.iloc[i]['context'],data.iloc[i]['question'],data.iloc[i]['answer'],)"],"metadata":{"id":"wJrrLq--A8Db","colab":{"base_uri":"https://localhost:8080/","height":418},"executionInfo":{"status":"error","timestamp":1689785276284,"user_tz":-330,"elapsed":14,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}},"outputId":"3c465b9d-af92-4625-b394-d31731f6fddc"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Input length of input_ids is 144, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-62558aa54cd7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-65-7ac2d51ff063>\u001b[0m in \u001b[0;36mpredict_answer\u001b[0;34m(context, question, ref_answer)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpredicted_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1539\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2363\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1077\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"]}]},{"cell_type":"code","source":["i=100\n","predict_answer(data.iloc[i]['context'],data.iloc[i]['question'],data.iloc[i]['answer'],)"],"metadata":{"id":"6IBTqHWFI8uG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689784252990,"user_tz":-330,"elapsed":1132,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}},"outputId":"6946735f-1485-4c16-baee-f7f43c9586cf"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Input length of input_ids is 144, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"]},{"output_type":"stream","name":"stdout","text":["Context: \n"," \"On the other hand, the Danavas, white as the clouds from which the rain hath dropped, possessing great strength and bold hearts, ascended the sky, and by hurling down thousands of mountains, continually harassed the gods. And those dreadful mountains, like masses of clouds, with their trees and flat tops, falling from the sky, collided with one another and produced a tremendous roar. And when thousands of warriors shouted without intermission in the field of battle and mountains with the woods thereon began to fall around, the earth with her forests trembled. Then the divine Nara appeared at the scene of the dreadful conflict between the Asuras and the Ganas (the followers of Rudra), and reducing to dust those rocks by means of his gold-headed arrows, he covered the heavens with dust. Thus discomfited by the gods, and seeing the furious discus scouring the fields of heaven like a blazing flame, the mighty Danavas entered the bowels of the earth, while others plunged into the sea of salt-waters.\n","\"And having gained the victory, the gods offered due respect to Mandara and placed him again on his own base. And the nectar-bearing gods made the heavens resound with their shouts, and went to their own abodes. And the gods, on returning to the heavens, rejoiced greatly, and Indra and the other deities made over to Narayana the vessel of Amrita for careful keeping.'\"\n","And so ends the nineteenth section in the Astika Parva of the Adi Parva.\n","\n","\n","Question: \n"," What did the Danavas hurl down from the sky to harass the gods according to the text?\n"]},{"output_type":"execute_result","data":{"text/plain":["{'Reference Answer: ': 'The Danavas hurled down thousands of mountains from the sky to harass the gods according to the text.',\n"," 'Predicted Answer: ': 'What did the Danavas hurl down from the sky to harass the gods according to the text?\"On the other hand, the Danavas, white as the clouds from which the rain hath dropped, possessing great strength and bold hearts, ascended the sky, and by hurling down thousands of mountains, continually harassed the gods. And those dreadful mountains, like masses of clouds, with their trees and flat tops, falling from the sky, collided with one another and produced a tremendous roar. And when thousands of warriors shouted without intermission in the field of battle and mountains with the woods thereon began to fall around, the earth with her forests trembled. Then the divine Nara appeared at the scene of the',\n"," 'BLEU Score: ': {'google_bleu': 0.09363295880149813}}"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["i=100\n","pred_ans = predict_answer(data.iloc[i]['context'],data.iloc[i]['question'],data.iloc[i]['answer'],)"],"metadata":{"id":"B9fd9XcDOm8I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689784661385,"user_tz":-330,"elapsed":1162,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}},"outputId":"1a48173f-d8d9-4410-888a-22a52c410d82"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Input length of input_ids is 144, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"]},{"output_type":"stream","name":"stdout","text":["Context: \n"," \"On the other hand, the Danavas, white as the clouds from which the rain hath dropped, possessing great strength and bold hearts, ascended the sky, and by hurling down thousands of mountains, continually harassed the gods. And those dreadful mountains, like masses of clouds, with their trees and flat tops, falling from the sky, collided with one another and produced a tremendous roar. And when thousands of warriors shouted without intermission in the field of battle and mountains with the woods thereon began to fall around, the earth with her forests trembled. Then the divine Nara appeared at the scene of the dreadful conflict between the Asuras and the Ganas (the followers of Rudra), and reducing to dust those rocks by means of his gold-headed arrows, he covered the heavens with dust. Thus discomfited by the gods, and seeing the furious discus scouring the fields of heaven like a blazing flame, the mighty Danavas entered the bowels of the earth, while others plunged into the sea of salt-waters.\n","\"And having gained the victory, the gods offered due respect to Mandara and placed him again on his own base. And the nectar-bearing gods made the heavens resound with their shouts, and went to their own abodes. And the gods, on returning to the heavens, rejoiced greatly, and Indra and the other deities made over to Narayana the vessel of Amrita for careful keeping.'\"\n","And so ends the nineteenth section in the Astika Parva of the Adi Parva.\n","\n","\n","Question: \n"," What did the Danavas hurl down from the sky to harass the gods according to the text?\n"]}]},{"cell_type":"code","source":["pred_answers=[]\n","bleu_score=[]\n","for i in range(len(data)):\n","  pred_ans = predict_answer(data.iloc[i]['context'],data.iloc[i]['question'],data.iloc[i]['answer'],)\n","  pred_answers.append(pred_ans['Predicted Answer: '])\n","  bleu_score.append(pred_ans['BLEU Score: '])\n"],"metadata":{"id":"AAGgYN_2PF-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predict_answer(data.iloc[i]['context'],data.iloc[i]['question'],data.iloc[i]['answer'],)"],"metadata":{"id":"VW78c6EUQv4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1 = pd.DataFrame({\"pred_answer\": pred_answers, \"bleu_score\":bleu_score})\n","df1.index = data.index\n","\n","val_data_index = val_data.index\n","\n","df1['train_data']=False\n","\n","for i in val_data_index:\n","  df1.loc[i,\"train_data\"]=True"],"metadata":{"id":"VA3y6wBZO9LY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1.to_csv('/content/drive/MyDrive/Booksummary/t5predicted_ans.csv')"],"metadata":{"id":"8f10g5VuUXwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"r1WfT-99SNVE","executionInfo":{"status":"ok","timestamp":1689740434580,"user_tz":-330,"elapsed":24,"user":{"displayName":"Hari Thapliyaal","userId":"09088303666341280217"}},"outputId":"ae56b05c-cf1b-4ec5-9822-3683bdf18a6e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            pred_answer  \\\n","0     The Mahabharata was composed by Ugrasrava, the...   \n","1     The Mahabharata is a sacred and sacred book, w...   \n","2     The creation of the world began according to t...   \n","3     The universe will be affected at the end of th...   \n","4     The generation of Devas was thirty-three thous...   \n","...                                                 ...   \n","1114  Agni responds to Drona's request by saying tha...   \n","1115  Jarita refers to the \"eldest\" and \"next\" among...   \n","1116  Mandapala says that co-wives and clandestine l...   \n","1117  Mandapala compares Jarita's jealousy to the je...   \n","1118  Mandapala gives the advice that women should t...   \n","\n","                                bleu_score  train_data  \n","0     {'google_bleu': 0.21621621621621623}       False  \n","1     {'google_bleu': 0.09340659340659341}       False  \n","2     {'google_bleu': 0.02608695652173913}       False  \n","3     {'google_bleu': 0.19801980198019803}        True  \n","4     {'google_bleu': 0.06204379562043796}       False  \n","...                                    ...         ...  \n","1114  {'google_bleu': 0.16216216216216217}       False  \n","1115   {'google_bleu': 0.1111111111111111}       False  \n","1116                 {'google_bleu': 0.14}        True  \n","1117  {'google_bleu': 0.14912280701754385}       False  \n","1118  {'google_bleu': 0.05737704918032787}       False  \n","\n","[1119 rows x 3 columns]"],"text/html":["\n","\n","  <div id=\"df-004d1850-6d1f-41e1-b93e-98b509d31427\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pred_answer</th>\n","      <th>bleu_score</th>\n","      <th>train_data</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Mahabharata was composed by Ugrasrava, the...</td>\n","      <td>{'google_bleu': 0.21621621621621623}</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The Mahabharata is a sacred and sacred book, w...</td>\n","      <td>{'google_bleu': 0.09340659340659341}</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The creation of the world began according to t...</td>\n","      <td>{'google_bleu': 0.02608695652173913}</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The universe will be affected at the end of th...</td>\n","      <td>{'google_bleu': 0.19801980198019803}</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The generation of Devas was thirty-three thous...</td>\n","      <td>{'google_bleu': 0.06204379562043796}</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1114</th>\n","      <td>Agni responds to Drona's request by saying tha...</td>\n","      <td>{'google_bleu': 0.16216216216216217}</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1115</th>\n","      <td>Jarita refers to the \"eldest\" and \"next\" among...</td>\n","      <td>{'google_bleu': 0.1111111111111111}</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1116</th>\n","      <td>Mandapala says that co-wives and clandestine l...</td>\n","      <td>{'google_bleu': 0.14}</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1117</th>\n","      <td>Mandapala compares Jarita's jealousy to the je...</td>\n","      <td>{'google_bleu': 0.14912280701754385}</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1118</th>\n","      <td>Mandapala gives the advice that women should t...</td>\n","      <td>{'google_bleu': 0.05737704918032787}</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1119 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-004d1850-6d1f-41e1-b93e-98b509d31427')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-c8175739-3dc8-407c-97c4-1873da68208e\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c8175739-3dc8-407c-97c4-1873da68208e')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-c8175739-3dc8-407c-97c4-1873da68208e button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-004d1850-6d1f-41e1-b93e-98b509d31427 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-004d1850-6d1f-41e1-b93e-98b509d31427');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":[],"metadata":{"id":"CqR3ynrCWthX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rk34WOaqW6gl"},"execution_count":null,"outputs":[]}]}