{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tutorial-tensorflow.pynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM3EvFh8XuFhRks3dkWueJg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"egXYGITY9RAo"},"source":["#"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5q2-fyJQ9anA"},"source":["# Gradient"]},{"cell_type":"code","metadata":{"id":"gUljTzr49hrS","executionInfo":{"status":"ok","timestamp":1615036646362,"user_tz":-330,"elapsed":3990,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}}},"source":["import torch\r\n","# The autograd package provides automatic differentiation \r\n","# for all operations on Tensors"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPMkKA6X94fe","executionInfo":{"status":"ok","timestamp":1615036649010,"user_tz":-330,"elapsed":2758,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}}},"source":["# requires_grad = True -> tracks all operations on the tensor. \r\n","x = torch.randn(5, requires_grad=True)\r\n","y = x * 2"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5wEUe2tW9wMq","executionInfo":{"status":"ok","timestamp":1615036651460,"user_tz":-330,"elapsed":1626,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"6c698f2d-eb0b-4730-eefe-29359809cda3"},"source":["# y was created as a result of an operation, so it has a grad_fn attribute.\r\n","# grad_fn: references a Function that has created the Tensor\r\n","print(x) # created by the user -> grad_fn is None\r\n","print(y)\r\n","print(y.grad_fn)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["tensor([ 0.1226, -0.2375, -0.0798,  1.2623, -0.1273], requires_grad=True)\n","tensor([ 0.2452, -0.4751, -0.1597,  2.5245, -0.2547], grad_fn=<MulBackward0>)\n","<MulBackward0 object at 0x7fbb8ff28250>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Li_B-1U6_4yc","executionInfo":{"status":"ok","timestamp":1615036657011,"user_tz":-330,"elapsed":1837,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"c7ee2d39-f6af-47bd-f609-b1d29f0e83a6"},"source":["# Let's compute the gradients with backpropagation\r\n","# When we finish our computation we can call .backward() and have all the gradients computed automatically.\r\n","# The gradient for this tensor will be accumulated into .grad attribute.\r\n","# It is the partial derivate of the function w.r.t. the tensor\r\n","\r\n","y=y.sum() #after you perform these operations like sum, mean, median etc.\r\n","y.backward() #you need to perform backward operation. griandent of x will be calculated if required_grad=True\r\n","print (x.grad ) #you can see the gradient. If ou change the y and again perform some calculation again after backward operation gradient will be calculated.\r\n","#gradient # dy/dx"],"execution_count":4,"outputs":[{"output_type":"stream","text":["tensor([2., 2., 2., 2., 2.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yl1AzdJY-rmn","executionInfo":{"status":"ok","timestamp":1615036671303,"user_tz":-330,"elapsed":1864,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}}},"source":[""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBCJG3C99_91","executionInfo":{"status":"ok","timestamp":1615037333426,"user_tz":-330,"elapsed":14165,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"d596063c-ba6c-477b-bd32-c0684ab89ac1"},"source":["print (x)\r\n","y = x * 2\r\n","print (y)\r\n","for _ in range(10):\r\n","    y = y * 2\r\n","\r\n","print(y)\r\n","print(y.shape)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["tensor([ 0.1226, -0.2375, -0.0798,  1.2623, -0.1273], requires_grad=True)\n","tensor([ 0.2452, -0.4751, -0.1597,  2.5245, -0.2547], grad_fn=<MulBackward0>)\n","tensor([ 251.0965, -486.4962, -163.4991, 2585.1348, -260.7670],\n","       grad_fn=<MulBackward0>)\n","torch.Size([5])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"liLT1geAtSsO","executionInfo":{"status":"ok","timestamp":1615037337358,"user_tz":-330,"elapsed":13810,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}}},"source":[""],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"ykXQR_X2tzrm"},"source":["# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\r\n","# It computes partial derivates while applying the chain rule\r\n","\r\n","# -------------\r\n","# Model with non-scalar output:\r\n","# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \r\n","# specify a gradient argument that is a tensor of matching shape.\r\n","# needed for vector-Jacobian product"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zus-cHNpuO70","executionInfo":{"status":"ok","timestamp":1615037297110,"user_tz":-330,"elapsed":2423,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"693e9bb5-eec2-4719-f4d2-76cbebadf05c"},"source":["y.shape"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dZo5RuitNUl","executionInfo":{"status":"ok","timestamp":1615037338864,"user_tz":-330,"elapsed":7475,"user":{"displayName":"Hari Thapliyaal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghe9hJnok_MYMV4Ol_O45RoplvJrRkuikXSvQWNtg=s64","userId":"09088303666341280217"}},"outputId":"96a895e9-870b-4a25-9e0d-23f819dea95d"},"source":["v = torch.tensor([0.1, 1.0, 0.0001, 0.01, 0.001], dtype=torch.float32)\r\n","y.backward(v)\r\n","print(x.grad)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["tensor([4302.7998, 6146.0000, 4098.2046, 4118.4800, 4100.0479])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xDAY2ZKtum5W"},"source":["# -------------\r\n","# Stop a tensor from tracking history:\r\n","# For example during our training loop when we want to update our weights\r\n","# then this update operation should not be part of the gradient computation\r\n","# - x.requires_grad_(False)\r\n","# - x.detach()\r\n","# - wrap in 'with torch.no_grad():'\r\n","\r\n","# .requires_grad_(...) changes an existing flag in-place.\r\n","a = torch.randn(2, 2)\r\n","print(a.requires_grad)\r\n","b = ((a * 3) / (a - 1))\r\n","print(b.grad_fn)\r\n","a.requires_grad_(True)\r\n","print(a.requires_grad)\r\n","b = (a * a).sum()\r\n","print(b.grad_fn)"],"execution_count":null,"outputs":[]}]}